{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import concatenate\n",
    "# Load the dataset\n",
    "final_dataset_balanced = pd.read_parquet('../Data/Data/FinalDataSet/Date_final_dataset_balanced_float32.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_df_train=final_dataset_balanced[final_dataset_balanced.acq_date<'2022-01-01']\n",
    "wf_df_valid=final_dataset_balanced[final_dataset_balanced.acq_date>='2022-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acq_date\n",
      "latitude\n",
      "longitude\n",
      "is_holiday\n",
      "day_of_week\n",
      "day_of_year\n",
      "is_weekend\n",
      "NDVI\n",
      "SoilMoisture\n",
      "sea_distance\n",
      "station_lat\n",
      "station_lon\n",
      "average_temperature_lag_1\n",
      "average_temperature_lag_2\n",
      "average_temperature_lag_3\n",
      "average_temperature_lag_4\n",
      "average_temperature_lag_5\n",
      "average_temperature_lag_6\n",
      "average_temperature_lag_7\n",
      "average_temperature_lag_8\n",
      "average_temperature_lag_9\n",
      "average_temperature_lag_10\n",
      "average_temperature_lag_11\n",
      "average_temperature_lag_12\n",
      "average_temperature_lag_13\n",
      "average_temperature_lag_14\n",
      "average_temperature_lag_15\n",
      "maximum_temperature_lag_1\n",
      "maximum_temperature_lag_2\n",
      "maximum_temperature_lag_3\n",
      "maximum_temperature_lag_4\n",
      "maximum_temperature_lag_5\n",
      "maximum_temperature_lag_6\n",
      "maximum_temperature_lag_7\n",
      "maximum_temperature_lag_8\n",
      "maximum_temperature_lag_9\n",
      "maximum_temperature_lag_10\n",
      "maximum_temperature_lag_11\n",
      "maximum_temperature_lag_12\n",
      "maximum_temperature_lag_13\n",
      "maximum_temperature_lag_14\n",
      "maximum_temperature_lag_15\n",
      "minimum_temperature_lag_1\n",
      "minimum_temperature_lag_2\n",
      "minimum_temperature_lag_3\n",
      "minimum_temperature_lag_4\n",
      "minimum_temperature_lag_5\n",
      "minimum_temperature_lag_6\n",
      "minimum_temperature_lag_7\n",
      "minimum_temperature_lag_8\n",
      "minimum_temperature_lag_9\n",
      "minimum_temperature_lag_10\n",
      "minimum_temperature_lag_11\n",
      "minimum_temperature_lag_12\n",
      "minimum_temperature_lag_13\n",
      "minimum_temperature_lag_14\n",
      "minimum_temperature_lag_15\n",
      "precipitation_lag_1\n",
      "precipitation_lag_2\n",
      "precipitation_lag_3\n",
      "precipitation_lag_4\n",
      "precipitation_lag_5\n",
      "precipitation_lag_6\n",
      "precipitation_lag_7\n",
      "precipitation_lag_8\n",
      "precipitation_lag_9\n",
      "precipitation_lag_10\n",
      "precipitation_lag_11\n",
      "precipitation_lag_12\n",
      "precipitation_lag_13\n",
      "precipitation_lag_14\n",
      "precipitation_lag_15\n",
      "snow_depth_lag_1\n",
      "snow_depth_lag_2\n",
      "snow_depth_lag_3\n",
      "snow_depth_lag_4\n",
      "snow_depth_lag_5\n",
      "snow_depth_lag_6\n",
      "snow_depth_lag_7\n",
      "snow_depth_lag_8\n",
      "snow_depth_lag_9\n",
      "snow_depth_lag_10\n",
      "snow_depth_lag_11\n",
      "snow_depth_lag_12\n",
      "snow_depth_lag_13\n",
      "snow_depth_lag_14\n",
      "snow_depth_lag_15\n",
      "wind_speed_lag_1\n",
      "wind_speed_lag_2\n",
      "wind_speed_lag_3\n",
      "wind_speed_lag_4\n",
      "wind_speed_lag_5\n",
      "wind_speed_lag_6\n",
      "wind_speed_lag_7\n",
      "wind_speed_lag_8\n",
      "wind_speed_lag_9\n",
      "wind_speed_lag_10\n",
      "wind_speed_lag_11\n",
      "wind_speed_lag_12\n",
      "wind_speed_lag_13\n",
      "wind_speed_lag_14\n",
      "wind_speed_lag_15\n",
      "maximum_sustained_wind_speed_lag_1\n",
      "maximum_sustained_wind_speed_lag_2\n",
      "maximum_sustained_wind_speed_lag_3\n",
      "maximum_sustained_wind_speed_lag_4\n",
      "maximum_sustained_wind_speed_lag_5\n",
      "maximum_sustained_wind_speed_lag_6\n",
      "maximum_sustained_wind_speed_lag_7\n",
      "maximum_sustained_wind_speed_lag_8\n",
      "maximum_sustained_wind_speed_lag_9\n",
      "maximum_sustained_wind_speed_lag_10\n",
      "maximum_sustained_wind_speed_lag_11\n",
      "maximum_sustained_wind_speed_lag_12\n",
      "maximum_sustained_wind_speed_lag_13\n",
      "maximum_sustained_wind_speed_lag_14\n",
      "maximum_sustained_wind_speed_lag_15\n",
      "wind_gust_lag_1\n",
      "wind_gust_lag_2\n",
      "wind_gust_lag_3\n",
      "wind_gust_lag_4\n",
      "wind_gust_lag_5\n",
      "wind_gust_lag_6\n",
      "wind_gust_lag_7\n",
      "wind_gust_lag_8\n",
      "wind_gust_lag_9\n",
      "wind_gust_lag_10\n",
      "wind_gust_lag_11\n",
      "wind_gust_lag_12\n",
      "wind_gust_lag_13\n",
      "wind_gust_lag_14\n",
      "wind_gust_lag_15\n",
      "dew_point_lag_1\n",
      "dew_point_lag_2\n",
      "dew_point_lag_3\n",
      "dew_point_lag_4\n",
      "dew_point_lag_5\n",
      "dew_point_lag_6\n",
      "dew_point_lag_7\n",
      "dew_point_lag_8\n",
      "dew_point_lag_9\n",
      "dew_point_lag_10\n",
      "dew_point_lag_11\n",
      "dew_point_lag_12\n",
      "dew_point_lag_13\n",
      "dew_point_lag_14\n",
      "dew_point_lag_15\n",
      "fog_lag_1\n",
      "fog_lag_2\n",
      "fog_lag_3\n",
      "fog_lag_4\n",
      "fog_lag_5\n",
      "fog_lag_6\n",
      "fog_lag_7\n",
      "fog_lag_8\n",
      "fog_lag_9\n",
      "fog_lag_10\n",
      "fog_lag_11\n",
      "fog_lag_12\n",
      "fog_lag_13\n",
      "fog_lag_14\n",
      "fog_lag_15\n",
      "thunder_lag_1\n",
      "thunder_lag_2\n",
      "thunder_lag_3\n",
      "thunder_lag_4\n",
      "thunder_lag_5\n",
      "thunder_lag_6\n",
      "thunder_lag_7\n",
      "thunder_lag_8\n",
      "thunder_lag_9\n",
      "thunder_lag_10\n",
      "thunder_lag_11\n",
      "thunder_lag_12\n",
      "thunder_lag_13\n",
      "thunder_lag_14\n",
      "thunder_lag_15\n",
      "lat_lag_1\n",
      "lat_lag_2\n",
      "lat_lag_3\n",
      "lat_lag_4\n",
      "lat_lag_5\n",
      "lat_lag_6\n",
      "lat_lag_7\n",
      "lat_lag_8\n",
      "lat_lag_9\n",
      "lat_lag_10\n",
      "lat_lag_11\n",
      "lat_lag_12\n",
      "lat_lag_13\n",
      "lat_lag_14\n",
      "lat_lag_15\n",
      "lon_lag_1\n",
      "lon_lag_2\n",
      "lon_lag_3\n",
      "lon_lag_4\n",
      "lon_lag_5\n",
      "lon_lag_6\n",
      "lon_lag_7\n",
      "lon_lag_8\n",
      "lon_lag_9\n",
      "lon_lag_10\n",
      "lon_lag_11\n",
      "lon_lag_12\n",
      "lon_lag_13\n",
      "lon_lag_14\n",
      "lon_lag_15\n",
      "average_temperature_weekly_mean\n",
      "maximum_temperature_weekly_mean\n",
      "minimum_temperature_weekly_mean\n",
      "precipitation_weekly_mean\n",
      "snow_depth_weekly_mean\n",
      "wind_gust_weekly_mean\n",
      "dew_point_weekly_mean\n",
      "average_temperature_last_1_year\n",
      "average_temperature_last_2_year\n",
      "average_temperature_last_3_year\n",
      "maximum_temperature_last_1_year\n",
      "maximum_temperature_last_2_year\n",
      "maximum_temperature_last_3_year\n",
      "minimum_temperature_last_1_year\n",
      "minimum_temperature_last_2_year\n",
      "minimum_temperature_last_3_year\n",
      "precipitation_last_1_year\n",
      "precipitation_last_2_year\n",
      "precipitation_last_3_year\n",
      "snow_depth_last_1_year\n",
      "snow_depth_last_2_year\n",
      "snow_depth_last_3_year\n",
      "wind_gust_last_1_year\n",
      "wind_gust_last_2_year\n",
      "wind_gust_last_3_year\n",
      "dew_point_last_1_year\n",
      "dew_point_last_2_year\n",
      "dew_point_last_3_year\n",
      "average_temperature_monthly_mean\n",
      "maximum_temperature_monthly_mean\n",
      "minimum_temperature_monthly_mean\n",
      "precipitation_monthly_mean\n",
      "snow_depth_monthly_mean\n",
      "wind_gust_monthly_mean\n",
      "dew_point_monthly_mean\n",
      "average_temperature_last_1_year_monthly_mean\n",
      "average_temperature_last_2_year_monthly_mean\n",
      "average_temperature_last_3_year_monthly_mean\n",
      "maximum_temperature_last_1_year_monthly_mean\n",
      "maximum_temperature_last_2_year_monthly_mean\n",
      "maximum_temperature_last_3_year_monthly_mean\n",
      "minimum_temperature_last_1_year_monthly_mean\n",
      "minimum_temperature_last_2_year_monthly_mean\n",
      "minimum_temperature_last_3_year_monthly_mean\n",
      "precipitation_last_1_year_monthly_mean\n",
      "precipitation_last_2_year_monthly_mean\n",
      "precipitation_last_3_year_monthly_mean\n",
      "snow_depth_last_1_year_monthly_mean\n",
      "snow_depth_last_2_year_monthly_mean\n",
      "snow_depth_last_3_year_monthly_mean\n",
      "wind_gust_last_1_year_monthly_mean\n",
      "wind_gust_last_2_year_monthly_mean\n",
      "wind_gust_last_3_year_monthly_mean\n",
      "dew_point_last_1_year_monthly_mean\n",
      "dew_point_last_2_year_monthly_mean\n",
      "dew_point_last_3_year_monthly_mean\n",
      "average_temperature_quarterly_mean\n",
      "maximum_temperature_quarterly_mean\n",
      "minimum_temperature_quarterly_mean\n",
      "precipitation_quarterly_mean\n",
      "snow_depth_quarterly_mean\n",
      "wind_gust_quarterly_mean\n",
      "dew_point_quarterly_mean\n",
      "average_temperature_yearly_mean\n",
      "maximum_temperature_yearly_mean\n",
      "minimum_temperature_yearly_mean\n",
      "precipitation_yearly_mean\n",
      "snow_depth_yearly_mean\n",
      "wind_gust_yearly_mean\n",
      "dew_point_yearly_mean\n",
      "is_fire\n"
     ]
    }
   ],
   "source": [
    "for c in wf_df_train.columns:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the dataset by taking the same number of positive and negative samples\n",
    "# wf_df_train = wf_df_train.sample(frac=1)\n",
    "\n",
    "# Get the minimum number of samples in each class\n",
    "min_samples = min(wf_df_train['is_fire'].value_counts())\n",
    "\n",
    "# Balance the training dataset\n",
    "wf_df_train_balanced = wf_df_train.groupby('is_fire').apply(lambda x: x.sample(min_samples)).reset_index(drop=True)\n",
    "\n",
    "# Balance the validation dataset\n",
    "min_samples_valid = min(wf_df_valid['is_fire'].value_counts())\n",
    "wf_df_valid_balanced = wf_df_valid.groupby('is_fire').apply(lambda x: x.sample(min_samples_valid)).reset_index(drop=True)\n",
    "\n",
    "wf_df_train_balanced = wf_df_train_balanced.sample(frac=1)\n",
    "wf_df_valid_balanced = wf_df_valid_balanced.sample(frac=1)\n",
    "\n",
    "# removing dates\n",
    "acq_date_train=wf_df_train_balanced.pop('acq_date')\n",
    "acq_date_valid=wf_df_valid_balanced.pop('acq_date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>SoilMoisture</th>\n",
       "      <th>sea_distance</th>\n",
       "      <th>station_lat</th>\n",
       "      <th>...</th>\n",
       "      <th>wind_gust_quarterly_mean</th>\n",
       "      <th>dew_point_quarterly_mean</th>\n",
       "      <th>average_temperature_yearly_mean</th>\n",
       "      <th>maximum_temperature_yearly_mean</th>\n",
       "      <th>minimum_temperature_yearly_mean</th>\n",
       "      <th>precipitation_yearly_mean</th>\n",
       "      <th>snow_depth_yearly_mean</th>\n",
       "      <th>wind_gust_yearly_mean</th>\n",
       "      <th>dew_point_yearly_mean</th>\n",
       "      <th>is_fire</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7834</th>\n",
       "      <td>28.439558</td>\n",
       "      <td>-10.029773</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>707.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>92553.781250</td>\n",
       "      <td>29.017000</td>\n",
       "      <td>...</td>\n",
       "      <td>787.164124</td>\n",
       "      <td>59.295650</td>\n",
       "      <td>69.822739</td>\n",
       "      <td>79.701370</td>\n",
       "      <td>84.747673</td>\n",
       "      <td>2.744904</td>\n",
       "      <td>999.900024</td>\n",
       "      <td>183.254242</td>\n",
       "      <td>49.621368</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33887</th>\n",
       "      <td>35.050900</td>\n",
       "      <td>-5.728900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4378.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>57893.476562</td>\n",
       "      <td>35.150002</td>\n",
       "      <td>...</td>\n",
       "      <td>837.813171</td>\n",
       "      <td>56.591209</td>\n",
       "      <td>66.166298</td>\n",
       "      <td>71.470276</td>\n",
       "      <td>57.671505</td>\n",
       "      <td>0.348274</td>\n",
       "      <td>999.900024</td>\n",
       "      <td>114.576988</td>\n",
       "      <td>56.370411</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38003</th>\n",
       "      <td>35.269577</td>\n",
       "      <td>-5.109628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5743.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>43970.628906</td>\n",
       "      <td>35.167000</td>\n",
       "      <td>...</td>\n",
       "      <td>999.900024</td>\n",
       "      <td>52.151649</td>\n",
       "      <td>65.949043</td>\n",
       "      <td>76.781372</td>\n",
       "      <td>51.520000</td>\n",
       "      <td>0.066507</td>\n",
       "      <td>999.900024</td>\n",
       "      <td>999.900024</td>\n",
       "      <td>242.498215</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15305</th>\n",
       "      <td>31.672754</td>\n",
       "      <td>-7.460205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1882.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>224915.640625</td>\n",
       "      <td>31.607000</td>\n",
       "      <td>...</td>\n",
       "      <td>613.298889</td>\n",
       "      <td>51.762638</td>\n",
       "      <td>69.567673</td>\n",
       "      <td>84.606300</td>\n",
       "      <td>56.657536</td>\n",
       "      <td>0.012178</td>\n",
       "      <td>999.900024</td>\n",
       "      <td>864.437256</td>\n",
       "      <td>49.907806</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18590</th>\n",
       "      <td>32.170471</td>\n",
       "      <td>-8.861309</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2726.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>58737.328125</td>\n",
       "      <td>32.283001</td>\n",
       "      <td>...</td>\n",
       "      <td>988.934448</td>\n",
       "      <td>48.271667</td>\n",
       "      <td>68.063423</td>\n",
       "      <td>75.906578</td>\n",
       "      <td>59.228493</td>\n",
       "      <td>0.029315</td>\n",
       "      <td>999.900024</td>\n",
       "      <td>989.070435</td>\n",
       "      <td>56.575615</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23178</th>\n",
       "      <td>35.261955</td>\n",
       "      <td>-5.505293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5572.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>67425.273438</td>\n",
       "      <td>35.167000</td>\n",
       "      <td>...</td>\n",
       "      <td>999.900024</td>\n",
       "      <td>52.151649</td>\n",
       "      <td>65.949043</td>\n",
       "      <td>76.781372</td>\n",
       "      <td>51.520000</td>\n",
       "      <td>0.066507</td>\n",
       "      <td>999.900024</td>\n",
       "      <td>999.900024</td>\n",
       "      <td>242.498215</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37663</th>\n",
       "      <td>35.280132</td>\n",
       "      <td>-6.061341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3283.0</td>\n",
       "      <td>29.5</td>\n",
       "      <td>11754.611328</td>\n",
       "      <td>35.150002</td>\n",
       "      <td>...</td>\n",
       "      <td>837.813171</td>\n",
       "      <td>56.591209</td>\n",
       "      <td>66.166298</td>\n",
       "      <td>71.470276</td>\n",
       "      <td>57.671505</td>\n",
       "      <td>0.348274</td>\n",
       "      <td>999.900024</td>\n",
       "      <td>114.576988</td>\n",
       "      <td>56.370411</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6284</th>\n",
       "      <td>31.225174</td>\n",
       "      <td>-6.009823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1123.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>372666.875000</td>\n",
       "      <td>30.938999</td>\n",
       "      <td>...</td>\n",
       "      <td>946.883667</td>\n",
       "      <td>27.145653</td>\n",
       "      <td>69.711647</td>\n",
       "      <td>82.831917</td>\n",
       "      <td>55.054520</td>\n",
       "      <td>0.286767</td>\n",
       "      <td>999.900024</td>\n",
       "      <td>871.874268</td>\n",
       "      <td>26.710958</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2183</th>\n",
       "      <td>32.077972</td>\n",
       "      <td>-6.991622</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2495.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>222927.562500</td>\n",
       "      <td>32.367001</td>\n",
       "      <td>...</td>\n",
       "      <td>905.444580</td>\n",
       "      <td>50.809784</td>\n",
       "      <td>70.802742</td>\n",
       "      <td>82.505203</td>\n",
       "      <td>108.412605</td>\n",
       "      <td>4.931863</td>\n",
       "      <td>999.900024</td>\n",
       "      <td>976.021912</td>\n",
       "      <td>46.451508</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>28.447491</td>\n",
       "      <td>-8.793737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1030.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>188721.890625</td>\n",
       "      <td>29.017000</td>\n",
       "      <td>...</td>\n",
       "      <td>787.164124</td>\n",
       "      <td>59.295650</td>\n",
       "      <td>69.822739</td>\n",
       "      <td>79.701370</td>\n",
       "      <td>84.747673</td>\n",
       "      <td>2.744904</td>\n",
       "      <td>999.900024</td>\n",
       "      <td>183.254242</td>\n",
       "      <td>49.621368</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38526 rows × 277 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        latitude  longitude  is_holiday  day_of_week  day_of_year  is_weekend  \\\n",
       "7834   28.439558 -10.029773         1.0          6.0        310.0         1.0   \n",
       "33887  35.050900  -5.728900         0.0          3.0        195.0         0.0   \n",
       "38003  35.269577  -5.109628         0.0          4.0        196.0         0.0   \n",
       "15305  31.672754  -7.460205         0.0          1.0        263.0         0.0   \n",
       "18590  32.170471  -8.861309         1.0          1.0        123.0         0.0   \n",
       "...          ...        ...         ...          ...          ...         ...   \n",
       "23178  35.261955  -5.505293         0.0          6.0        212.0         1.0   \n",
       "37663  35.280132  -6.061341         0.0          3.0        195.0         0.0   \n",
       "6284   31.225174  -6.009823         0.0          1.0         46.0         0.0   \n",
       "2183   32.077972  -6.991622         0.0          0.0        353.0         0.0   \n",
       "1129   28.447491  -8.793737         0.0          0.0        276.0         0.0   \n",
       "\n",
       "         NDVI  SoilMoisture   sea_distance  station_lat  ...  \\\n",
       "7834    707.0          12.0   92553.781250    29.017000  ...   \n",
       "33887  4378.0          15.5   57893.476562    35.150002  ...   \n",
       "38003  5743.0          49.0   43970.628906    35.167000  ...   \n",
       "15305  1882.0          17.0  224915.640625    31.607000  ...   \n",
       "18590  2726.0          17.0   58737.328125    32.283001  ...   \n",
       "...       ...           ...            ...          ...  ...   \n",
       "23178  5572.0          20.0   67425.273438    35.167000  ...   \n",
       "37663  3283.0          29.5   11754.611328    35.150002  ...   \n",
       "6284   1123.0          21.0  372666.875000    30.938999  ...   \n",
       "2183   2495.0          31.0  222927.562500    32.367001  ...   \n",
       "1129   1030.0           3.0  188721.890625    29.017000  ...   \n",
       "\n",
       "       wind_gust_quarterly_mean  dew_point_quarterly_mean  \\\n",
       "7834                 787.164124                 59.295650   \n",
       "33887                837.813171                 56.591209   \n",
       "38003                999.900024                 52.151649   \n",
       "15305                613.298889                 51.762638   \n",
       "18590                988.934448                 48.271667   \n",
       "...                         ...                       ...   \n",
       "23178                999.900024                 52.151649   \n",
       "37663                837.813171                 56.591209   \n",
       "6284                 946.883667                 27.145653   \n",
       "2183                 905.444580                 50.809784   \n",
       "1129                 787.164124                 59.295650   \n",
       "\n",
       "       average_temperature_yearly_mean  maximum_temperature_yearly_mean  \\\n",
       "7834                         69.822739                        79.701370   \n",
       "33887                        66.166298                        71.470276   \n",
       "38003                        65.949043                        76.781372   \n",
       "15305                        69.567673                        84.606300   \n",
       "18590                        68.063423                        75.906578   \n",
       "...                                ...                              ...   \n",
       "23178                        65.949043                        76.781372   \n",
       "37663                        66.166298                        71.470276   \n",
       "6284                         69.711647                        82.831917   \n",
       "2183                         70.802742                        82.505203   \n",
       "1129                         69.822739                        79.701370   \n",
       "\n",
       "       minimum_temperature_yearly_mean  precipitation_yearly_mean  \\\n",
       "7834                         84.747673                   2.744904   \n",
       "33887                        57.671505                   0.348274   \n",
       "38003                        51.520000                   0.066507   \n",
       "15305                        56.657536                   0.012178   \n",
       "18590                        59.228493                   0.029315   \n",
       "...                                ...                        ...   \n",
       "23178                        51.520000                   0.066507   \n",
       "37663                        57.671505                   0.348274   \n",
       "6284                         55.054520                   0.286767   \n",
       "2183                        108.412605                   4.931863   \n",
       "1129                         84.747673                   2.744904   \n",
       "\n",
       "       snow_depth_yearly_mean  wind_gust_yearly_mean  dew_point_yearly_mean  \\\n",
       "7834               999.900024             183.254242              49.621368   \n",
       "33887              999.900024             114.576988              56.370411   \n",
       "38003              999.900024             999.900024             242.498215   \n",
       "15305              999.900024             864.437256              49.907806   \n",
       "18590              999.900024             989.070435              56.575615   \n",
       "...                       ...                    ...                    ...   \n",
       "23178              999.900024             999.900024             242.498215   \n",
       "37663              999.900024             114.576988              56.370411   \n",
       "6284               999.900024             871.874268              26.710958   \n",
       "2183               999.900024             976.021912              46.451508   \n",
       "1129               999.900024             183.254242              49.621368   \n",
       "\n",
       "       is_fire  \n",
       "7834       0.0  \n",
       "33887      1.0  \n",
       "38003      1.0  \n",
       "15305      0.0  \n",
       "18590      0.0  \n",
       "...        ...  \n",
       "23178      1.0  \n",
       "37663      1.0  \n",
       "6284       0.0  \n",
       "2183       0.0  \n",
       "1129       0.0  \n",
       "\n",
       "[38526 rows x 277 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf_df_valid_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Split the dataset into features and target\n",
    "# X = final_dataset_balanced.drop(columns=['is_fire'])\n",
    "# y = final_dataset_balanced['is_fire']\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Initialize a StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# # Fit the scaler on the training features and transform both the training and testing features\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Split the training dataset into features and target\n",
    "y_train = wf_df_train_balanced['is_fire']\n",
    "X_train = wf_df_train_balanced.drop(columns=['is_fire'])\n",
    "\n",
    "# Split the validation dataset into features and target\n",
    "y_valid = wf_df_valid_balanced['is_fire']\n",
    "X_valid = wf_df_valid_balanced.drop(columns=['is_fire'])\n",
    "\n",
    "# Initialize a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training features and transform both the training and validation features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFN POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Layer, LayerNormalization, Embedding, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming final_dataset_balanced and wf_df_train_balanced, wf_df_valid_balanced are predefined\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = final_dataset_balanced.drop(columns=['is_fire'])\n",
    "y = final_dataset_balanced['is_fire']\n",
    "\n",
    "# Remove DateTime columns (if any)\n",
    "datetime_columns = X.select_dtypes(include=['datetime64']).columns\n",
    "X = X.drop(columns=datetime_columns)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training features and transform both the training and testing features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Split the training dataset into features and target\n",
    "X_train = wf_df_train_balanced.drop(columns=['is_fire'])\n",
    "y_train = wf_df_train_balanced['is_fire']\n",
    "\n",
    "# Split the validation dataset into features and target\n",
    "X_valid = wf_df_valid_balanced.drop(columns=['is_fire'])\n",
    "y_valid = wf_df_valid_balanced['is_fire']\n",
    "\n",
    "# Remove DateTime columns (if any)\n",
    "datetime_columns_train = X_train.select_dtypes(include=['datetime64']).columns\n",
    "X_train = X_train.drop(columns=datetime_columns_train)\n",
    "X_valid = X_valid.drop(columns=datetime_columns_train)\n",
    "\n",
    "# Initialize a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training features and transform both the training and validation features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "# FFN with Positional Encoding\n",
    "class FFNWithPosEncoding(Layer):\n",
    "    def __init__(self, num_columns, embed_dim, ff_dim, rate=0.1):\n",
    "        super(FFNWithPosEncoding, self).__init__()\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim)  # Output matches embed_dim\n",
    "        ])\n",
    "        self.layernorm = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.pos_emb = Embedding(input_dim=num_columns, output_dim=embed_dim)\n",
    "        self.positions = tf.range(start=0, limit=num_columns, delta=1)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Expand dimensions of inputs to match embedding output\n",
    "        x = tf.expand_dims(inputs, -1)\n",
    "\n",
    "        # Pass inputs through the feed forward network\n",
    "        x = self.ffn(x)\n",
    "\n",
    "        # Get positional embeddings\n",
    "        pos_encoding = self.pos_emb(self.positions)\n",
    "\n",
    "        # Expand pos_encoding to match the batch size of inputs\n",
    "        pos_encoding = tf.expand_dims(pos_encoding, 0)\n",
    "        pos_encoding = tf.tile(pos_encoding, [tf.shape(inputs)[0], 1, 1])\n",
    "\n",
    "        # Add dropout\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        # Add positional encodings\n",
    "        x += pos_encoding\n",
    "\n",
    "        # Apply layer normalization\n",
    "        return self.layernorm(x)\n",
    "\n",
    "# Model configuration\n",
    "num_columns = X_train_scaled.shape[1]\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "ff_dim = 32  # Hidden layer size in feed forward network\n",
    "\n",
    "inputs = Input(shape=(num_columns,))\n",
    "x = FFNWithPosEncoding(num_columns, embed_dim, ff_dim)(inputs)\n",
    "x = Flatten()(x)  # Flatten the output to align with the output layer\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)  # Single output neuron for binary classification\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=10, batch_size=128, validation_data=(X_valid_scaled, y_valid), verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Predict using the trained model\n",
    "predicted_prob = model.predict(X_test_scaled)\n",
    "predicted_class = (predicted_prob > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "# Compare hardcoded labels and predicted labels\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': predicted_class\n",
    "})\n",
    "\n",
    "# Calculate the mean difference (not very meaningful for binary classification, but for demonstration)\n",
    "mean_difference = np.mean(np.abs(y_test - predicted_class))\n",
    "print(f\"Mean Difference: {mean_difference}\")\n",
    "\n",
    "# Display the comparison\n",
    "print(comparison_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embeding layer for continoues values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:278: UserWarning: Feature 227 is constant and will be replaced with 0.\n",
      "  warnings.warn(\n",
      "c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:278: UserWarning: Feature 255 is constant and will be replaced with 0.\n",
      "  warnings.warn(\n",
      "c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:278: UserWarning: Feature 226 is constant and will be replaced with 0.\n",
      "  warnings.warn(\n",
      "c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:278: UserWarning: Feature 227 is constant and will be replaced with 0.\n",
      "  warnings.warn(\n",
      "c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:278: UserWarning: Feature 254 is constant and will be replaced with 0.\n",
      "  warnings.warn(\n",
      "c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:278: UserWarning: Feature 255 is constant and will be replaced with 0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6471/6471 [==============================] - 36s 6ms/step - loss: 0.4199 - accuracy: 0.8156 - val_loss: 0.3223 - val_accuracy: 0.8571\n",
      "Epoch 2/10\n",
      "6471/6471 [==============================] - 32s 5ms/step - loss: 0.3934 - accuracy: 0.8307 - val_loss: 0.3219 - val_accuracy: 0.8595\n",
      "Epoch 3/10\n",
      "6471/6471 [==============================] - 36s 5ms/step - loss: 0.3887 - accuracy: 0.8327 - val_loss: 0.3273 - val_accuracy: 0.8528\n",
      "Epoch 4/10\n",
      "6471/6471 [==============================] - 34s 5ms/step - loss: 0.3834 - accuracy: 0.8355 - val_loss: 0.3190 - val_accuracy: 0.8642\n",
      "Epoch 5/10\n",
      "6471/6471 [==============================] - 35s 5ms/step - loss: 0.3792 - accuracy: 0.8370 - val_loss: 0.3484 - val_accuracy: 0.8507\n",
      "Epoch 6/10\n",
      "6471/6471 [==============================] - 34s 5ms/step - loss: 0.3766 - accuracy: 0.8383 - val_loss: 0.2946 - val_accuracy: 0.8632\n",
      "Epoch 7/10\n",
      "6471/6471 [==============================] - 33s 5ms/step - loss: 0.3746 - accuracy: 0.8394 - val_loss: 0.3153 - val_accuracy: 0.8609\n",
      "Epoch 8/10\n",
      "6471/6471 [==============================] - 34s 5ms/step - loss: 0.3730 - accuracy: 0.8399 - val_loss: 0.3320 - val_accuracy: 0.8517\n",
      "Epoch 9/10\n",
      "6471/6471 [==============================] - 40s 6ms/step - loss: 0.3721 - accuracy: 0.8404 - val_loss: 0.3373 - val_accuracy: 0.8483\n",
      "Epoch 10/10\n",
      "6471/6471 [==============================] - 39s 6ms/step - loss: 0.3707 - accuracy: 0.8413 - val_loss: 0.3359 - val_accuracy: 0.8530\n",
      "Test Accuracy: 0.8428348302841187\n",
      "5842/5842 [==============================] - 13s 2ms/step\n",
      "Mean Difference: 0.15716517403353342\n",
      "        Actual  Predicted\n",
      "710465     1.0          1\n",
      "628007     1.0          1\n",
      "280977     0.0          0\n",
      "345642     1.0          1\n",
      "138561     0.0          0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Layer, LayerNormalization, Embedding, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# Assuming final_dataset_balanced and wf_df_train_balanced, wf_df_valid_balanced are predefined\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = final_dataset_balanced.drop(columns=['is_fire'])\n",
    "y = final_dataset_balanced['is_fire']\n",
    "\n",
    "# Remove DateTime columns (if any)\n",
    "datetime_columns = X.select_dtypes(include=['datetime64']).columns\n",
    "X = X.drop(columns=datetime_columns)\n",
    "\n",
    "# Discretize continuous features\n",
    "discretizer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "X_discretized = discretizer.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_discretized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the training dataset into features and target\n",
    "X_train = wf_df_train_balanced.drop(columns=['is_fire'])\n",
    "y_train = wf_df_train_balanced['is_fire']\n",
    "\n",
    "# Split the validation dataset into features and target\n",
    "X_valid = wf_df_valid_balanced.drop(columns=['is_fire'])\n",
    "y_valid = wf_df_valid_balanced['is_fire']\n",
    "\n",
    "# Remove DateTime columns (if any)\n",
    "datetime_columns_train = X_train.select_dtypes(include=['datetime64']).columns\n",
    "X_train = X_train.drop(columns=datetime_columns_train)\n",
    "X_valid = X_valid.drop(columns=datetime_columns_train)\n",
    "\n",
    "# Discretize continuous features\n",
    "X_train_discretized = discretizer.fit_transform(X_train)\n",
    "X_valid_discretized = discretizer.transform(X_valid)\n",
    "\n",
    "# FFN with Positional Encoding\n",
    "class FFNWithPosEncoding(Layer):\n",
    "    def __init__(self, num_columns, embed_dim, ff_dim, rate=0.1):\n",
    "        super(FFNWithPosEncoding, self).__init__()\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim)  # Output matches embed_dim\n",
    "        ])\n",
    "        self.layernorm = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.pos_emb = Embedding(input_dim=num_columns, output_dim=embed_dim)\n",
    "        self.positions = tf.range(start=0, limit=num_columns, delta=1)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Expand dimensions of inputs to match embedding output\n",
    "        x = tf.expand_dims(inputs, -1)\n",
    "\n",
    "        # Pass inputs through the feed forward network\n",
    "        x = self.ffn(x)\n",
    "\n",
    "        # Get positional embeddings\n",
    "        pos_encoding = self.pos_emb(self.positions)\n",
    "\n",
    "        # Expand pos_encoding to match the batch size of inputs\n",
    "        pos_encoding = tf.expand_dims(pos_encoding, 0)\n",
    "        pos_encoding = tf.tile(pos_encoding, [tf.shape(inputs)[0], 1, 1])\n",
    "\n",
    "        # Add dropout\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        # Add positional encodings\n",
    "        x += pos_encoding\n",
    "\n",
    "        # Apply layer normalization\n",
    "        return self.layernorm(x)\n",
    "\n",
    "# Model configuration\n",
    "num_columns = X_train_discretized.shape[1]\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "ff_dim = 32  # Hidden layer size in feed forward network\n",
    "\n",
    "inputs = Input(shape=(num_columns,))\n",
    "x = FFNWithPosEncoding(num_columns, embed_dim, ff_dim)(inputs)\n",
    "x = Flatten()(x)  # Flatten the output to align with the output layer\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)  # Single output neuron for binary classification\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_discretized, y_train, epochs=10, batch_size=128, validation_data=(X_valid_discretized, y_valid), verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Predict using the trained model\n",
    "predicted_prob = model.predict(X_test)\n",
    "predicted_class = (predicted_prob > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "# Compare hardcoded labels and predicted labels\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': predicted_class\n",
    "})\n",
    "\n",
    "# Calculate the mean difference (not very meaningful for binary classification, but for demonstration)\n",
    "mean_difference = np.mean(np.abs(y_test - predicted_class))\n",
    "print(f\"Mean Difference: {mean_difference}\")\n",
    "\n",
    "# Display the comparison\n",
    "print(comparison_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFN _ POS NO EMBEDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.3922 - accuracy: 0.8235 - val_loss: 0.3426 - val_accuracy: 0.8562\n",
      "Epoch 2/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.3280 - accuracy: 0.8618 - val_loss: 0.3079 - val_accuracy: 0.8716\n",
      "Epoch 3/100\n",
      "5842/5842 [==============================] - 26s 5ms/step - loss: 0.3020 - accuracy: 0.8747 - val_loss: 0.2912 - val_accuracy: 0.8800\n",
      "Epoch 4/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.2860 - accuracy: 0.8824 - val_loss: 0.2798 - val_accuracy: 0.8856\n",
      "Epoch 5/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.2751 - accuracy: 0.8878 - val_loss: 0.2668 - val_accuracy: 0.8920\n",
      "Epoch 6/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.2666 - accuracy: 0.8915 - val_loss: 0.2618 - val_accuracy: 0.8940\n",
      "Epoch 7/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.2603 - accuracy: 0.8947 - val_loss: 0.2555 - val_accuracy: 0.8967\n",
      "Epoch 8/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.2546 - accuracy: 0.8974 - val_loss: 0.2527 - val_accuracy: 0.8988\n",
      "Epoch 9/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.2503 - accuracy: 0.8992 - val_loss: 0.2475 - val_accuracy: 0.9000\n",
      "Epoch 10/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.2466 - accuracy: 0.9010 - val_loss: 0.2477 - val_accuracy: 0.9000\n",
      "Epoch 11/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.2433 - accuracy: 0.9025 - val_loss: 0.2472 - val_accuracy: 0.9021\n",
      "Epoch 12/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.2402 - accuracy: 0.9039 - val_loss: 0.2395 - val_accuracy: 0.9050\n",
      "Epoch 13/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.2376 - accuracy: 0.9054 - val_loss: 0.2359 - val_accuracy: 0.9042\n",
      "Epoch 14/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.2350 - accuracy: 0.9066 - val_loss: 0.2382 - val_accuracy: 0.9061\n",
      "Epoch 15/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.2329 - accuracy: 0.9077 - val_loss: 0.2341 - val_accuracy: 0.9072\n",
      "Epoch 16/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.2307 - accuracy: 0.9089 - val_loss: 0.2305 - val_accuracy: 0.9084\n",
      "Epoch 17/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.2291 - accuracy: 0.9096 - val_loss: 0.2317 - val_accuracy: 0.9088\n",
      "Epoch 18/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.2274 - accuracy: 0.9106 - val_loss: 0.2279 - val_accuracy: 0.9102\n",
      "Epoch 19/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.2256 - accuracy: 0.9112 - val_loss: 0.2262 - val_accuracy: 0.9123\n",
      "Epoch 20/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.2240 - accuracy: 0.9122 - val_loss: 0.2266 - val_accuracy: 0.9098\n",
      "Epoch 21/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.2226 - accuracy: 0.9130 - val_loss: 0.2278 - val_accuracy: 0.9093\n",
      "Epoch 22/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.2209 - accuracy: 0.9138 - val_loss: 0.2248 - val_accuracy: 0.9126\n",
      "Epoch 23/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.2197 - accuracy: 0.9143 - val_loss: 0.2208 - val_accuracy: 0.9144\n",
      "Epoch 24/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.2184 - accuracy: 0.9151 - val_loss: 0.2255 - val_accuracy: 0.9136\n",
      "Epoch 25/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.2171 - accuracy: 0.9156 - val_loss: 0.2180 - val_accuracy: 0.9143\n",
      "Epoch 26/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.2161 - accuracy: 0.9157 - val_loss: 0.2161 - val_accuracy: 0.9163\n",
      "Epoch 27/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.2148 - accuracy: 0.9168 - val_loss: 0.2222 - val_accuracy: 0.9146\n",
      "Epoch 28/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.2137 - accuracy: 0.9170 - val_loss: 0.2171 - val_accuracy: 0.9155\n",
      "Epoch 29/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.2125 - accuracy: 0.9177 - val_loss: 0.2157 - val_accuracy: 0.9158\n",
      "Epoch 30/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.2116 - accuracy: 0.9180 - val_loss: 0.2157 - val_accuracy: 0.9162\n",
      "Epoch 31/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.2106 - accuracy: 0.9187 - val_loss: 0.2138 - val_accuracy: 0.9188\n",
      "Epoch 32/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.2098 - accuracy: 0.9189 - val_loss: 0.2123 - val_accuracy: 0.9187\n",
      "Epoch 33/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.2088 - accuracy: 0.9193 - val_loss: 0.2116 - val_accuracy: 0.9190\n",
      "Epoch 34/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.2078 - accuracy: 0.9200 - val_loss: 0.2117 - val_accuracy: 0.9177\n",
      "Epoch 35/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.2070 - accuracy: 0.9203 - val_loss: 0.2110 - val_accuracy: 0.9190\n",
      "Epoch 36/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.2059 - accuracy: 0.9209 - val_loss: 0.2099 - val_accuracy: 0.9197\n",
      "Epoch 37/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.2050 - accuracy: 0.9211 - val_loss: 0.2066 - val_accuracy: 0.9206\n",
      "Epoch 38/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.2041 - accuracy: 0.9217 - val_loss: 0.2087 - val_accuracy: 0.9199\n",
      "Epoch 39/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.2036 - accuracy: 0.9220 - val_loss: 0.2061 - val_accuracy: 0.9212\n",
      "Epoch 40/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.2027 - accuracy: 0.9222 - val_loss: 0.2059 - val_accuracy: 0.9216\n",
      "Epoch 41/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.2019 - accuracy: 0.9228 - val_loss: 0.2054 - val_accuracy: 0.9225\n",
      "Epoch 42/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.2012 - accuracy: 0.9233 - val_loss: 0.2047 - val_accuracy: 0.9208\n",
      "Epoch 43/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.2004 - accuracy: 0.9238 - val_loss: 0.2063 - val_accuracy: 0.9227\n",
      "Epoch 44/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.1998 - accuracy: 0.9241 - val_loss: 0.2043 - val_accuracy: 0.9239\n",
      "Epoch 45/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1990 - accuracy: 0.9245 - val_loss: 0.2068 - val_accuracy: 0.9199\n",
      "Epoch 46/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1983 - accuracy: 0.9249 - val_loss: 0.2027 - val_accuracy: 0.9250\n",
      "Epoch 47/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1977 - accuracy: 0.9251 - val_loss: 0.2034 - val_accuracy: 0.9236\n",
      "Epoch 48/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1972 - accuracy: 0.9254 - val_loss: 0.2036 - val_accuracy: 0.9204\n",
      "Epoch 49/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1965 - accuracy: 0.9259 - val_loss: 0.1993 - val_accuracy: 0.9240\n",
      "Epoch 50/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1958 - accuracy: 0.9261 - val_loss: 0.2018 - val_accuracy: 0.9247\n",
      "Epoch 51/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1955 - accuracy: 0.9262 - val_loss: 0.2013 - val_accuracy: 0.9237\n",
      "Epoch 52/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1948 - accuracy: 0.9265 - val_loss: 0.1994 - val_accuracy: 0.9246\n",
      "Epoch 53/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.1945 - accuracy: 0.9268 - val_loss: 0.2019 - val_accuracy: 0.9240\n",
      "Epoch 54/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1937 - accuracy: 0.9274 - val_loss: 0.1990 - val_accuracy: 0.9250\n",
      "Epoch 55/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1936 - accuracy: 0.9275 - val_loss: 0.1998 - val_accuracy: 0.9259\n",
      "Epoch 56/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1930 - accuracy: 0.9278 - val_loss: 0.1980 - val_accuracy: 0.9257\n",
      "Epoch 57/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.1924 - accuracy: 0.9279 - val_loss: 0.1973 - val_accuracy: 0.9276\n",
      "Epoch 58/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1920 - accuracy: 0.9281 - val_loss: 0.1991 - val_accuracy: 0.9259\n",
      "Epoch 59/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1914 - accuracy: 0.9285 - val_loss: 0.1974 - val_accuracy: 0.9265\n",
      "Epoch 60/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1909 - accuracy: 0.9288 - val_loss: 0.1961 - val_accuracy: 0.9256\n",
      "Epoch 61/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.1906 - accuracy: 0.9291 - val_loss: 0.1974 - val_accuracy: 0.9280\n",
      "Epoch 62/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1904 - accuracy: 0.9292 - val_loss: 0.1944 - val_accuracy: 0.9280\n",
      "Epoch 63/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1898 - accuracy: 0.9294 - val_loss: 0.1946 - val_accuracy: 0.9280\n",
      "Epoch 64/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1895 - accuracy: 0.9296 - val_loss: 0.1978 - val_accuracy: 0.9273\n",
      "Epoch 65/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.1893 - accuracy: 0.9297 - val_loss: 0.1979 - val_accuracy: 0.9277\n",
      "Epoch 66/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1888 - accuracy: 0.9302 - val_loss: 0.1952 - val_accuracy: 0.9277\n",
      "Epoch 67/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.1882 - accuracy: 0.9303 - val_loss: 0.1951 - val_accuracy: 0.9281\n",
      "Epoch 68/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1877 - accuracy: 0.9306 - val_loss: 0.1930 - val_accuracy: 0.9291\n",
      "Epoch 69/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1875 - accuracy: 0.9308 - val_loss: 0.1958 - val_accuracy: 0.9282\n",
      "Epoch 70/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1870 - accuracy: 0.9309 - val_loss: 0.1899 - val_accuracy: 0.9296\n",
      "Epoch 71/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1866 - accuracy: 0.9313 - val_loss: 0.1897 - val_accuracy: 0.9293\n",
      "Epoch 72/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1863 - accuracy: 0.9315 - val_loss: 0.1882 - val_accuracy: 0.9322\n",
      "Epoch 73/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1857 - accuracy: 0.9317 - val_loss: 0.1934 - val_accuracy: 0.9296\n",
      "Epoch 74/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1856 - accuracy: 0.9317 - val_loss: 0.1944 - val_accuracy: 0.9289\n",
      "Epoch 75/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.1853 - accuracy: 0.9319 - val_loss: 0.1926 - val_accuracy: 0.9294\n",
      "Epoch 76/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.1848 - accuracy: 0.9321 - val_loss: 0.1872 - val_accuracy: 0.9321\n",
      "Epoch 77/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.1844 - accuracy: 0.9325 - val_loss: 0.1868 - val_accuracy: 0.9321\n",
      "Epoch 78/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.1840 - accuracy: 0.9325 - val_loss: 0.1947 - val_accuracy: 0.9278\n",
      "Epoch 79/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1835 - accuracy: 0.9329 - val_loss: 0.1915 - val_accuracy: 0.9301\n",
      "Epoch 80/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1832 - accuracy: 0.9329 - val_loss: 0.1922 - val_accuracy: 0.9315\n",
      "Epoch 81/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1827 - accuracy: 0.9333 - val_loss: 0.1909 - val_accuracy: 0.9303\n",
      "Epoch 82/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.1823 - accuracy: 0.9334 - val_loss: 0.1885 - val_accuracy: 0.9318\n",
      "Epoch 83/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1821 - accuracy: 0.9335 - val_loss: 0.1899 - val_accuracy: 0.9306\n",
      "Epoch 84/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1818 - accuracy: 0.9339 - val_loss: 0.1904 - val_accuracy: 0.9302\n",
      "Epoch 85/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1816 - accuracy: 0.9338 - val_loss: 0.1915 - val_accuracy: 0.9309\n",
      "Epoch 86/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.1810 - accuracy: 0.9341 - val_loss: 0.1880 - val_accuracy: 0.9324\n",
      "Epoch 87/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.1810 - accuracy: 0.9342 - val_loss: 0.1914 - val_accuracy: 0.9310\n",
      "Epoch 88/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1804 - accuracy: 0.9345 - val_loss: 0.1850 - val_accuracy: 0.9332\n",
      "Epoch 89/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1802 - accuracy: 0.9347 - val_loss: 0.1893 - val_accuracy: 0.9324\n",
      "Epoch 90/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.1798 - accuracy: 0.9348 - val_loss: 0.1855 - val_accuracy: 0.9333\n",
      "Epoch 91/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1795 - accuracy: 0.9350 - val_loss: 0.1869 - val_accuracy: 0.9324\n",
      "Epoch 92/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1793 - accuracy: 0.9351 - val_loss: 0.1846 - val_accuracy: 0.9336\n",
      "Epoch 93/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1788 - accuracy: 0.9355 - val_loss: 0.1838 - val_accuracy: 0.9337\n",
      "Epoch 94/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1787 - accuracy: 0.9358 - val_loss: 0.1846 - val_accuracy: 0.9349\n",
      "Epoch 95/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.1783 - accuracy: 0.9357 - val_loss: 0.1866 - val_accuracy: 0.9333\n",
      "Epoch 96/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1779 - accuracy: 0.9362 - val_loss: 0.1925 - val_accuracy: 0.9315\n",
      "Epoch 97/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1775 - accuracy: 0.9363 - val_loss: 0.1910 - val_accuracy: 0.9301\n",
      "Epoch 98/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.1773 - accuracy: 0.9365 - val_loss: 0.1863 - val_accuracy: 0.9335\n",
      "Epoch 99/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.1771 - accuracy: 0.9367 - val_loss: 0.1910 - val_accuracy: 0.9324\n",
      "Epoch 100/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1765 - accuracy: 0.9370 - val_loss: 0.1900 - val_accuracy: 0.9315\n",
      "Test Accuracy: 0.931477963924408\n",
      "5842/5842 [==============================] - 8s 1ms/step\n",
      "        Actual  Predicted\n",
      "710465     1.0          1\n",
      "628007     1.0          1\n",
      "280977     0.0          0\n",
      "345642     1.0          1\n",
      "138561     0.0          0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Layer, LayerNormalization, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming final_dataset_balanced and wf_df_train_balanced, wf_df_valid_balanced are predefined\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = final_dataset_balanced.drop(columns=['is_fire'])\n",
    "y = final_dataset_balanced['is_fire']\n",
    "\n",
    "# Remove DateTime columns (if any)\n",
    "datetime_columns = X.select_dtypes(include=['datetime64']).columns\n",
    "X = X.drop(columns=datetime_columns)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training features and transform both the training and testing features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# FFN with Positional Encoding\n",
    "class FFNWithPosEncoding(Layer):\n",
    "    def __init__(self, num_features, ff_dim, rate=0.1):\n",
    "        super(FFNWithPosEncoding, self).__init__()\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(num_features)  # Output matches the number of features\n",
    "        ])\n",
    "        self.layernorm = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Pass inputs through the feed forward network\n",
    "        x = self.ffn(inputs)\n",
    "\n",
    "        # Calculate positional encoding\n",
    "        position_encoding = self.get_positional_encoding(tf.shape(inputs)[0], tf.shape(inputs)[1])\n",
    "\n",
    "        # Add positional encodings\n",
    "        x += position_encoding\n",
    "\n",
    "        # Apply dropout\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        # Apply layer normalization\n",
    "        return self.layernorm(x)\n",
    "\n",
    "    def get_positional_encoding(self, batch_size, num_features):\n",
    "        position_indices = tf.range(num_features, dtype=tf.float32)\n",
    "        position_encoding = position_indices[tf.newaxis, :]  # Shape: (1, num_features)\n",
    "        position_encoding = tf.tile(position_encoding, [batch_size, 1])  # Match batch size\n",
    "        return position_encoding\n",
    "\n",
    "# Model configuration\n",
    "num_features = X_train_scaled.shape[1]\n",
    "ff_dim = 32  # Hidden layer size in feed forward network\n",
    "\n",
    "inputs = Input(shape=(num_features,))\n",
    "x = FFNWithPosEncoding(num_features, ff_dim)(inputs)\n",
    "x = Flatten()(x)  # Flatten the output to align with the output layer\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)  # Single output neuron for binary classification\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=128, validation_data=(X_test_scaled, y_test), verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Predict using the trained model\n",
    "predicted_prob = model.predict(X_test_scaled)\n",
    "predicted_class = (predicted_prob > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "# Compare hardcoded labels and predicted labels\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': predicted_class\n",
    "})\n",
    "\n",
    "# Display the comparison\n",
    "print(comparison_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equivalat simple straitforword FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5842/5842 [==============================] - 26s 4ms/step - loss: 0.3231 - accuracy: 0.8612 - val_loss: 0.2579 - val_accuracy: 0.8931\n",
      "Epoch 2/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.2437 - accuracy: 0.9001 - val_loss: 0.2159 - val_accuracy: 0.9130\n",
      "Epoch 3/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.2114 - accuracy: 0.9157 - val_loss: 0.1959 - val_accuracy: 0.9223\n",
      "Epoch 4/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.1913 - accuracy: 0.9258 - val_loss: 0.1777 - val_accuracy: 0.9333\n",
      "Epoch 5/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1766 - accuracy: 0.9330 - val_loss: 0.1643 - val_accuracy: 0.9396\n",
      "Epoch 6/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.1666 - accuracy: 0.9380 - val_loss: 0.1558 - val_accuracy: 0.9451\n",
      "Epoch 7/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.1587 - accuracy: 0.9419 - val_loss: 0.1487 - val_accuracy: 0.9470\n",
      "Epoch 8/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.1515 - accuracy: 0.9455 - val_loss: 0.1444 - val_accuracy: 0.9499\n",
      "Epoch 9/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.1467 - accuracy: 0.9480 - val_loss: 0.1376 - val_accuracy: 0.9522\n",
      "Epoch 10/100\n",
      "5842/5842 [==============================] - 23s 4ms/step - loss: 0.1423 - accuracy: 0.9500 - val_loss: 0.1350 - val_accuracy: 0.9556\n",
      "Epoch 11/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.1374 - accuracy: 0.9522 - val_loss: 0.1313 - val_accuracy: 0.9561\n",
      "Epoch 12/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.1341 - accuracy: 0.9537 - val_loss: 0.1295 - val_accuracy: 0.9568\n",
      "Epoch 13/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.1312 - accuracy: 0.9553 - val_loss: 0.1240 - val_accuracy: 0.9601\n",
      "Epoch 14/100\n",
      "5842/5842 [==============================] - 23s 4ms/step - loss: 0.1277 - accuracy: 0.9566 - val_loss: 0.1210 - val_accuracy: 0.9602\n",
      "Epoch 15/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.1253 - accuracy: 0.9579 - val_loss: 0.1246 - val_accuracy: 0.9591\n",
      "Epoch 16/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.1235 - accuracy: 0.9586 - val_loss: 0.1207 - val_accuracy: 0.9607\n",
      "Epoch 17/100\n",
      "5842/5842 [==============================] - 23s 4ms/step - loss: 0.1209 - accuracy: 0.9598 - val_loss: 0.1182 - val_accuracy: 0.9620\n",
      "Epoch 18/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.1193 - accuracy: 0.9605 - val_loss: 0.1148 - val_accuracy: 0.9634\n",
      "Epoch 19/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.1171 - accuracy: 0.9614 - val_loss: 0.1121 - val_accuracy: 0.9666\n",
      "Epoch 20/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.1152 - accuracy: 0.9623 - val_loss: 0.1117 - val_accuracy: 0.9650\n",
      "Epoch 21/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.1137 - accuracy: 0.9627 - val_loss: 0.1114 - val_accuracy: 0.9649\n",
      "Epoch 22/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.1117 - accuracy: 0.9636 - val_loss: 0.1096 - val_accuracy: 0.9667\n",
      "Epoch 23/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.1105 - accuracy: 0.9642 - val_loss: 0.1069 - val_accuracy: 0.9679\n",
      "Epoch 24/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.1091 - accuracy: 0.9649 - val_loss: 0.1073 - val_accuracy: 0.9669\n",
      "Epoch 25/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.1079 - accuracy: 0.9656 - val_loss: 0.1054 - val_accuracy: 0.9670\n",
      "Epoch 26/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.1066 - accuracy: 0.9661 - val_loss: 0.1041 - val_accuracy: 0.9685\n",
      "Epoch 27/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.1057 - accuracy: 0.9663 - val_loss: 0.1020 - val_accuracy: 0.9694\n",
      "Epoch 28/100\n",
      "5842/5842 [==============================] - 23s 4ms/step - loss: 0.1045 - accuracy: 0.9670 - val_loss: 0.1033 - val_accuracy: 0.9691\n",
      "Epoch 29/100\n",
      "5842/5842 [==============================] - 23s 4ms/step - loss: 0.1040 - accuracy: 0.9673 - val_loss: 0.1030 - val_accuracy: 0.9686\n",
      "Epoch 30/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.1024 - accuracy: 0.9677 - val_loss: 0.1005 - val_accuracy: 0.9698\n",
      "Epoch 31/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.1014 - accuracy: 0.9682 - val_loss: 0.1012 - val_accuracy: 0.9700\n",
      "Epoch 32/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.1006 - accuracy: 0.9686 - val_loss: 0.1006 - val_accuracy: 0.9696\n",
      "Epoch 33/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.1002 - accuracy: 0.9686 - val_loss: 0.0994 - val_accuracy: 0.9705\n",
      "Epoch 34/100\n",
      "5842/5842 [==============================] - 23s 4ms/step - loss: 0.0987 - accuracy: 0.9693 - val_loss: 0.0961 - val_accuracy: 0.9720\n",
      "Epoch 35/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0985 - accuracy: 0.9694 - val_loss: 0.0993 - val_accuracy: 0.9702\n",
      "Epoch 36/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0971 - accuracy: 0.9699 - val_loss: 0.0990 - val_accuracy: 0.9710\n",
      "Epoch 37/100\n",
      "5842/5842 [==============================] - 23s 4ms/step - loss: 0.0966 - accuracy: 0.9701 - val_loss: 0.0954 - val_accuracy: 0.9722\n",
      "Epoch 38/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0960 - accuracy: 0.9704 - val_loss: 0.0944 - val_accuracy: 0.9733\n",
      "Epoch 39/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0952 - accuracy: 0.9707 - val_loss: 0.0974 - val_accuracy: 0.9710\n",
      "Epoch 40/100\n",
      "5842/5842 [==============================] - 23s 4ms/step - loss: 0.0950 - accuracy: 0.9709 - val_loss: 0.0989 - val_accuracy: 0.9710\n",
      "Epoch 41/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0942 - accuracy: 0.9713 - val_loss: 0.0922 - val_accuracy: 0.9737\n",
      "Epoch 42/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0930 - accuracy: 0.9718 - val_loss: 0.0915 - val_accuracy: 0.9744\n",
      "Epoch 43/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0930 - accuracy: 0.9717 - val_loss: 0.0919 - val_accuracy: 0.9740\n",
      "Epoch 44/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0928 - accuracy: 0.9720 - val_loss: 0.0965 - val_accuracy: 0.9724\n",
      "Epoch 45/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0919 - accuracy: 0.9721 - val_loss: 0.0903 - val_accuracy: 0.9744\n",
      "Epoch 46/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0912 - accuracy: 0.9724 - val_loss: 0.0932 - val_accuracy: 0.9734\n",
      "Epoch 47/100\n",
      "5842/5842 [==============================] - 23s 4ms/step - loss: 0.0906 - accuracy: 0.9727 - val_loss: 0.0905 - val_accuracy: 0.9748\n",
      "Epoch 48/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0904 - accuracy: 0.9728 - val_loss: 0.0949 - val_accuracy: 0.9727\n",
      "Epoch 49/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0897 - accuracy: 0.9730 - val_loss: 0.0898 - val_accuracy: 0.9743\n",
      "Epoch 50/100\n",
      "5842/5842 [==============================] - 23s 4ms/step - loss: 0.0889 - accuracy: 0.9733 - val_loss: 0.0892 - val_accuracy: 0.9744\n",
      "Epoch 51/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.0887 - accuracy: 0.9735 - val_loss: 0.0898 - val_accuracy: 0.9746\n",
      "Epoch 52/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0880 - accuracy: 0.9738 - val_loss: 0.0917 - val_accuracy: 0.9743\n",
      "Epoch 53/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0877 - accuracy: 0.9738 - val_loss: 0.0891 - val_accuracy: 0.9751\n",
      "Epoch 54/100\n",
      "5842/5842 [==============================] - 23s 4ms/step - loss: 0.0876 - accuracy: 0.9738 - val_loss: 0.0883 - val_accuracy: 0.9750\n",
      "Epoch 55/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0872 - accuracy: 0.9741 - val_loss: 0.0882 - val_accuracy: 0.9745\n",
      "Epoch 56/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0868 - accuracy: 0.9743 - val_loss: 0.0863 - val_accuracy: 0.9764\n",
      "Epoch 57/100\n",
      "5842/5842 [==============================] - 23s 4ms/step - loss: 0.0859 - accuracy: 0.9745 - val_loss: 0.0879 - val_accuracy: 0.9752\n",
      "Epoch 58/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0862 - accuracy: 0.9745 - val_loss: 0.0858 - val_accuracy: 0.9763\n",
      "Epoch 59/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0854 - accuracy: 0.9749 - val_loss: 0.0903 - val_accuracy: 0.9746\n",
      "Epoch 60/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0847 - accuracy: 0.9750 - val_loss: 0.0865 - val_accuracy: 0.9759\n",
      "Epoch 61/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0845 - accuracy: 0.9752 - val_loss: 0.0857 - val_accuracy: 0.9764\n",
      "Epoch 62/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0844 - accuracy: 0.9752 - val_loss: 0.0876 - val_accuracy: 0.9749\n",
      "Epoch 63/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0835 - accuracy: 0.9754 - val_loss: 0.0883 - val_accuracy: 0.9756\n",
      "Epoch 64/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0835 - accuracy: 0.9756 - val_loss: 0.0909 - val_accuracy: 0.9752\n",
      "Epoch 65/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0826 - accuracy: 0.9758 - val_loss: 0.0855 - val_accuracy: 0.9763\n",
      "Epoch 66/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0830 - accuracy: 0.9757 - val_loss: 0.0878 - val_accuracy: 0.9755\n",
      "Epoch 67/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0828 - accuracy: 0.9759 - val_loss: 0.0844 - val_accuracy: 0.9769\n",
      "Epoch 68/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0823 - accuracy: 0.9761 - val_loss: 0.0871 - val_accuracy: 0.9754\n",
      "Epoch 69/100\n",
      "5842/5842 [==============================] - 23s 4ms/step - loss: 0.0819 - accuracy: 0.9760 - val_loss: 0.0857 - val_accuracy: 0.9755\n",
      "Epoch 70/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.0820 - accuracy: 0.9762 - val_loss: 0.0884 - val_accuracy: 0.9755\n",
      "Epoch 71/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.0819 - accuracy: 0.9762 - val_loss: 0.0828 - val_accuracy: 0.9775\n",
      "Epoch 72/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0814 - accuracy: 0.9763 - val_loss: 0.0862 - val_accuracy: 0.9766\n",
      "Epoch 73/100\n",
      "5842/5842 [==============================] - 23s 4ms/step - loss: 0.0802 - accuracy: 0.9768 - val_loss: 0.0843 - val_accuracy: 0.9764\n",
      "Epoch 74/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.0807 - accuracy: 0.9767 - val_loss: 0.0847 - val_accuracy: 0.9766\n",
      "Epoch 75/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0800 - accuracy: 0.9770 - val_loss: 0.0840 - val_accuracy: 0.9772\n",
      "Epoch 76/100\n",
      "5842/5842 [==============================] - 23s 4ms/step - loss: 0.0799 - accuracy: 0.9768 - val_loss: 0.0850 - val_accuracy: 0.9762\n",
      "Epoch 77/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0792 - accuracy: 0.9772 - val_loss: 0.0812 - val_accuracy: 0.9777\n",
      "Epoch 78/100\n",
      "5842/5842 [==============================] - 23s 4ms/step - loss: 0.0794 - accuracy: 0.9771 - val_loss: 0.0837 - val_accuracy: 0.9774\n",
      "Epoch 79/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0795 - accuracy: 0.9772 - val_loss: 0.0818 - val_accuracy: 0.9774\n",
      "Epoch 80/100\n",
      "5842/5842 [==============================] - 23s 4ms/step - loss: 0.0787 - accuracy: 0.9773 - val_loss: 0.0812 - val_accuracy: 0.9784\n",
      "Epoch 81/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0788 - accuracy: 0.9774 - val_loss: 0.0837 - val_accuracy: 0.9776\n",
      "Epoch 82/100\n",
      "5842/5842 [==============================] - 23s 4ms/step - loss: 0.0782 - accuracy: 0.9776 - val_loss: 0.0828 - val_accuracy: 0.9771\n",
      "Epoch 83/100\n",
      "5842/5842 [==============================] - 23s 4ms/step - loss: 0.0781 - accuracy: 0.9776 - val_loss: 0.0813 - val_accuracy: 0.9783\n",
      "Epoch 84/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0784 - accuracy: 0.9775 - val_loss: 0.0798 - val_accuracy: 0.9782\n",
      "Epoch 85/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.0778 - accuracy: 0.9777 - val_loss: 0.0818 - val_accuracy: 0.9776\n",
      "Epoch 86/100\n",
      "5842/5842 [==============================] - 23s 4ms/step - loss: 0.0768 - accuracy: 0.9781 - val_loss: 0.0826 - val_accuracy: 0.9773\n",
      "Epoch 87/100\n",
      "5842/5842 [==============================] - 23s 4ms/step - loss: 0.0775 - accuracy: 0.9779 - val_loss: 0.0807 - val_accuracy: 0.9791\n",
      "Epoch 88/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0765 - accuracy: 0.9782 - val_loss: 0.0816 - val_accuracy: 0.9779\n",
      "Epoch 89/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0765 - accuracy: 0.9782 - val_loss: 0.0828 - val_accuracy: 0.9773\n",
      "Epoch 90/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0766 - accuracy: 0.9782 - val_loss: 0.0847 - val_accuracy: 0.9769\n",
      "Epoch 91/100\n",
      "5842/5842 [==============================] - 22s 4ms/step - loss: 0.0762 - accuracy: 0.9784 - val_loss: 0.0805 - val_accuracy: 0.9780\n",
      "Epoch 92/100\n",
      "5842/5842 [==============================] - 23s 4ms/step - loss: 0.0755 - accuracy: 0.9785 - val_loss: 0.0801 - val_accuracy: 0.9789\n",
      "Epoch 93/100\n",
      "5842/5842 [==============================] - 23s 4ms/step - loss: 0.0760 - accuracy: 0.9785 - val_loss: 0.0806 - val_accuracy: 0.9779\n",
      "Epoch 94/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0751 - accuracy: 0.9786 - val_loss: 0.0839 - val_accuracy: 0.9769\n",
      "Epoch 95/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0751 - accuracy: 0.9786 - val_loss: 0.0817 - val_accuracy: 0.9781\n",
      "Epoch 96/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.0753 - accuracy: 0.9787 - val_loss: 0.0807 - val_accuracy: 0.9780\n",
      "Epoch 97/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.0753 - accuracy: 0.9786 - val_loss: 0.0817 - val_accuracy: 0.9782\n",
      "Epoch 98/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0742 - accuracy: 0.9789 - val_loss: 0.0768 - val_accuracy: 0.9791\n",
      "Epoch 99/100\n",
      "5842/5842 [==============================] - 24s 4ms/step - loss: 0.0744 - accuracy: 0.9789 - val_loss: 0.0784 - val_accuracy: 0.9801\n",
      "Epoch 100/100\n",
      "5842/5842 [==============================] - 25s 4ms/step - loss: 0.0738 - accuracy: 0.9792 - val_loss: 0.0819 - val_accuracy: 0.9780\n",
      "Test Accuracy: 0.9780064225196838\n",
      "5842/5842 [==============================] - 7s 1ms/step\n",
      "        Actual  Predicted\n",
      "710465     1.0          1\n",
      "628007     1.0          1\n",
      "280977     0.0          0\n",
      "345642     1.0          1\n",
      "138561     0.0          0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, LayerNormalization, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming final_dataset_balanced and wf_df_train_balanced, wf_df_valid_balanced are predefined\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = final_dataset_balanced.drop(columns=['is_fire'])\n",
    "y = final_dataset_balanced['is_fire']\n",
    "\n",
    "# Remove DateTime columns (if any)\n",
    "datetime_columns = X.select_dtypes(include=['datetime64']).columns\n",
    "X = X.drop(columns=datetime_columns)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training features and transform both the training and testing features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the simple FFN model\n",
    "num_features = X_train_scaled.shape[1]\n",
    "ff_dim = 32  # Hidden layer size in feed forward network\n",
    "\n",
    "inputs = Input(shape=(num_features,))\n",
    "x = Dense(ff_dim, activation=\"relu\")(inputs)\n",
    "x = Dense(num_features, activation=\"relu\")(x)  # Match the number of output dimensions as the original FFN\n",
    "x = LayerNormalization(epsilon=1e-6)(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Flatten()(x)\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)  # Single output neuron for binary classification\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=128, validation_data=(X_test_scaled, y_test), verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Predict using the trained model\n",
    "predicted_prob = model.predict(X_test_scaled)\n",
    "predicted_class = (predicted_prob > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "# Compare hardcoded labels and predicted labels\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': predicted_class\n",
    "})\n",
    "\n",
    "# Display the comparison\n",
    "print(comparison_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trasformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:278: UserWarning: Feature 227 is constant and will be replaced with 0.\n",
      "  warnings.warn(\n",
      "c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:278: UserWarning: Feature 255 is constant and will be replaced with 0.\n",
      "  warnings.warn(\n",
      "c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:278: UserWarning: Feature 226 is constant and will be replaced with 0.\n",
      "  warnings.warn(\n",
      "c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:278: UserWarning: Feature 227 is constant and will be replaced with 0.\n",
      "  warnings.warn(\n",
      "c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:278: UserWarning: Feature 254 is constant and will be replaced with 0.\n",
      "  warnings.warn(\n",
      "c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:278: UserWarning: Feature 255 is constant and will be replaced with 0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'model_2/positional_encoding/add' defined at (most recent call last):\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\asyncio\\windows_events.py\", line 321, in run_forever\n      super().run_forever()\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\ayoub\\AppData\\Local\\Temp\\ipykernel_59260\\379422335.py\", line 105, in <module>\n      history = model.fit(X_train_discretized, y_train, epochs=10, batch_size=128, validation_data=(X_valid_discretized, y_valid), verbose=1)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\ayoub\\AppData\\Local\\Temp\\ipykernel_59260\\379422335.py\", line 69, in call\n      return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\nNode: 'model_2/positional_encoding/add'\nrequired broadcastable shapes\n\t [[{{node model_2/positional_encoding/add}}]] [Op:__inference_train_function_5991502]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 105\u001b[0m\n\u001b[0;32m    102\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_discretized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_valid_discretized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m    108\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'model_2/positional_encoding/add' defined at (most recent call last):\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\asyncio\\windows_events.py\", line 321, in run_forever\n      super().run_forever()\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3006, in run_cell\n      result = self._run_cell(\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3061, in _run_cell\n      result = runner(coro)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3266, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3445, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3505, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\ayoub\\AppData\\Local\\Temp\\ipykernel_59260\\379422335.py\", line 105, in <module>\n      history = model.fit(X_train_discretized, y_train, epochs=10, batch_size=128, validation_data=(X_valid_discretized, y_valid), verbose=1)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\ayoub\\AppData\\Local\\Temp\\ipykernel_59260\\379422335.py\", line 69, in call\n      return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\nNode: 'model_2/positional_encoding/add'\nrequired broadcastable shapes\n\t [[{{node model_2/positional_encoding/add}}]] [Op:__inference_train_function_5991502]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, LayerNormalization, Flatten, Input, MultiHeadAttention, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
    "\n",
    "# Assuming final_dataset_balanced and wf_df_train_balanced, wf_df_valid_balanced are predefined\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = final_dataset_balanced.drop(columns=['is_fire'])\n",
    "y = final_dataset_balanced['is_fire']\n",
    "\n",
    "# Remove DateTime columns (if any)\n",
    "datetime_columns = X.select_dtypes(include=['datetime64']).columns\n",
    "X = X.drop(columns=datetime_columns)\n",
    "\n",
    "# Discretize continuous features\n",
    "discretizer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "X_discretized = discretizer.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_discretized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the training dataset into features and target\n",
    "X_train = wf_df_train_balanced.drop(columns=['is_fire'])\n",
    "y_train = wf_df_train_balanced['is_fire']\n",
    "\n",
    "# Split the validation dataset into features and target\n",
    "X_valid = wf_df_valid_balanced.drop(columns=['is_fire'])\n",
    "y_valid = wf_df_valid_balanced['is_fire']\n",
    "\n",
    "# Remove DateTime columns (if any)\n",
    "datetime_columns_train = X_train.select_dtypes(include=['datetime64']).columns\n",
    "X_train = X_train.drop(columns=datetime_columns_train)\n",
    "X_valid = X_valid.drop(columns=datetime_columns_train)\n",
    "\n",
    "# Discretize continuous features\n",
    "X_train_discretized = discretizer.fit_transform(X_train)\n",
    "X_valid_discretized = discretizer.transform(X_valid)\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(Layer):\n",
    "    def __init__(self, num_columns, embed_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(num_columns, embed_dim)\n",
    "    \n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "    \n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis],\n",
    "                                     np.arange(d_model)[np.newaxis, :],\n",
    "                                     d_model)\n",
    "        \n",
    "        # apply sin to even indices in the array; 2i\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        \n",
    "        # apply cos to odd indices in the array; 2i+1\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        \n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "# Transformer Block\n",
    "def transformer_block(inputs, num_heads, ff_dim, dropout_rate):\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
    "    attn_output = Dropout(dropout_rate)(attn_output)\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(Add()([inputs, attn_output]))\n",
    "    \n",
    "    ffn_output = Dense(ff_dim, activation='relu')(out1)\n",
    "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
    "    ffn_output = Dropout(dropout_rate)(ffn_output)\n",
    "    return LayerNormalization(epsilon=1e-6)(Add()([out1, ffn_output]))\n",
    "\n",
    "# Model configuration\n",
    "num_columns = X_train_discretized.shape[1]\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network\n",
    "dropout_rate = 0.1\n",
    "\n",
    "inputs = Input(shape=(num_columns,))\n",
    "x = Dense(embed_dim)(inputs)\n",
    "x = PositionalEncoding(num_columns, embed_dim)(x)\n",
    "\n",
    "# Add multiple transformer blocks\n",
    "num_transformer_blocks = 2\n",
    "for _ in range(num_transformer_blocks):\n",
    "    x = transformer_block(x, num_heads, ff_dim, dropout_rate)\n",
    "\n",
    "x = Flatten()(x)  # Flatten the output to align with the output layer\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)  # Single output neuron for binary classification\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_discretized, y_train, epochs=10, batch_size=128, validation_data=(X_valid_discretized, y_valid), verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Predict using the trained model\n",
    "predicted_prob = model.predict(X_test)\n",
    "predicted_class = (predicted_prob > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "# Compare hardcoded labels and predicted labels\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': predicted_class\n",
    "})\n",
    "\n",
    "# Calculate the mean difference (not very meaningful for binary classification, but for demonstration)\n",
    "mean_difference = np.mean(np.abs(y_test - predicted_class))\n",
    "print(f\"Mean Difference: {mean_difference}\")\n",
    "\n",
    "# Display the comparison\n",
    "print(comparison_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minimum_ffn(input_dim, output_dim,normalizer,scale=1,midlel_function='sigmoid' ):\n",
    "    # Clear the session\n",
    "    K.clear_session()\n",
    "    layer1_size=16\n",
    "    # Create the model\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "    # Batch normalization for input\n",
    "    normalized_input = input_layer\n",
    "\n",
    "    x = Dense(layer1_size*scale)(normalized_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.7)(x)\n",
    "\n",
    "    x = Dense(8*scale)(normalized_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(midlel_function)(x)\n",
    "    x = Dropout(0.6)(x)\n",
    "\n",
    "    x = Dense(4*scale)(normalized_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.6)(x)\n",
    "  \n",
    "    x = Dense(64)(normalized_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    output_layer = Dense(8)(x)\n",
    "    output_layer = Dense(1, activation='sigmoid')(output_layer)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    # Compile the model with Huber loss function and AdamW optimizer\n",
    "    # optimizer = AdamW(weight_decay=1e-4)\n",
    "    # model.compile(loss=Huber(), optimizer=optimizer)    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',AUC(curve='PR', name='auc_pr'),Precision(), Recall()])\n",
    "\n",
    "\n",
    "    #model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multipath_model(input_dim, output_dim, paths,layer1_size=2000):\n",
    "    # Clear the session\n",
    "    K.clear_session()\n",
    "\n",
    "    # Create the model\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "    # Batch normalization for input\n",
    "    normalized_input = input_layer\n",
    "\n",
    "    # Common path\n",
    "    x = Dense(layer1_size)(normalized_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # Define paths\n",
    "    path_outputs = []\n",
    "    for path in paths:\n",
    "        activation_function = path[0]\n",
    "        dropout_rate = path[1]\n",
    "        layer_sizes = path[2:]\n",
    "        path_output = x\n",
    "        for size in layer_sizes:\n",
    "            path_output = Dense(size)(path_output)\n",
    "            path_output = BatchNormalization()(path_output)\n",
    "            path_output = Activation(activation_function)(path_output)\n",
    "            path_output = Dropout(dropout_rate)(path_output)\n",
    "        path_outputs.append(path_output)\n",
    "\n",
    "    # Concatenate paths\n",
    "    concat = concatenate(path_outputs)\n",
    "\n",
    "    # Output layer\n",
    "    output_layer = Dense(256)(concat)\n",
    "    output_layer = BatchNormalization()(output_layer)\n",
    "    output_layer = Dense(128)(output_layer)\n",
    "    output_layer = BatchNormalization()(output_layer)\n",
    "    output_layer = Dense(64)(output_layer)\n",
    "    output_layer = Dense(1, activation='sigmoid')(output_layer)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    # Compile the model with Huber loss function and AdamW optimizer\n",
    "    optimizer = AdamW(weight_decay=1e-4)\n",
    "    #model.compile(loss=Huber(), optimizer=optimizer)    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',AUC(curve='PR', name='auc_pr'),Precision(), Recall()])\n",
    "\n",
    "\n",
    "    #model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minidirect_multipath_model(input_dim, output_dim, paths):\n",
    "    # Clear the session\n",
    "    K.clear_session()\n",
    "\n",
    "    # Create the model\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "    # Batch normalization for input\n",
    "    normalized_input = input_layer\n",
    "\n",
    "\n",
    "    x = normalized_input\n",
    "\n",
    "    # Define paths\n",
    "    path_outputs = []\n",
    "    for path in paths:\n",
    "        activation_function = path[0]\n",
    "        dropout_rate = path[1]\n",
    "        layer_sizes = path[2:]\n",
    "        path_output = x\n",
    "        for size in layer_sizes:\n",
    "            path_output = Dense(size)(path_output)\n",
    "            path_output = BatchNormalization()(path_output)\n",
    "            path_output = Activation(activation_function)(path_output)\n",
    "            path_output = Dropout(dropout_rate)(path_output)\n",
    "        path_outputs.append(path_output)\n",
    "\n",
    "    # Concatenate paths\n",
    "    concat = concatenate(path_outputs)\n",
    "\n",
    "    output_layer = Dense(64)(concat)\n",
    "    output_layer = BatchNormalization()(output_layer)\n",
    "    output_layer = Dense(29)(output_layer)\n",
    "    output_layer = Dense(1, activation='sigmoid')(output_layer)\n",
    "    \n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    # Compile the model with Huber loss function and AdamW optimizer\n",
    "    # optimizer = AdamW(weight_decay=1e-4)\n",
    "    #model.compile(loss=Huber(), optimizer=optimizer)    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',AUC(curve='PR', name='auc_pr'),Precision(), Recall()])\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "def create_minidirect_multipath_l1l2reg_model(input_dim, output_dim, paths, normalizer):\n",
    "    # Clear the session\n",
    "    K.clear_session()\n",
    "\n",
    "    # Create the model\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "    # Batch normalization for input\n",
    "    normalized_input = input_layer\n",
    "\n",
    "    x = normalized_input\n",
    "\n",
    "    # Define paths\n",
    "    path_outputs = []\n",
    "    for path in paths:\n",
    "        activation_function = path[0]\n",
    "        dropout_rate = path[1]\n",
    "        layer_sizes = path[2:]\n",
    "        path_output = x\n",
    "        for size in layer_sizes:\n",
    "            path_output = Dense(size, kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(path_output)\n",
    "            path_output = BatchNormalization()(path_output)\n",
    "            path_output = Activation(activation_function)(path_output)\n",
    "            path_output = Dropout(dropout_rate)(path_output)\n",
    "        path_outputs.append(path_output)\n",
    "\n",
    "    # Concatenate paths\n",
    "    concat = concatenate(path_outputs)\n",
    "\n",
    "    output_layer = Dense(64, kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(concat)\n",
    "    output_layer = BatchNormalization()(output_layer)\n",
    "    output_layer = Dense(29, kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(output_layer)\n",
    "    output_layer = Dense(1, activation='linear', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(output_layer)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    # Compile the model with Huber loss function and AdamW optimizer\n",
    "    # optimizer = AdamW(weight_decay=1e-4)\n",
    "    #model.compile(loss=Huber(), optimizer=optimizer)    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',AUC(curve='PR', name='auc_pr'),Precision(), Recall()])\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multipathencoder_cnn(input_dim, output_dim, size, layer1_size=2000):\n",
    "    # Clear the session\n",
    "    K.clear_session()\n",
    "\n",
    "    # Create the model\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "    # Batch normalization for input\n",
    "    normalized_input = input_layer\n",
    "\n",
    "    # Common path\n",
    "    x = Dense(layer1_size)(normalized_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    paths = size*[['relu', 0.4, size]]\n",
    "    \n",
    "    # Define paths\n",
    "    path_outputs = []\n",
    "    for path in paths:\n",
    "        activation_function = path[0]\n",
    "        dropout_rate = path[1]\n",
    "        layer_sizes = path[2:]\n",
    "        path_output = x\n",
    "        for size in layer_sizes:\n",
    "            path_output = Dense(size)(path_output)\n",
    "            path_output = BatchNormalization()(path_output)\n",
    "            path_output = Activation(activation_function)(path_output)\n",
    "            path_output = Dropout(dropout_rate)(path_output)\n",
    "        path_outputs.append(path_output)\n",
    "\n",
    "    # Concatenate paths\n",
    "    concat = concatenate(path_outputs)\n",
    "    reshape = Reshape((size, size, 1))(concat)  # Reshape to (size, size, 1) to fit CNN input shape\n",
    "\n",
    "    # Triple-layer CNN\n",
    "    conv = Conv2D(64, 3, padding='same')(reshape)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    conv = Conv2D(64, 3, padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    conv = Conv2D(64, 3, padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    pool = GlobalMaxPooling2D()(conv)\n",
    "\n",
    "    # Output layer\n",
    "    output_layer = Dense(196)(pool)\n",
    "    output_layer = BatchNormalization()(output_layer)\n",
    "    output_layer = Dense(64)(output_layer)\n",
    "    output_layer = Dense(1, activation='sigmoid')(output_layer)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    # Compile the model with Huber loss function and AdamW optimizer\n",
    "    # optimizer = AdamW(weight_decay=1e-4)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',AUC(curve='PR', name='auc_pr'),Precision(), Recall()])\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Sequential model\n",
    "from tensorflow.keras.metrics import AUC,Precision, Recall\n",
    "\n",
    "\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='softplus'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.6),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.7),\n",
    "    Dense(8, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',AUC(curve='PR', name='auc_pr'),Precision(), Recall()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_32 (Dense)            (None, 128)               35456     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 128)              512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 8)                 1032      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 8)                32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71,089\n",
      "Trainable params: 70,305\n",
      "Non-trainable params: 784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bn\n",
      "Epoch 1/60\n",
      "6471/6471 [==============================] - 144s 22ms/step - loss: 0.4093 - accuracy: 0.8191 - auc_pr: 0.8662 - precision_7: 0.7892 - recall_7: 0.8708 - val_loss: 0.3214 - val_accuracy: 0.8586 - val_auc_pr: 0.9446 - val_precision_7: 0.8267 - val_recall_7: 0.9075\n",
      "Epoch 2/60\n",
      "6471/6471 [==============================] - 135s 21ms/step - loss: 0.3435 - accuracy: 0.8543 - auc_pr: 0.9058 - precision_7: 0.8274 - recall_7: 0.8953 - val_loss: 0.3355 - val_accuracy: 0.8679 - val_auc_pr: 0.9284 - val_precision_7: 0.8567 - val_recall_7: 0.8836\n",
      "Epoch 3/60\n",
      "6471/6471 [==============================] - 133s 20ms/step - loss: 0.3213 - accuracy: 0.8647 - auc_pr: 0.9175 - precision_7: 0.8405 - recall_7: 0.9001 - val_loss: 0.3156 - val_accuracy: 0.8813 - val_auc_pr: 0.9386 - val_precision_7: 0.8698 - val_recall_7: 0.8968\n",
      "Epoch 4/60\n",
      "6471/6471 [==============================] - 132s 20ms/step - loss: 0.3079 - accuracy: 0.8707 - auc_pr: 0.9241 - precision_7: 0.8469 - recall_7: 0.9049 - val_loss: 0.3393 - val_accuracy: 0.8780 - val_auc_pr: 0.9299 - val_precision_7: 0.8727 - val_recall_7: 0.8850\n",
      "Epoch 5/60\n",
      "6471/6471 [==============================] - 130s 20ms/step - loss: 0.2984 - accuracy: 0.8753 - auc_pr: 0.9277 - precision_7: 0.8513 - recall_7: 0.9096 - val_loss: 0.3920 - val_accuracy: 0.8432 - val_auc_pr: 0.8952 - val_precision_7: 0.8843 - val_recall_7: 0.7896\n",
      "Epoch 6/60\n",
      "6471/6471 [==============================] - 129s 20ms/step - loss: 0.2912 - accuracy: 0.8788 - auc_pr: 0.9312 - precision_7: 0.8548 - recall_7: 0.9126 - val_loss: 0.3736 - val_accuracy: 0.8481 - val_auc_pr: 0.8983 - val_precision_7: 0.8912 - val_recall_7: 0.7930\n",
      "Epoch 7/60\n",
      "6471/6471 [==============================] - 132s 20ms/step - loss: 0.2845 - accuracy: 0.8818 - auc_pr: 0.9334 - precision_7: 0.8577 - recall_7: 0.9154 - val_loss: 0.4024 - val_accuracy: 0.8192 - val_auc_pr: 0.8920 - val_precision_7: 0.8800 - val_recall_7: 0.7393\n",
      "Epoch 8/60\n",
      "6471/6471 [==============================] - 132s 20ms/step - loss: 0.2796 - accuracy: 0.8837 - auc_pr: 0.9353 - precision_7: 0.8589 - recall_7: 0.9181 - val_loss: 0.3708 - val_accuracy: 0.8809 - val_auc_pr: 0.8947 - val_precision_7: 0.8900 - val_recall_7: 0.8692\n",
      "Epoch 9/60\n",
      "6471/6471 [==============================] - 132s 20ms/step - loss: 0.2751 - accuracy: 0.8858 - auc_pr: 0.9371 - precision_7: 0.8604 - recall_7: 0.9210 - val_loss: 0.4290 - val_accuracy: 0.8384 - val_auc_pr: 0.8498 - val_precision_7: 0.8768 - val_recall_7: 0.7875\n",
      "Epoch 10/60\n",
      "6471/6471 [==============================] - 135s 21ms/step - loss: 0.2713 - accuracy: 0.8877 - auc_pr: 0.9385 - precision_7: 0.8625 - recall_7: 0.9226 - val_loss: 0.3890 - val_accuracy: 0.8579 - val_auc_pr: 0.8832 - val_precision_7: 0.8974 - val_recall_7: 0.8082\n",
      "Epoch 11/60\n",
      "6471/6471 [==============================] - 135s 21ms/step - loss: 0.2678 - accuracy: 0.8897 - auc_pr: 0.9395 - precision_7: 0.8646 - recall_7: 0.9241 - val_loss: 0.3893 - val_accuracy: 0.8855 - val_auc_pr: 0.8890 - val_precision_7: 0.9139 - val_recall_7: 0.8513\n",
      "Epoch 12/60\n",
      "6471/6471 [==============================] - 133s 21ms/step - loss: 0.2631 - accuracy: 0.8918 - auc_pr: 0.9414 - precision_7: 0.8666 - recall_7: 0.9262 - val_loss: 0.4511 - val_accuracy: 0.8369 - val_auc_pr: 0.8606 - val_precision_7: 0.9132 - val_recall_7: 0.7445\n",
      "Epoch 13/60\n",
      "6471/6471 [==============================] - 134s 21ms/step - loss: 0.2604 - accuracy: 0.8935 - auc_pr: 0.9421 - precision_7: 0.8681 - recall_7: 0.9280 - val_loss: 0.4016 - val_accuracy: 0.8517 - val_auc_pr: 0.8661 - val_precision_7: 0.8983 - val_recall_7: 0.7932\n",
      "Epoch 14/60\n",
      "6471/6471 [==============================] - 134s 21ms/step - loss: 0.2587 - accuracy: 0.8941 - auc_pr: 0.9428 - precision_7: 0.8690 - recall_7: 0.9282 - val_loss: 0.4867 - val_accuracy: 0.7578 - val_auc_pr: 0.8507 - val_precision_7: 0.9108 - val_recall_7: 0.5716\n",
      "Epoch 15/60\n",
      "6471/6471 [==============================] - 133s 21ms/step - loss: 0.2552 - accuracy: 0.8958 - auc_pr: 0.9439 - precision_7: 0.8703 - recall_7: 0.9301 - val_loss: 0.4627 - val_accuracy: 0.8378 - val_auc_pr: 0.8439 - val_precision_7: 0.9175 - val_recall_7: 0.7423\n",
      "Epoch 16/60\n",
      "6471/6471 [==============================] - 134s 21ms/step - loss: 0.2545 - accuracy: 0.8968 - auc_pr: 0.9441 - precision_7: 0.8701 - recall_7: 0.9328 - val_loss: 0.4168 - val_accuracy: 0.8739 - val_auc_pr: 0.8751 - val_precision_7: 0.9213 - val_recall_7: 0.8177\n",
      "Epoch 17/60\n",
      "6471/6471 [==============================] - 134s 21ms/step - loss: 0.2517 - accuracy: 0.8982 - auc_pr: 0.9449 - precision_7: 0.8717 - recall_7: 0.9338 - val_loss: 0.4078 - val_accuracy: 0.8573 - val_auc_pr: 0.8803 - val_precision_7: 0.9193 - val_recall_7: 0.7833\n",
      "Epoch 18/60\n",
      "6471/6471 [==============================] - 133s 21ms/step - loss: 0.2486 - accuracy: 0.8995 - auc_pr: 0.9459 - precision_7: 0.8730 - recall_7: 0.9351 - val_loss: 0.4117 - val_accuracy: 0.8504 - val_auc_pr: 0.8810 - val_precision_7: 0.9181 - val_recall_7: 0.7695\n",
      "Epoch 19/60\n",
      "6471/6471 [==============================] - 136s 21ms/step - loss: 0.2477 - accuracy: 0.8998 - auc_pr: 0.9464 - precision_7: 0.8738 - recall_7: 0.9347 - val_loss: 0.5318 - val_accuracy: 0.7618 - val_auc_pr: 0.8325 - val_precision_7: 0.9145 - val_recall_7: 0.5777\n",
      "Epoch 20/60\n",
      "6471/6471 [==============================] - 136s 21ms/step - loss: 0.2457 - accuracy: 0.9008 - auc_pr: 0.9469 - precision_7: 0.8743 - recall_7: 0.9363 - val_loss: 0.4183 - val_accuracy: 0.8515 - val_auc_pr: 0.8907 - val_precision_7: 0.9118 - val_recall_7: 0.7783\n",
      "Epoch 21/60\n",
      "6471/6471 [==============================] - 134s 21ms/step - loss: 0.2435 - accuracy: 0.9022 - auc_pr: 0.9477 - precision_7: 0.8761 - recall_7: 0.9369 - val_loss: 0.4420 - val_accuracy: 0.8437 - val_auc_pr: 0.8544 - val_precision_7: 0.9067 - val_recall_7: 0.7661\n",
      "Epoch 22/60\n",
      "6471/6471 [==============================] - 136s 21ms/step - loss: 0.2428 - accuracy: 0.9027 - auc_pr: 0.9477 - precision_7: 0.8761 - recall_7: 0.9380 - val_loss: 0.4878 - val_accuracy: 0.8148 - val_auc_pr: 0.8547 - val_precision_7: 0.9274 - val_recall_7: 0.6831\n",
      "Epoch 23/60\n",
      "6471/6471 [==============================] - 135s 21ms/step - loss: 0.2411 - accuracy: 0.9034 - auc_pr: 0.9483 - precision_7: 0.8772 - recall_7: 0.9381 - val_loss: 0.4352 - val_accuracy: 0.8675 - val_auc_pr: 0.8591 - val_precision_7: 0.9178 - val_recall_7: 0.8074\n",
      "Epoch 24/60\n",
      "6471/6471 [==============================] - 133s 21ms/step - loss: 0.2401 - accuracy: 0.9042 - auc_pr: 0.9484 - precision_7: 0.8773 - recall_7: 0.9398 - val_loss: 0.4314 - val_accuracy: 0.8407 - val_auc_pr: 0.8785 - val_precision_7: 0.9115 - val_recall_7: 0.7546\n",
      "Epoch 25/60\n",
      "6471/6471 [==============================] - 134s 21ms/step - loss: 0.2388 - accuracy: 0.9048 - auc_pr: 0.9491 - precision_7: 0.8778 - recall_7: 0.9404 - val_loss: 0.4312 - val_accuracy: 0.8698 - val_auc_pr: 0.8625 - val_precision_7: 0.9236 - val_recall_7: 0.8062\n",
      "Epoch 26/60\n",
      "6471/6471 [==============================] - 133s 21ms/step - loss: 0.2377 - accuracy: 0.9056 - auc_pr: 0.9491 - precision_7: 0.8785 - recall_7: 0.9413 - val_loss: 0.4776 - val_accuracy: 0.8346 - val_auc_pr: 0.8448 - val_precision_7: 0.9229 - val_recall_7: 0.7302\n",
      "Epoch 27/60\n",
      "6471/6471 [==============================] - 133s 21ms/step - loss: 0.2358 - accuracy: 0.9062 - auc_pr: 0.9498 - precision_7: 0.8789 - recall_7: 0.9423 - val_loss: 0.4275 - val_accuracy: 0.8808 - val_auc_pr: 0.8752 - val_precision_7: 0.9264 - val_recall_7: 0.8273\n",
      "Epoch 28/60\n",
      "6471/6471 [==============================] - 135s 21ms/step - loss: 0.2351 - accuracy: 0.9065 - auc_pr: 0.9502 - precision_7: 0.8794 - recall_7: 0.9422 - val_loss: 0.4460 - val_accuracy: 0.8410 - val_auc_pr: 0.8488 - val_precision_7: 0.9017 - val_recall_7: 0.7655\n",
      "Epoch 29/60\n",
      "6471/6471 [==============================] - 134s 21ms/step - loss: 0.2342 - accuracy: 0.9072 - auc_pr: 0.9503 - precision_7: 0.8800 - recall_7: 0.9429 - val_loss: 0.5003 - val_accuracy: 0.7533 - val_auc_pr: 0.8644 - val_precision_7: 0.9253 - val_recall_7: 0.5512\n",
      "Epoch 30/60\n",
      "6471/6471 [==============================] - 135s 21ms/step - loss: 0.2332 - accuracy: 0.9077 - auc_pr: 0.9507 - precision_7: 0.8801 - recall_7: 0.9441 - val_loss: 0.4899 - val_accuracy: 0.7994 - val_auc_pr: 0.8556 - val_precision_7: 0.9251 - val_recall_7: 0.6516\n",
      "Epoch 31/60\n",
      "6471/6471 [==============================] - 136s 21ms/step - loss: 0.2314 - accuracy: 0.9085 - auc_pr: 0.9511 - precision_7: 0.8813 - recall_7: 0.9440 - val_loss: 0.4390 - val_accuracy: 0.8763 - val_auc_pr: 0.8697 - val_precision_7: 0.9195 - val_recall_7: 0.8248\n",
      "Epoch 32/60\n",
      "6471/6471 [==============================] - 136s 21ms/step - loss: 0.2308 - accuracy: 0.9090 - auc_pr: 0.9509 - precision_7: 0.8818 - recall_7: 0.9446 - val_loss: 0.4779 - val_accuracy: 0.8340 - val_auc_pr: 0.8601 - val_precision_7: 0.9208 - val_recall_7: 0.7307\n",
      "Epoch 33/60\n",
      "6471/6471 [==============================] - 134s 21ms/step - loss: 0.2299 - accuracy: 0.9093 - auc_pr: 0.9518 - precision_7: 0.8822 - recall_7: 0.9448 - val_loss: 0.5229 - val_accuracy: 0.7570 - val_auc_pr: 0.8468 - val_precision_7: 0.9154 - val_recall_7: 0.5664\n",
      "Epoch 34/60\n",
      "6471/6471 [==============================] - 133s 20ms/step - loss: 0.2297 - accuracy: 0.9094 - auc_pr: 0.9517 - precision_7: 0.8818 - recall_7: 0.9457 - val_loss: 0.5199 - val_accuracy: 0.7924 - val_auc_pr: 0.8615 - val_precision_7: 0.9332 - val_recall_7: 0.6300\n",
      "Epoch 35/60\n",
      "6471/6471 [==============================] - 134s 21ms/step - loss: 0.2277 - accuracy: 0.9104 - auc_pr: 0.9523 - precision_7: 0.8831 - recall_7: 0.9460 - val_loss: 0.4081 - val_accuracy: 0.8750 - val_auc_pr: 0.8999 - val_precision_7: 0.9117 - val_recall_7: 0.8306\n",
      "Epoch 36/60\n",
      "6471/6471 [==============================] - 134s 21ms/step - loss: 0.2271 - accuracy: 0.9106 - auc_pr: 0.9528 - precision_7: 0.8831 - recall_7: 0.9465 - val_loss: 0.5361 - val_accuracy: 0.8010 - val_auc_pr: 0.8364 - val_precision_7: 0.9232 - val_recall_7: 0.6566\n",
      "Epoch 37/60\n",
      "6471/6471 [==============================] - 136s 21ms/step - loss: 0.2263 - accuracy: 0.9115 - auc_pr: 0.9527 - precision_7: 0.8842 - recall_7: 0.9469 - val_loss: 0.5200 - val_accuracy: 0.7581 - val_auc_pr: 0.8646 - val_precision_7: 0.9275 - val_recall_7: 0.5599\n",
      "Epoch 38/60\n",
      "6471/6471 [==============================] - 136s 21ms/step - loss: 0.2258 - accuracy: 0.9117 - auc_pr: 0.9526 - precision_7: 0.8839 - recall_7: 0.9480 - val_loss: 0.5939 - val_accuracy: 0.6204 - val_auc_pr: 0.8491 - val_precision_7: 0.8838 - val_recall_7: 0.2772\n",
      "Epoch 39/60\n",
      "6471/6471 [==============================] - 136s 21ms/step - loss: 0.2253 - accuracy: 0.9115 - auc_pr: 0.9531 - precision_7: 0.8834 - recall_7: 0.9483 - val_loss: 0.4907 - val_accuracy: 0.8378 - val_auc_pr: 0.8306 - val_precision_7: 0.9082 - val_recall_7: 0.7515\n",
      "Epoch 40/60\n",
      "6471/6471 [==============================] - 133s 21ms/step - loss: 0.2239 - accuracy: 0.9126 - auc_pr: 0.9536 - precision_7: 0.8845 - recall_7: 0.9491 - val_loss: 0.5199 - val_accuracy: 0.7655 - val_auc_pr: 0.8499 - val_precision_7: 0.9254 - val_recall_7: 0.5776\n",
      "Epoch 41/60\n",
      "6471/6471 [==============================] - 135s 21ms/step - loss: 0.2233 - accuracy: 0.9127 - auc_pr: 0.9534 - precision_7: 0.8848 - recall_7: 0.9490 - val_loss: 0.5635 - val_accuracy: 0.6895 - val_auc_pr: 0.8535 - val_precision_7: 0.9150 - val_recall_7: 0.4177\n",
      "Epoch 42/60\n",
      "6471/6471 [==============================] - 134s 21ms/step - loss: 0.2223 - accuracy: 0.9134 - auc_pr: 0.9538 - precision_7: 0.8856 - recall_7: 0.9494 - val_loss: 0.4363 - val_accuracy: 0.8818 - val_auc_pr: 0.8849 - val_precision_7: 0.9251 - val_recall_7: 0.8309\n",
      "Epoch 43/60\n",
      "6471/6471 [==============================] - 134s 21ms/step - loss: 0.2219 - accuracy: 0.9136 - auc_pr: 0.9539 - precision_7: 0.8858 - recall_7: 0.9496 - val_loss: 0.6107 - val_accuracy: 0.5531 - val_auc_pr: 0.8622 - val_precision_7: 0.8318 - val_recall_7: 0.1330\n",
      "Epoch 44/60\n",
      "6471/6471 [==============================] - 136s 21ms/step - loss: 0.2206 - accuracy: 0.9141 - auc_pr: 0.9543 - precision_7: 0.8862 - recall_7: 0.9501 - val_loss: 0.4800 - val_accuracy: 0.8200 - val_auc_pr: 0.8664 - val_precision_7: 0.9333 - val_recall_7: 0.6894\n",
      "Epoch 45/60\n",
      "6471/6471 [==============================] - 134s 21ms/step - loss: 0.2198 - accuracy: 0.9143 - auc_pr: 0.9547 - precision_7: 0.8861 - recall_7: 0.9509 - val_loss: 0.6200 - val_accuracy: 0.6383 - val_auc_pr: 0.8676 - val_precision_7: 0.9176 - val_recall_7: 0.3039\n",
      "Epoch 46/60\n",
      "6471/6471 [==============================] - 136s 21ms/step - loss: 0.2195 - accuracy: 0.9147 - auc_pr: 0.9546 - precision_7: 0.8864 - recall_7: 0.9513 - val_loss: 0.5828 - val_accuracy: 0.6667 - val_auc_pr: 0.8368 - val_precision_7: 0.9006 - val_recall_7: 0.3748\n",
      "Epoch 47/60\n",
      "6471/6471 [==============================] - 136s 21ms/step - loss: 0.2184 - accuracy: 0.9151 - auc_pr: 0.9550 - precision_7: 0.8869 - recall_7: 0.9516 - val_loss: 0.5875 - val_accuracy: 0.7067 - val_auc_pr: 0.8519 - val_precision_7: 0.9216 - val_recall_7: 0.4520\n",
      "Epoch 48/60\n",
      "6471/6471 [==============================] - 134s 21ms/step - loss: 0.2182 - accuracy: 0.9154 - auc_pr: 0.9550 - precision_7: 0.8874 - recall_7: 0.9516 - val_loss: 0.5380 - val_accuracy: 0.7489 - val_auc_pr: 0.8662 - val_precision_7: 0.9362 - val_recall_7: 0.5342\n",
      "Epoch 49/60\n",
      "6471/6471 [==============================] - 135s 21ms/step - loss: 0.2182 - accuracy: 0.9154 - auc_pr: 0.9551 - precision_7: 0.8870 - recall_7: 0.9521 - val_loss: 0.5313 - val_accuracy: 0.8065 - val_auc_pr: 0.8492 - val_precision_7: 0.9259 - val_recall_7: 0.6664\n",
      "Epoch 50/60\n",
      "6471/6471 [==============================] - 134s 21ms/step - loss: 0.2168 - accuracy: 0.9160 - auc_pr: 0.9555 - precision_7: 0.8879 - recall_7: 0.9523 - val_loss: 0.4825 - val_accuracy: 0.8366 - val_auc_pr: 0.8701 - val_precision_7: 0.9276 - val_recall_7: 0.7302\n",
      "Epoch 51/60\n",
      "6471/6471 [==============================] - 134s 21ms/step - loss: 0.2165 - accuracy: 0.9158 - auc_pr: 0.9556 - precision_7: 0.8872 - recall_7: 0.9527 - val_loss: 0.5728 - val_accuracy: 0.6844 - val_auc_pr: 0.8611 - val_precision_7: 0.9209 - val_recall_7: 0.4035\n",
      "Epoch 52/60\n",
      "6471/6471 [==============================] - 135s 21ms/step - loss: 0.2159 - accuracy: 0.9165 - auc_pr: 0.9557 - precision_7: 0.8878 - recall_7: 0.9535 - val_loss: 0.5946 - val_accuracy: 0.6402 - val_auc_pr: 0.8667 - val_precision_7: 0.9139 - val_recall_7: 0.3096\n",
      "Epoch 53/60\n",
      "6471/6471 [==============================] - 131s 20ms/step - loss: 0.2154 - accuracy: 0.9166 - auc_pr: 0.9562 - precision_7: 0.8886 - recall_7: 0.9526 - val_loss: 0.5774 - val_accuracy: 0.7378 - val_auc_pr: 0.8473 - val_precision_7: 0.9258 - val_recall_7: 0.5169\n",
      "Epoch 54/60\n",
      "6471/6471 [==============================] - 131s 20ms/step - loss: 0.2152 - accuracy: 0.9164 - auc_pr: 0.9560 - precision_7: 0.8880 - recall_7: 0.9531 - val_loss: 0.5375 - val_accuracy: 0.8191 - val_auc_pr: 0.8852 - val_precision_7: 0.9543 - val_recall_7: 0.6703\n",
      "Epoch 55/60\n",
      "6471/6471 [==============================] - 131s 20ms/step - loss: 0.2144 - accuracy: 0.9169 - auc_pr: 0.9563 - precision_7: 0.8885 - recall_7: 0.9534 - val_loss: 0.6321 - val_accuracy: 0.6218 - val_auc_pr: 0.8453 - val_precision_7: 0.8909 - val_recall_7: 0.2776\n",
      "Epoch 56/60\n",
      "6471/6471 [==============================] - 133s 21ms/step - loss: 0.2147 - accuracy: 0.9169 - auc_pr: 0.9565 - precision_7: 0.8886 - recall_7: 0.9534 - val_loss: 0.5842 - val_accuracy: 0.7575 - val_auc_pr: 0.8093 - val_precision_7: 0.8878 - val_recall_7: 0.5896\n",
      "Epoch 57/60\n",
      "6471/6471 [==============================] - 134s 21ms/step - loss: 0.2135 - accuracy: 0.9174 - auc_pr: 0.9566 - precision_7: 0.8892 - recall_7: 0.9535 - val_loss: 0.5617 - val_accuracy: 0.7750 - val_auc_pr: 0.8278 - val_precision_7: 0.9046 - val_recall_7: 0.6149\n",
      "Epoch 58/60\n",
      "6471/6471 [==============================] - 135s 21ms/step - loss: 0.2133 - accuracy: 0.9177 - auc_pr: 0.9565 - precision_7: 0.8904 - recall_7: 0.9526 - val_loss: 0.6029 - val_accuracy: 0.6776 - val_auc_pr: 0.8304 - val_precision_7: 0.9006 - val_recall_7: 0.3992\n",
      "Epoch 59/60\n",
      "6471/6471 [==============================] - 134s 21ms/step - loss: 0.2121 - accuracy: 0.9181 - auc_pr: 0.9575 - precision_7: 0.8911 - recall_7: 0.9525 - val_loss: 0.5454 - val_accuracy: 0.7262 - val_auc_pr: 0.8626 - val_precision_7: 0.9267 - val_recall_7: 0.4914\n",
      "Epoch 60/60\n",
      "6471/6471 [==============================] - 135s 21ms/step - loss: 0.2112 - accuracy: 0.9185 - auc_pr: 0.9572 - precision_7: 0.8917 - recall_7: 0.9527 - val_loss: 0.5854 - val_accuracy: 0.6460 - val_auc_pr: 0.8565 - val_precision_7: 0.9042 - val_recall_7: 0.3266\n"
     ]
    }
   ],
   "source": [
    "#Neral Network rulu 128 - sigmoid 1 :  accuracy: 0.8977 - auc_pr: 0.9452 - precision: 0.8667 - recall: 0.9401 - val_loss: 1.4164 - val_accuracy: 0.7669 - val_auc_pr: 0.8374 - val_precision: 0.8515 - val_recall: 0.6464\n",
    "# relu 128 64 8- sigmoid 1 :  loss: 0.2566 - accuracy: 0.8933 - auc_pr: 0.9432 - precision_1: 0.8582 - recall_1: 0.9424 - val_loss: 0.5065 - val_accuracy: 0.8888 - val_auc_pr: 0.8797 - val_precision_1: 0.8719 - val_recall_1: 0.9116\n",
    "# relu 128 64 - sigmoid 1 : loss: 0.2392 - accuracy: 0.9009 - auc_pr: 0.9505 - precision: 0.8639 - recall: 0.9518 - val_loss: 0.9174 - val_accuracy: 0.8723 - val_auc_pr: 0.8569 - val_precision: 0.8740 - val_recall: 0.8700\n",
    "# relu 3*256 - sigmoid 1 : Best Index: 33, Best Accuracy: 0.8907231688499451, Best AUC PR: 0.8968896269798279, Best Precision: 0.8906420469284058, Best Recall: 0.8908270001411438\n",
    "# gelu 3*256 sigmoid 1: Best Index: 7, Best Accuracy: 0.8779525756835938, Best AUC PR: 0.9052767753601074, Best Precision: 0.8704900741577148, Best Recall: 0.8880236744880676\n",
    "# relu softplus 128 sigmoid 1: Best Index: 20, Best Accuracy: 0.8921507596969604, Best AUC PR: 0.8492059111595154, Best Precision: 0.8752608299255371, Best Recall: 0.9146550297737122\n",
    "print(\"bn\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=60, batch_size=128, verbose=1, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Index: 20, Best Accuracy: 0.8921507596969604, Best AUC PR: 0.8492059111595154, Best Precision: 0.8752608299255371, Best Recall: 0.9146550297737122\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "best_index = np.argmax(history.history['val_accuracy'])\n",
    "best_accuracy = history.history['val_accuracy'][best_index]\n",
    "best_auc_pr = history.history['val_auc_pr'][best_index]\n",
    "best_precision = history.history['val_precision_6'][best_index]\n",
    "best_recall = history.history['val_recall_6'][best_index]\n",
    "print(f\"Best Index: {best_index}, Best Accuracy: {best_accuracy}, Best AUC PR: {best_auc_pr}, Best Precision: {best_precision}, Best Recall: {best_recall}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "12942/12942 [==============================] - 180s 14ms/step - loss: 0.4002 - accuracy: 0.8249 - auc_pr: 0.8770 - precision: 0.7905 - recall: 0.8842 - val_loss: 0.3608 - val_accuracy: 0.8402 - val_auc_pr: 0.9075 - val_precision: 0.8010 - val_recall: 0.9053\n",
      "Epoch 2/20\n",
      "12942/12942 [==============================] - 180s 14ms/step - loss: 0.3627 - accuracy: 0.8449 - auc_pr: 0.9003 - precision: 0.8099 - recall: 0.9013 - val_loss: 0.4032 - val_accuracy: 0.8003 - val_auc_pr: 0.8978 - val_precision: 0.8011 - val_recall: 0.7990\n",
      "Epoch 3/20\n",
      "12942/12942 [==============================] - 180s 14ms/step - loss: 0.3491 - accuracy: 0.8517 - auc_pr: 0.9074 - precision: 0.8164 - recall: 0.9074 - val_loss: 0.4449 - val_accuracy: 0.7987 - val_auc_pr: 0.8975 - val_precision: 0.8132 - val_recall: 0.7755\n",
      "Epoch 4/20\n",
      "12942/12942 [==============================] - 175s 14ms/step - loss: 0.3422 - accuracy: 0.8543 - auc_pr: 0.9104 - precision: 0.8187 - recall: 0.9102 - val_loss: 0.4237 - val_accuracy: 0.8087 - val_auc_pr: 0.8868 - val_precision: 0.8147 - val_recall: 0.7993\n",
      "Epoch 5/20\n",
      "12942/12942 [==============================] - 172s 13ms/step - loss: 0.3361 - accuracy: 0.8565 - auc_pr: 0.9132 - precision: 0.8206 - recall: 0.9125 - val_loss: 0.4542 - val_accuracy: 0.8165 - val_auc_pr: 0.9109 - val_precision: 0.8282 - val_recall: 0.7987\n",
      "Epoch 6/20\n",
      "12942/12942 [==============================] - 174s 13ms/step - loss: 0.3341 - accuracy: 0.8577 - auc_pr: 0.9141 - precision: 0.8216 - recall: 0.9138 - val_loss: 0.4962 - val_accuracy: 0.8069 - val_auc_pr: 0.8952 - val_precision: 0.8234 - val_recall: 0.7813\n",
      "Epoch 7/20\n",
      "12942/12942 [==============================] - 176s 14ms/step - loss: 0.3304 - accuracy: 0.8590 - auc_pr: 0.9157 - precision: 0.8224 - recall: 0.9158 - val_loss: 0.5205 - val_accuracy: 0.7996 - val_auc_pr: 0.8740 - val_precision: 0.8233 - val_recall: 0.7630\n",
      "Epoch 8/20\n",
      "12942/12942 [==============================] - 175s 13ms/step - loss: 0.3282 - accuracy: 0.8596 - auc_pr: 0.9171 - precision: 0.8227 - recall: 0.9168 - val_loss: 0.4987 - val_accuracy: 0.8332 - val_auc_pr: 0.9036 - val_precision: 0.8350 - val_recall: 0.8306\n",
      "Epoch 9/20\n",
      "12942/12942 [==============================] - 173s 13ms/step - loss: 0.3259 - accuracy: 0.8603 - auc_pr: 0.9177 - precision: 0.8231 - recall: 0.9180 - val_loss: 0.5611 - val_accuracy: 0.7962 - val_auc_pr: 0.8853 - val_precision: 0.8241 - val_recall: 0.7532\n",
      "Epoch 10/20\n",
      "12942/12942 [==============================] - 176s 14ms/step - loss: 0.3241 - accuracy: 0.8609 - auc_pr: 0.9184 - precision: 0.8233 - recall: 0.9191 - val_loss: 0.6892 - val_accuracy: 0.7386 - val_auc_pr: 0.8523 - val_precision: 0.7952 - val_recall: 0.6426\n",
      "Epoch 11/20\n",
      "12942/12942 [==============================] - 179s 14ms/step - loss: 0.3237 - accuracy: 0.8614 - auc_pr: 0.9184 - precision: 0.8233 - recall: 0.9204 - val_loss: 0.6400 - val_accuracy: 0.7649 - val_auc_pr: 0.8579 - val_precision: 0.8088 - val_recall: 0.6939\n",
      "Epoch 12/20\n",
      "12942/12942 [==============================] - 177s 14ms/step - loss: 0.3218 - accuracy: 0.8620 - auc_pr: 0.9195 - precision: 0.8234 - recall: 0.9217 - val_loss: 0.6707 - val_accuracy: 0.7457 - val_auc_pr: 0.8496 - val_precision: 0.7962 - val_recall: 0.6604\n",
      "Epoch 13/20\n",
      "12942/12942 [==============================] - 176s 14ms/step - loss: 0.3210 - accuracy: 0.8617 - auc_pr: 0.9197 - precision: 0.8233 - recall: 0.9211 - val_loss: 0.6782 - val_accuracy: 0.7435 - val_auc_pr: 0.8558 - val_precision: 0.8025 - val_recall: 0.6458\n",
      "Epoch 14/20\n",
      "12942/12942 [==============================] - 174s 13ms/step - loss: 0.3199 - accuracy: 0.8624 - auc_pr: 0.9200 - precision: 0.8239 - recall: 0.9217 - val_loss: 0.7439 - val_accuracy: 0.7466 - val_auc_pr: 0.8524 - val_precision: 0.8038 - val_recall: 0.6524\n",
      "Epoch 15/20\n",
      "12942/12942 [==============================] - 179s 14ms/step - loss: 0.3192 - accuracy: 0.8633 - auc_pr: 0.9203 - precision: 0.8253 - recall: 0.9216 - val_loss: 0.8992 - val_accuracy: 0.7468 - val_auc_pr: 0.8329 - val_precision: 0.8097 - val_recall: 0.6453\n",
      "Epoch 16/20\n",
      "12942/12942 [==============================] - 178s 14ms/step - loss: 0.3177 - accuracy: 0.8636 - auc_pr: 0.9211 - precision: 0.8248 - recall: 0.9231 - val_loss: 0.8677 - val_accuracy: 0.7451 - val_auc_pr: 0.8380 - val_precision: 0.8012 - val_recall: 0.6520\n",
      "Epoch 17/20\n",
      "12942/12942 [==============================] - 176s 14ms/step - loss: 0.3174 - accuracy: 0.8640 - auc_pr: 0.9211 - precision: 0.8252 - recall: 0.9235 - val_loss: 0.7886 - val_accuracy: 0.7373 - val_auc_pr: 0.8614 - val_precision: 0.8007 - val_recall: 0.6319\n",
      "Epoch 18/20\n",
      "12942/12942 [==============================] - 176s 14ms/step - loss: 0.3162 - accuracy: 0.8643 - auc_pr: 0.9217 - precision: 0.8260 - recall: 0.9231 - val_loss: 0.7531 - val_accuracy: 0.7468 - val_auc_pr: 0.8472 - val_precision: 0.8040 - val_recall: 0.6528\n",
      "Epoch 19/20\n",
      "12942/12942 [==============================] - 179s 14ms/step - loss: 0.3157 - accuracy: 0.8643 - auc_pr: 0.9219 - precision: 0.8262 - recall: 0.9226 - val_loss: 0.9177 - val_accuracy: 0.7400 - val_auc_pr: 0.8265 - val_precision: 0.8021 - val_recall: 0.6373\n",
      "Epoch 20/20\n",
      "12942/12942 [==============================] - 177s 14ms/step - loss: 0.3145 - accuracy: 0.8647 - auc_pr: 0.9224 - precision: 0.8262 - recall: 0.9237 - val_loss: 0.8626 - val_accuracy: 0.7376 - val_auc_pr: 0.8514 - val_precision: 0.8005 - val_recall: 0.6331\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22c8d22c1c0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the function\n",
    "def create_minimum_ffn(input_dim, output_dim, scale=1, middle_function='sigmoid'):\n",
    "    # Clear the session\n",
    "    K.clear_session()\n",
    "    layer1_size=16\n",
    "    # Create the model\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "    # Batch normalization for input\n",
    "    normalized_input = input_layer\n",
    "\n",
    "    x = Dense(layer1_size*scale)(normalized_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.7)(x)\n",
    "\n",
    "    x = Dense(8*scale)(normalized_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(middle_function)(x)\n",
    "    x = Dropout(0.6)(x)\n",
    "\n",
    "    x = Dense(4*scale)(normalized_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.6)(x)\n",
    "  \n",
    "    x = Dense(64)(normalized_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    output_layer = Dense(8)(x)\n",
    "    output_layer = Dense(1, activation='sigmoid')(output_layer)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',AUC(curve='PR', name='auc_pr'),Precision(), Recall()])\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "#def create_minimum_ffn(input_dim, output_dim, scale=1, middle_function='sigmoid'):\n",
    "\n",
    "model = create_minimum_ffn(X_train.shape[1], 1)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_scaled, y_train, epochs=20, batch_size=64, verbose=1, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the function\n",
    "def create_minidirect_multipath_model(input_dim, output_dim, paths):\n",
    "    # Clear the session\n",
    "    K.clear_session()\n",
    "\n",
    "    # Create the model\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "    # Batch normalization for input\n",
    "    normalized_input = input_layer\n",
    "\n",
    "\n",
    "    x = normalized_input\n",
    "\n",
    "    # Define paths\n",
    "    path_outputs = []\n",
    "    for path in paths:\n",
    "        activation_function = path[0]\n",
    "        dropout_rate = path[1]\n",
    "        layer_sizes = path[2:]\n",
    "        path_output = x\n",
    "        for size in layer_sizes:\n",
    "            path_output = Dense(size)(path_output)\n",
    "            path_output = BatchNormalization()(path_output)\n",
    "            path_output = Activation(activation_function)(path_output)\n",
    "            path_output = Dropout(dropout_rate)(path_output)\n",
    "        path_outputs.append(path_output)\n",
    "\n",
    "    # Concatenate paths\n",
    "    concat = concatenate(path_outputs)\n",
    "\n",
    "    output_layer = Dense(64)(concat)\n",
    "    output_layer = BatchNormalization()(output_layer)\n",
    "    output_layer = Dropout(dropout_rate)(output_layer)\n",
    "    output_layer = Dense(4)(output_layer)\n",
    "    output_layer = Dense(1, activation='sigmoid')(output_layer)\n",
    "    \n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    # Compile the model with Huber loss function and AdamW optimizer\n",
    "    # optimizer = AdamW(weight_decay=1e-4)\n",
    "    #model.compile(loss=Huber(), optimizer=optimizer)    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',AUC(curve='PR', name='auc_pr'),Precision(), Recall()])\n",
    "\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6471/6471 [==============================] - 252s 38ms/step - loss: 0.4125 - accuracy: 0.8185 - auc_pr: 0.8673 - precision: 0.7840 - recall: 0.8793 - val_loss: 0.3169 - val_accuracy: 0.8589 - val_auc_pr: 0.9510 - val_precision: 0.8121 - val_recall: 0.9340\n",
      "Epoch 2/20\n",
      "6471/6471 [==============================] - 242s 37ms/step - loss: 0.3556 - accuracy: 0.8487 - auc_pr: 0.9019 - precision: 0.8180 - recall: 0.8970 - val_loss: 0.3169 - val_accuracy: 0.8676 - val_auc_pr: 0.9376 - val_precision: 0.8291 - val_recall: 0.9260\n",
      "Epoch 3/20\n",
      "6471/6471 [==============================] - 243s 38ms/step - loss: 0.3337 - accuracy: 0.8591 - auc_pr: 0.9133 - precision: 0.8299 - recall: 0.9034 - val_loss: 0.3307 - val_accuracy: 0.8581 - val_auc_pr: 0.9150 - val_precision: 0.8315 - val_recall: 0.8981\n",
      "Epoch 4/20\n",
      "6471/6471 [==============================] - 242s 37ms/step - loss: 0.3197 - accuracy: 0.8650 - auc_pr: 0.9202 - precision: 0.8357 - recall: 0.9086 - val_loss: 0.3439 - val_accuracy: 0.8646 - val_auc_pr: 0.9113 - val_precision: 0.8382 - val_recall: 0.9035\n",
      "Epoch 5/20\n",
      "6471/6471 [==============================] - 246s 38ms/step - loss: 0.3106 - accuracy: 0.8688 - auc_pr: 0.9241 - precision: 0.8398 - recall: 0.9114 - val_loss: 0.4317 - val_accuracy: 0.8321 - val_auc_pr: 0.8781 - val_precision: 0.8347 - val_recall: 0.8282\n",
      "Epoch 6/20\n",
      "6471/6471 [==============================] - 247s 38ms/step - loss: 0.3037 - accuracy: 0.8723 - auc_pr: 0.9270 - precision: 0.8432 - recall: 0.9146 - val_loss: 0.3746 - val_accuracy: 0.8611 - val_auc_pr: 0.9047 - val_precision: 0.8414 - val_recall: 0.8898\n",
      "Epoch 7/20\n",
      "6471/6471 [==============================] - 245s 38ms/step - loss: 0.2984 - accuracy: 0.8746 - auc_pr: 0.9290 - precision: 0.8454 - recall: 0.9168 - val_loss: 0.3936 - val_accuracy: 0.8695 - val_auc_pr: 0.9024 - val_precision: 0.8478 - val_recall: 0.9008\n",
      "Epoch 8/20\n",
      "6471/6471 [==============================] - 244s 38ms/step - loss: 0.2946 - accuracy: 0.8763 - auc_pr: 0.9305 - precision: 0.8470 - recall: 0.9184 - val_loss: 0.4763 - val_accuracy: 0.8619 - val_auc_pr: 0.8614 - val_precision: 0.8526 - val_recall: 0.8750\n",
      "Epoch 9/20\n",
      "6471/6471 [==============================] - 245s 38ms/step - loss: 0.2905 - accuracy: 0.8781 - auc_pr: 0.9322 - precision: 0.8489 - recall: 0.9199 - val_loss: 0.4739 - val_accuracy: 0.8443 - val_auc_pr: 0.8635 - val_precision: 0.8480 - val_recall: 0.8390\n",
      "Epoch 10/20\n",
      "6471/6471 [==============================] - 243s 38ms/step - loss: 0.2870 - accuracy: 0.8796 - auc_pr: 0.9335 - precision: 0.8505 - recall: 0.9210 - val_loss: 0.4969 - val_accuracy: 0.7960 - val_auc_pr: 0.8530 - val_precision: 0.8325 - val_recall: 0.7411\n",
      "Epoch 11/20\n",
      "6471/6471 [==============================] - 246s 38ms/step - loss: 0.2850 - accuracy: 0.8808 - auc_pr: 0.9344 - precision: 0.8516 - recall: 0.9223 - val_loss: 0.4722 - val_accuracy: 0.8553 - val_auc_pr: 0.8879 - val_precision: 0.8500 - val_recall: 0.8627\n",
      "Epoch 12/20\n",
      "6471/6471 [==============================] - 242s 37ms/step - loss: 0.2823 - accuracy: 0.8820 - auc_pr: 0.9354 - precision: 0.8532 - recall: 0.9227 - val_loss: 0.5360 - val_accuracy: 0.8435 - val_auc_pr: 0.8527 - val_precision: 0.8442 - val_recall: 0.8424\n",
      "Epoch 13/20\n",
      "6471/6471 [==============================] - 247s 38ms/step - loss: 0.2798 - accuracy: 0.8828 - auc_pr: 0.9366 - precision: 0.8540 - recall: 0.9235 - val_loss: 0.5902 - val_accuracy: 0.8281 - val_auc_pr: 0.8553 - val_precision: 0.8467 - val_recall: 0.8014\n",
      "Epoch 14/20\n",
      "6471/6471 [==============================] - 239s 37ms/step - loss: 0.2783 - accuracy: 0.8841 - auc_pr: 0.9371 - precision: 0.8555 - recall: 0.9243 - val_loss: 0.5719 - val_accuracy: 0.8488 - val_auc_pr: 0.8684 - val_precision: 0.8573 - val_recall: 0.8368\n",
      "Epoch 15/20\n",
      "6471/6471 [==============================] - 242s 37ms/step - loss: 0.2757 - accuracy: 0.8849 - auc_pr: 0.9381 - precision: 0.8563 - recall: 0.9249 - val_loss: 0.6891 - val_accuracy: 0.7617 - val_auc_pr: 0.8362 - val_precision: 0.8255 - val_recall: 0.6638\n",
      "Epoch 16/20\n",
      "6471/6471 [==============================] - 243s 38ms/step - loss: 0.2736 - accuracy: 0.8859 - auc_pr: 0.9389 - precision: 0.8575 - recall: 0.9257 - val_loss: 0.8215 - val_accuracy: 0.7539 - val_auc_pr: 0.8202 - val_precision: 0.8213 - val_recall: 0.6489\n",
      "Epoch 17/20\n",
      "6471/6471 [==============================] - 239s 37ms/step - loss: 0.2729 - accuracy: 0.8863 - auc_pr: 0.9390 - precision: 0.8577 - recall: 0.9262 - val_loss: 0.6659 - val_accuracy: 0.7619 - val_auc_pr: 0.8344 - val_precision: 0.8237 - val_recall: 0.6666\n",
      "Epoch 18/20\n",
      "6471/6471 [==============================] - 243s 37ms/step - loss: 0.2719 - accuracy: 0.8873 - auc_pr: 0.9395 - precision: 0.8588 - recall: 0.9269 - val_loss: 0.7883 - val_accuracy: 0.7604 - val_auc_pr: 0.8360 - val_precision: 0.8313 - val_recall: 0.6535\n",
      "Epoch 19/20\n",
      "6471/6471 [==============================] - 244s 38ms/step - loss: 0.2705 - accuracy: 0.8876 - auc_pr: 0.9400 - precision: 0.8593 - recall: 0.9269 - val_loss: 0.6907 - val_accuracy: 0.7584 - val_auc_pr: 0.8309 - val_precision: 0.8209 - val_recall: 0.6610\n",
      "Epoch 20/20\n",
      "6471/6471 [==============================] - 242s 37ms/step - loss: 0.2687 - accuracy: 0.8886 - auc_pr: 0.9408 - precision: 0.8605 - recall: 0.9277 - val_loss: 0.7859 - val_accuracy: 0.7623 - val_auc_pr: 0.8231 - val_precision: 0.8228 - val_recall: 0.6686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22c90a0f9a0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#def create_minidirect_multipath_model(input_dim, output_dim, paths):\n",
    "paths=[['relu', 0.5, 32, 8]]*6#, ['relu', 0.5, 16, 8], ['relu', 0.2, 8, 4],['relu', 0.5,64,16, 8, 4],['relu', 0.5,64,16, 8, 4]]\n",
    "model = create_minidirect_multipath_model(X_train.shape[1], 1,paths)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_scaled, y_train, epochs=20, batch_size=128, verbose=1, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6471/6471 [==============================] - 219s 33ms/step - loss: 0.4130 - accuracy: 0.8193 - auc_pr: 0.8673 - precision: 0.7877 - recall: 0.8744 - val_loss: 0.3565 - val_accuracy: 0.8494 - val_auc_pr: 0.9210 - val_precision: 0.8162 - val_recall: 0.9019\n",
      "Epoch 2/20\n",
      "6471/6471 [==============================] - 210s 32ms/step - loss: 0.3567 - accuracy: 0.8490 - auc_pr: 0.9010 - precision: 0.8224 - recall: 0.8904 - val_loss: 0.3681 - val_accuracy: 0.8549 - val_auc_pr: 0.8917 - val_precision: 0.8321 - val_recall: 0.8892\n",
      "Epoch 3/20\n",
      "6471/6471 [==============================] - 209s 32ms/step - loss: 0.3372 - accuracy: 0.8578 - auc_pr: 0.9117 - precision: 0.8318 - recall: 0.8971 - val_loss: 0.3471 - val_accuracy: 0.8683 - val_auc_pr: 0.9111 - val_precision: 0.8378 - val_recall: 0.9135\n",
      "Epoch 4/20\n",
      "6471/6471 [==============================] - 212s 33ms/step - loss: 0.3263 - accuracy: 0.8631 - auc_pr: 0.9163 - precision: 0.8368 - recall: 0.9020 - val_loss: 0.3750 - val_accuracy: 0.8677 - val_auc_pr: 0.8854 - val_precision: 0.8429 - val_recall: 0.9038\n",
      "Epoch 5/20\n",
      "6471/6471 [==============================] - 210s 32ms/step - loss: 0.3194 - accuracy: 0.8660 - auc_pr: 0.9198 - precision: 0.8393 - recall: 0.9053 - val_loss: 0.3645 - val_accuracy: 0.8696 - val_auc_pr: 0.9045 - val_precision: 0.8465 - val_recall: 0.9029\n",
      "Epoch 6/20\n",
      "6471/6471 [==============================] - 211s 33ms/step - loss: 0.3134 - accuracy: 0.8689 - auc_pr: 0.9223 - precision: 0.8422 - recall: 0.9079 - val_loss: 0.4077 - val_accuracy: 0.8682 - val_auc_pr: 0.8795 - val_precision: 0.8531 - val_recall: 0.8896\n",
      "Epoch 7/20\n",
      "6471/6471 [==============================] - 209s 32ms/step - loss: 0.3090 - accuracy: 0.8699 - auc_pr: 0.9244 - precision: 0.8435 - recall: 0.9085 - val_loss: 0.3840 - val_accuracy: 0.8720 - val_auc_pr: 0.8929 - val_precision: 0.8528 - val_recall: 0.8991\n",
      "Epoch 8/20\n",
      "6471/6471 [==============================] - 208s 32ms/step - loss: 0.3050 - accuracy: 0.8721 - auc_pr: 0.9258 - precision: 0.8452 - recall: 0.9110 - val_loss: 0.3990 - val_accuracy: 0.8697 - val_auc_pr: 0.8932 - val_precision: 0.8546 - val_recall: 0.8910\n",
      "Epoch 9/20\n",
      "6471/6471 [==============================] - 211s 33ms/step - loss: 0.3021 - accuracy: 0.8737 - auc_pr: 0.9274 - precision: 0.8473 - recall: 0.9117 - val_loss: 0.4298 - val_accuracy: 0.8600 - val_auc_pr: 0.8717 - val_precision: 0.8523 - val_recall: 0.8710\n",
      "Epoch 10/20\n",
      "6471/6471 [==============================] - 216s 33ms/step - loss: 0.3004 - accuracy: 0.8743 - auc_pr: 0.9278 - precision: 0.8476 - recall: 0.9127 - val_loss: 0.4106 - val_accuracy: 0.8673 - val_auc_pr: 0.8991 - val_precision: 0.8509 - val_recall: 0.8906\n",
      "Epoch 11/20\n",
      "6471/6471 [==============================] - 214s 33ms/step - loss: 0.2975 - accuracy: 0.8760 - auc_pr: 0.9287 - precision: 0.8493 - recall: 0.9141 - val_loss: 0.4199 - val_accuracy: 0.8635 - val_auc_pr: 0.8850 - val_precision: 0.8544 - val_recall: 0.8764\n",
      "Epoch 12/20\n",
      "6471/6471 [==============================] - 217s 34ms/step - loss: 0.2953 - accuracy: 0.8769 - auc_pr: 0.9294 - precision: 0.8497 - recall: 0.9157 - val_loss: 0.4436 - val_accuracy: 0.8659 - val_auc_pr: 0.8936 - val_precision: 0.8565 - val_recall: 0.8792\n",
      "Epoch 13/20\n",
      "6471/6471 [==============================] - 218s 34ms/step - loss: 0.2934 - accuracy: 0.8773 - auc_pr: 0.9307 - precision: 0.8502 - recall: 0.9160 - val_loss: 0.4814 - val_accuracy: 0.8290 - val_auc_pr: 0.8757 - val_precision: 0.8495 - val_recall: 0.7997\n",
      "Epoch 14/20\n",
      "6471/6471 [==============================] - 221s 34ms/step - loss: 0.2908 - accuracy: 0.8788 - auc_pr: 0.9314 - precision: 0.8512 - recall: 0.9182 - val_loss: 0.4438 - val_accuracy: 0.8782 - val_auc_pr: 0.8906 - val_precision: 0.8650 - val_recall: 0.8964\n",
      "Epoch 15/20\n",
      "6471/6471 [==============================] - 222s 34ms/step - loss: 0.2899 - accuracy: 0.8791 - auc_pr: 0.9318 - precision: 0.8516 - recall: 0.9183 - val_loss: 0.4732 - val_accuracy: 0.8644 - val_auc_pr: 0.8816 - val_precision: 0.8563 - val_recall: 0.8757\n",
      "Epoch 16/20\n",
      "6471/6471 [==============================] - 222s 34ms/step - loss: 0.2879 - accuracy: 0.8801 - auc_pr: 0.9327 - precision: 0.8522 - recall: 0.9197 - val_loss: 0.4505 - val_accuracy: 0.8748 - val_auc_pr: 0.8994 - val_precision: 0.8686 - val_recall: 0.8832\n",
      "Epoch 17/20\n",
      "6471/6471 [==============================] - 221s 34ms/step - loss: 0.2869 - accuracy: 0.8803 - auc_pr: 0.9330 - precision: 0.8525 - recall: 0.9198 - val_loss: 0.4855 - val_accuracy: 0.8640 - val_auc_pr: 0.8944 - val_precision: 0.8555 - val_recall: 0.8760\n",
      "Epoch 18/20\n",
      "6471/6471 [==============================] - 218s 34ms/step - loss: 0.2848 - accuracy: 0.8812 - auc_pr: 0.9341 - precision: 0.8535 - recall: 0.9205 - val_loss: 0.5112 - val_accuracy: 0.8623 - val_auc_pr: 0.8820 - val_precision: 0.8611 - val_recall: 0.8638\n",
      "Epoch 19/20\n",
      "6471/6471 [==============================] - 221s 34ms/step - loss: 0.2838 - accuracy: 0.8812 - auc_pr: 0.9346 - precision: 0.8532 - recall: 0.9208 - val_loss: 0.5158 - val_accuracy: 0.8490 - val_auc_pr: 0.8762 - val_precision: 0.8645 - val_recall: 0.8277\n",
      "Epoch 20/20\n",
      "6471/6471 [==============================] - 215s 33ms/step - loss: 0.2827 - accuracy: 0.8822 - auc_pr: 0.9347 - precision: 0.8542 - recall: 0.9218 - val_loss: 0.5150 - val_accuracy: 0.8661 - val_auc_pr: 0.8982 - val_precision: 0.8638 - val_recall: 0.8692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22ae2f1ff10>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths=[['relu', 0.4, 32,16, 8]]*3#, ['relu', 0.5, 16, 8], ['relu', 0.2, 8, 4],['relu', 0.5,64,16, 8, 4],['relu', 0.5,64,16, 8, 4]]\n",
    "model = create_minidirect_multipath_model(X_train.shape[1], 1,paths)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_scaled, y_train, epochs=20, batch_size=128, verbose=1, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6471/6471 [==============================] - 293s 44ms/step - loss: 0.4248 - accuracy: 0.8126 - auc_pr: 0.8571 - precision: 0.7758 - recall: 0.8792 - val_loss: 0.3468 - val_accuracy: 0.8432 - val_auc_pr: 0.9386 - val_precision: 0.8012 - val_recall: 0.9129\n",
      "Epoch 2/20\n",
      "6471/6471 [==============================] - 279s 43ms/step - loss: 0.3684 - accuracy: 0.8434 - auc_pr: 0.8935 - precision: 0.8123 - recall: 0.8932 - val_loss: 0.3413 - val_accuracy: 0.8515 - val_auc_pr: 0.9486 - val_precision: 0.8115 - val_recall: 0.9156\n",
      "Epoch 3/20\n",
      "6471/6471 [==============================] - 280s 43ms/step - loss: 0.3471 - accuracy: 0.8535 - auc_pr: 0.9055 - precision: 0.8238 - recall: 0.8993 - val_loss: 0.3375 - val_accuracy: 0.8570 - val_auc_pr: 0.9384 - val_precision: 0.8281 - val_recall: 0.9010\n",
      "Epoch 4/20\n",
      "6471/6471 [==============================] - 278s 43ms/step - loss: 0.3335 - accuracy: 0.8592 - auc_pr: 0.9127 - precision: 0.8301 - recall: 0.9034 - val_loss: 0.3629 - val_accuracy: 0.8576 - val_auc_pr: 0.9124 - val_precision: 0.8302 - val_recall: 0.8989\n",
      "Epoch 5/20\n",
      "6471/6471 [==============================] - 279s 43ms/step - loss: 0.3243 - accuracy: 0.8632 - auc_pr: 0.9169 - precision: 0.8340 - recall: 0.9068 - val_loss: 0.3773 - val_accuracy: 0.8639 - val_auc_pr: 0.8938 - val_precision: 0.8329 - val_recall: 0.9106\n",
      "Epoch 6/20\n",
      "6471/6471 [==============================] - 274s 42ms/step - loss: 0.3182 - accuracy: 0.8657 - auc_pr: 0.9199 - precision: 0.8368 - recall: 0.9085 - val_loss: 0.4031 - val_accuracy: 0.8595 - val_auc_pr: 0.8681 - val_precision: 0.8348 - val_recall: 0.8964\n",
      "Epoch 7/20\n",
      "6471/6471 [==============================] - 274s 42ms/step - loss: 0.3135 - accuracy: 0.8681 - auc_pr: 0.9220 - precision: 0.8391 - recall: 0.9109 - val_loss: 0.4477 - val_accuracy: 0.8132 - val_auc_pr: 0.8488 - val_precision: 0.8227 - val_recall: 0.7985\n",
      "Epoch 8/20\n",
      "6471/6471 [==============================] - 277s 43ms/step - loss: 0.3097 - accuracy: 0.8698 - auc_pr: 0.9235 - precision: 0.8406 - recall: 0.9127 - val_loss: 0.4819 - val_accuracy: 0.8035 - val_auc_pr: 0.8273 - val_precision: 0.8211 - val_recall: 0.7760\n",
      "Epoch 9/20\n",
      "6471/6471 [==============================] - 278s 43ms/step - loss: 0.3058 - accuracy: 0.8712 - auc_pr: 0.9254 - precision: 0.8422 - recall: 0.9136 - val_loss: 0.4628 - val_accuracy: 0.8158 - val_auc_pr: 0.8525 - val_precision: 0.8310 - val_recall: 0.7929\n",
      "Epoch 10/20\n",
      "6471/6471 [==============================] - 276s 43ms/step - loss: 0.3019 - accuracy: 0.8729 - auc_pr: 0.9271 - precision: 0.8439 - recall: 0.9150 - val_loss: 0.4737 - val_accuracy: 0.8169 - val_auc_pr: 0.8634 - val_precision: 0.8341 - val_recall: 0.7911\n",
      "Epoch 11/20\n",
      "6471/6471 [==============================] - 284s 44ms/step - loss: 0.3000 - accuracy: 0.8738 - auc_pr: 0.9276 - precision: 0.8450 - recall: 0.9155 - val_loss: 0.5533 - val_accuracy: 0.7757 - val_auc_pr: 0.8340 - val_precision: 0.8180 - val_recall: 0.7090\n",
      "Epoch 12/20\n",
      "6471/6471 [==============================] - 286s 44ms/step - loss: 0.2972 - accuracy: 0.8752 - auc_pr: 0.9287 - precision: 0.8463 - recall: 0.9170 - val_loss: 0.5359 - val_accuracy: 0.8012 - val_auc_pr: 0.8277 - val_precision: 0.8232 - val_recall: 0.7671\n",
      "Epoch 13/20\n",
      "6471/6471 [==============================] - 272s 42ms/step - loss: 0.2953 - accuracy: 0.8762 - auc_pr: 0.9294 - precision: 0.8472 - recall: 0.9179 - val_loss: 0.6334 - val_accuracy: 0.7612 - val_auc_pr: 0.8256 - val_precision: 0.8198 - val_recall: 0.6696\n",
      "Epoch 14/20\n",
      "6471/6471 [==============================] - 287s 44ms/step - loss: 0.2933 - accuracy: 0.8768 - auc_pr: 0.9303 - precision: 0.8479 - recall: 0.9183 - val_loss: 0.5663 - val_accuracy: 0.8037 - val_auc_pr: 0.8316 - val_precision: 0.8290 - val_recall: 0.7651\n",
      "Epoch 15/20\n",
      "6471/6471 [==============================] - 287s 44ms/step - loss: 0.2908 - accuracy: 0.8775 - auc_pr: 0.9316 - precision: 0.8485 - recall: 0.9191 - val_loss: 0.6641 - val_accuracy: 0.7689 - val_auc_pr: 0.8189 - val_precision: 0.8191 - val_recall: 0.6902\n",
      "Epoch 16/20\n",
      "6471/6471 [==============================] - 286s 44ms/step - loss: 0.2896 - accuracy: 0.8785 - auc_pr: 0.9321 - precision: 0.8498 - recall: 0.9194 - val_loss: 0.6965 - val_accuracy: 0.7642 - val_auc_pr: 0.8216 - val_precision: 0.8229 - val_recall: 0.6733\n",
      "Epoch 17/20\n",
      "6471/6471 [==============================] - 293s 45ms/step - loss: 0.2879 - accuracy: 0.8793 - auc_pr: 0.9326 - precision: 0.8506 - recall: 0.9202 - val_loss: 0.7742 - val_accuracy: 0.7604 - val_auc_pr: 0.8069 - val_precision: 0.8183 - val_recall: 0.6694\n",
      "Epoch 18/20\n",
      "6471/6471 [==============================] - 291s 45ms/step - loss: 0.2864 - accuracy: 0.8798 - auc_pr: 0.9330 - precision: 0.8511 - recall: 0.9208 - val_loss: 0.7288 - val_accuracy: 0.7645 - val_auc_pr: 0.8288 - val_precision: 0.8217 - val_recall: 0.6755\n",
      "Epoch 19/20\n",
      "6471/6471 [==============================] - 291s 45ms/step - loss: 0.2849 - accuracy: 0.8808 - auc_pr: 0.9337 - precision: 0.8522 - recall: 0.9215 - val_loss: 0.6735 - val_accuracy: 0.7820 - val_auc_pr: 0.8281 - val_precision: 0.8242 - val_recall: 0.7169\n",
      "Epoch 20/20\n",
      "6471/6471 [==============================] - 289s 45ms/step - loss: 0.2842 - accuracy: 0.8812 - auc_pr: 0.9338 - precision: 0.8528 - recall: 0.9214 - val_loss: 0.6432 - val_accuracy: 0.8097 - val_auc_pr: 0.8252 - val_precision: 0.8309 - val_recall: 0.7777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22c8a387640>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths=[['relu', 0.5, 32,16, 8]]*5#, ['relu', 0.5, 16, 8], ['relu', 0.2, 8, 4],['relu', 0.5,64,16, 8, 4],['relu', 0.5,64,16, 8, 4]]\n",
    "model = create_minidirect_multipath_model(X_train.shape[1], 1,paths)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_scaled, y_train, epochs=20, batch_size=128, verbose=1, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "809/809 [==============================] - 4s 5ms/step - loss: 0.2328 - accuracy: 0.9045 - auc_pr: 0.9522 - precision: 0.8677 - recall: 0.9546 - val_loss: 0.7155 - val_accuracy: 0.8725 - val_auc_pr: 0.8344 - val_precision: 0.8783 - val_recall: 0.8648\n",
      "Epoch 2/20\n",
      "809/809 [==============================] - 4s 5ms/step - loss: 0.2316 - accuracy: 0.9054 - auc_pr: 0.9528 - precision: 0.8688 - recall: 0.9551 - val_loss: 0.7230 - val_accuracy: 0.8703 - val_auc_pr: 0.8568 - val_precision: 0.8724 - val_recall: 0.8676\n",
      "Epoch 3/20\n",
      "809/809 [==============================] - 4s 5ms/step - loss: 0.2300 - accuracy: 0.9062 - auc_pr: 0.9533 - precision: 0.8694 - recall: 0.9560 - val_loss: 0.7847 - val_accuracy: 0.8459 - val_auc_pr: 0.8638 - val_precision: 0.8787 - val_recall: 0.8026\n",
      "Epoch 4/20\n",
      "809/809 [==============================] - 4s 5ms/step - loss: 0.2282 - accuracy: 0.9070 - auc_pr: 0.9533 - precision: 0.8706 - recall: 0.9562 - val_loss: 0.7663 - val_accuracy: 0.8741 - val_auc_pr: 0.8490 - val_precision: 0.8822 - val_recall: 0.8635\n",
      "Epoch 5/20\n",
      "809/809 [==============================] - 4s 5ms/step - loss: 0.2267 - accuracy: 0.9075 - auc_pr: 0.9541 - precision: 0.8710 - recall: 0.9567 - val_loss: 0.7759 - val_accuracy: 0.8702 - val_auc_pr: 0.8445 - val_precision: 0.8828 - val_recall: 0.8537\n",
      "Epoch 6/20\n",
      "809/809 [==============================] - 4s 5ms/step - loss: 0.2254 - accuracy: 0.9081 - auc_pr: 0.9544 - precision: 0.8716 - recall: 0.9574 - val_loss: 0.8013 - val_accuracy: 0.8708 - val_auc_pr: 0.8399 - val_precision: 0.8785 - val_recall: 0.8605\n",
      "Epoch 7/20\n",
      "809/809 [==============================] - 4s 5ms/step - loss: 0.2240 - accuracy: 0.9089 - auc_pr: 0.9548 - precision: 0.8725 - recall: 0.9577 - val_loss: 0.8198 - val_accuracy: 0.8720 - val_auc_pr: 0.8402 - val_precision: 0.8842 - val_recall: 0.8560\n",
      "Epoch 8/20\n",
      "809/809 [==============================] - 4s 5ms/step - loss: 0.2229 - accuracy: 0.9099 - auc_pr: 0.9549 - precision: 0.8737 - recall: 0.9583 - val_loss: 0.9165 - val_accuracy: 0.8161 - val_auc_pr: 0.8427 - val_precision: 0.8693 - val_recall: 0.7440\n",
      "Epoch 9/20\n",
      "809/809 [==============================] - 4s 5ms/step - loss: 0.2222 - accuracy: 0.9101 - auc_pr: 0.9549 - precision: 0.8737 - recall: 0.9588 - val_loss: 0.9003 - val_accuracy: 0.8592 - val_auc_pr: 0.8509 - val_precision: 0.8853 - val_recall: 0.8254\n",
      "Epoch 10/20\n",
      "809/809 [==============================] - 4s 5ms/step - loss: 0.2207 - accuracy: 0.9110 - auc_pr: 0.9556 - precision: 0.8749 - recall: 0.9592 - val_loss: 0.9615 - val_accuracy: 0.8491 - val_auc_pr: 0.8412 - val_precision: 0.8824 - val_recall: 0.8056\n",
      "Epoch 11/20\n",
      "809/809 [==============================] - 4s 5ms/step - loss: 0.2203 - accuracy: 0.9111 - auc_pr: 0.9556 - precision: 0.8748 - recall: 0.9596 - val_loss: 0.9705 - val_accuracy: 0.8465 - val_auc_pr: 0.8338 - val_precision: 0.8773 - val_recall: 0.8057\n",
      "Epoch 12/20\n",
      "809/809 [==============================] - 4s 5ms/step - loss: 0.2186 - accuracy: 0.9122 - auc_pr: 0.9559 - precision: 0.8762 - recall: 0.9600 - val_loss: 1.0102 - val_accuracy: 0.8309 - val_auc_pr: 0.8268 - val_precision: 0.8796 - val_recall: 0.7669\n",
      "Epoch 13/20\n",
      "809/809 [==============================] - 4s 5ms/step - loss: 0.2181 - accuracy: 0.9120 - auc_pr: 0.9565 - precision: 0.8759 - recall: 0.9599 - val_loss: 1.0673 - val_accuracy: 0.8192 - val_auc_pr: 0.8250 - val_precision: 0.8748 - val_recall: 0.7450\n",
      "Epoch 14/20\n",
      "809/809 [==============================] - 4s 5ms/step - loss: 0.2165 - accuracy: 0.9132 - auc_pr: 0.9567 - precision: 0.8773 - recall: 0.9608 - val_loss: 1.1767 - val_accuracy: 0.8100 - val_auc_pr: 0.8329 - val_precision: 0.8769 - val_recall: 0.7212\n",
      "Epoch 15/20\n",
      "809/809 [==============================] - 4s 5ms/step - loss: 0.2167 - accuracy: 0.9131 - auc_pr: 0.9568 - precision: 0.8770 - recall: 0.9610 - val_loss: 1.0403 - val_accuracy: 0.8307 - val_auc_pr: 0.8721 - val_precision: 0.8797 - val_recall: 0.7661\n",
      "Epoch 16/20\n",
      "809/809 [==============================] - 4s 5ms/step - loss: 0.2147 - accuracy: 0.9139 - auc_pr: 0.9573 - precision: 0.8781 - recall: 0.9612 - val_loss: 1.0867 - val_accuracy: 0.8243 - val_auc_pr: 0.8341 - val_precision: 0.8788 - val_recall: 0.7524\n",
      "Epoch 17/20\n",
      "809/809 [==============================] - 4s 5ms/step - loss: 0.2141 - accuracy: 0.9142 - auc_pr: 0.9574 - precision: 0.8785 - recall: 0.9615 - val_loss: 1.1576 - val_accuracy: 0.8253 - val_auc_pr: 0.8267 - val_precision: 0.8809 - val_recall: 0.7523\n",
      "Epoch 18/20\n",
      "809/809 [==============================] - 4s 5ms/step - loss: 0.2135 - accuracy: 0.9147 - auc_pr: 0.9574 - precision: 0.8790 - recall: 0.9618 - val_loss: 1.2006 - val_accuracy: 0.8228 - val_auc_pr: 0.8259 - val_precision: 0.8796 - val_recall: 0.7481\n",
      "Epoch 19/20\n",
      "809/809 [==============================] - 4s 5ms/step - loss: 0.2133 - accuracy: 0.9150 - auc_pr: 0.9578 - precision: 0.8795 - recall: 0.9617 - val_loss: 1.2654 - val_accuracy: 0.8162 - val_auc_pr: 0.8266 - val_precision: 0.8818 - val_recall: 0.7303\n",
      "Epoch 20/20\n",
      "809/809 [==============================] - 4s 5ms/step - loss: 0.2121 - accuracy: 0.9159 - auc_pr: 0.9578 - precision: 0.8805 - recall: 0.9624 - val_loss: 1.3021 - val_accuracy: 0.8173 - val_auc_pr: 0.8244 - val_precision: 0.8820 - val_recall: 0.7326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayoub\\AppData\\Local\\Temp\\ipykernel_8904\\1314869960.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df2 = pd.concat([results_df2, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1618/1618 [==============================] - 7s 5ms/step - loss: 0.2256 - accuracy: 0.9092 - auc_pr: 0.9539 - precision: 0.8734 - recall: 0.9572 - val_loss: 1.0389 - val_accuracy: 0.8192 - val_auc_pr: 0.8387 - val_precision: 0.8745 - val_recall: 0.7454\n",
      "Epoch 2/20\n",
      "1618/1618 [==============================] - 8s 5ms/step - loss: 0.2213 - accuracy: 0.9112 - auc_pr: 0.9553 - precision: 0.8758 - recall: 0.9584 - val_loss: 1.1448 - val_accuracy: 0.8016 - val_auc_pr: 0.8180 - val_precision: 0.8768 - val_recall: 0.7017\n",
      "Epoch 3/20\n",
      "1618/1618 [==============================] - 8s 5ms/step - loss: 0.2200 - accuracy: 0.9119 - auc_pr: 0.9554 - precision: 0.8767 - recall: 0.9586 - val_loss: 1.1198 - val_accuracy: 0.8260 - val_auc_pr: 0.8185 - val_precision: 0.8813 - val_recall: 0.7536\n",
      "Epoch 4/20\n",
      "1618/1618 [==============================] - 8s 5ms/step - loss: 0.2191 - accuracy: 0.9120 - auc_pr: 0.9560 - precision: 0.8770 - recall: 0.9585 - val_loss: 1.0403 - val_accuracy: 0.8228 - val_auc_pr: 0.8409 - val_precision: 0.8788 - val_recall: 0.7489\n",
      "Epoch 5/20\n",
      "1618/1618 [==============================] - 8s 5ms/step - loss: 0.2174 - accuracy: 0.9131 - auc_pr: 0.9565 - precision: 0.8782 - recall: 0.9592 - val_loss: 1.1066 - val_accuracy: 0.8250 - val_auc_pr: 0.8196 - val_precision: 0.8757 - val_recall: 0.7576\n",
      "Epoch 6/20\n",
      "1618/1618 [==============================] - 8s 5ms/step - loss: 0.2168 - accuracy: 0.9136 - auc_pr: 0.9565 - precision: 0.8787 - recall: 0.9597 - val_loss: 1.1728 - val_accuracy: 0.8052 - val_auc_pr: 0.8142 - val_precision: 0.8764 - val_recall: 0.7106\n",
      "Epoch 7/20\n",
      "1618/1618 [==============================] - 8s 5ms/step - loss: 0.2160 - accuracy: 0.9138 - auc_pr: 0.9568 - precision: 0.8789 - recall: 0.9599 - val_loss: 1.0983 - val_accuracy: 0.8389 - val_auc_pr: 0.8277 - val_precision: 0.8853 - val_recall: 0.7788\n",
      "Epoch 8/20\n",
      "1618/1618 [==============================] - 8s 5ms/step - loss: 0.2148 - accuracy: 0.9147 - auc_pr: 0.9570 - precision: 0.8797 - recall: 0.9606 - val_loss: 1.1020 - val_accuracy: 0.8222 - val_auc_pr: 0.8418 - val_precision: 0.8793 - val_recall: 0.7468\n",
      "Epoch 9/20\n",
      "1618/1618 [==============================] - 8s 5ms/step - loss: 0.2134 - accuracy: 0.9151 - auc_pr: 0.9574 - precision: 0.8805 - recall: 0.9605 - val_loss: 1.2952 - val_accuracy: 0.7767 - val_auc_pr: 0.8159 - val_precision: 0.8675 - val_recall: 0.6531\n",
      "Epoch 10/20\n",
      "1618/1618 [==============================] - 8s 5ms/step - loss: 0.2131 - accuracy: 0.9154 - auc_pr: 0.9575 - precision: 0.8809 - recall: 0.9606 - val_loss: 1.3525 - val_accuracy: 0.7703 - val_auc_pr: 0.8225 - val_precision: 0.8732 - val_recall: 0.6326\n",
      "Epoch 11/20\n",
      "1618/1618 [==============================] - 7s 5ms/step - loss: 0.2119 - accuracy: 0.9157 - auc_pr: 0.9583 - precision: 0.8813 - recall: 0.9609 - val_loss: 1.3809 - val_accuracy: 0.7678 - val_auc_pr: 0.8151 - val_precision: 0.8657 - val_recall: 0.6339\n",
      "Epoch 12/20\n",
      "1618/1618 [==============================] - 7s 4ms/step - loss: 0.2106 - accuracy: 0.9166 - auc_pr: 0.9584 - precision: 0.8823 - recall: 0.9614 - val_loss: 1.2408 - val_accuracy: 0.8159 - val_auc_pr: 0.8276 - val_precision: 0.8880 - val_recall: 0.7230\n",
      "Epoch 13/20\n",
      "1618/1618 [==============================] - 7s 5ms/step - loss: 0.2104 - accuracy: 0.9169 - auc_pr: 0.9583 - precision: 0.8825 - recall: 0.9620 - val_loss: 1.4900 - val_accuracy: 0.7699 - val_auc_pr: 0.8124 - val_precision: 0.8750 - val_recall: 0.6299\n",
      "Epoch 14/20\n",
      "1618/1618 [==============================] - 8s 5ms/step - loss: 0.2106 - accuracy: 0.9167 - auc_pr: 0.9588 - precision: 0.8822 - recall: 0.9618 - val_loss: 1.4578 - val_accuracy: 0.7710 - val_auc_pr: 0.8037 - val_precision: 0.8746 - val_recall: 0.6327\n",
      "Epoch 15/20\n",
      "1618/1618 [==============================] - 8s 5ms/step - loss: 0.2084 - accuracy: 0.9177 - auc_pr: 0.9590 - precision: 0.8839 - recall: 0.9618 - val_loss: 1.4879 - val_accuracy: 0.7689 - val_auc_pr: 0.8311 - val_precision: 0.8772 - val_recall: 0.6253\n",
      "Epoch 16/20\n",
      "1618/1618 [==============================] - 8s 5ms/step - loss: 0.2072 - accuracy: 0.9183 - auc_pr: 0.9594 - precision: 0.8846 - recall: 0.9622 - val_loss: 1.3541 - val_accuracy: 0.7707 - val_auc_pr: 0.8146 - val_precision: 0.8697 - val_recall: 0.6368\n",
      "Epoch 17/20\n",
      "1618/1618 [==============================] - 8s 5ms/step - loss: 0.2082 - accuracy: 0.9183 - auc_pr: 0.9590 - precision: 0.8843 - recall: 0.9625 - val_loss: 1.5033 - val_accuracy: 0.7711 - val_auc_pr: 0.8236 - val_precision: 0.8753 - val_recall: 0.6322\n",
      "Epoch 18/20\n",
      "1618/1618 [==============================] - 8s 5ms/step - loss: 0.2065 - accuracy: 0.9187 - auc_pr: 0.9596 - precision: 0.8846 - recall: 0.9632 - val_loss: 1.5169 - val_accuracy: 0.7905 - val_auc_pr: 0.8249 - val_precision: 0.8852 - val_recall: 0.6676\n",
      "Epoch 19/20\n",
      "1618/1618 [==============================] - 8s 5ms/step - loss: 0.2061 - accuracy: 0.9190 - auc_pr: 0.9595 - precision: 0.8853 - recall: 0.9628 - val_loss: 1.6090 - val_accuracy: 0.7716 - val_auc_pr: 0.8153 - val_precision: 0.8773 - val_recall: 0.6316\n",
      "Epoch 20/20\n",
      "1618/1618 [==============================] - 8s 5ms/step - loss: 0.2055 - accuracy: 0.9192 - auc_pr: 0.9600 - precision: 0.8855 - recall: 0.9629 - val_loss: 1.5237 - val_accuracy: 0.7909 - val_auc_pr: 0.8188 - val_precision: 0.8863 - val_recall: 0.6674\n",
      "Epoch 1/20\n",
      "3236/3236 [==============================] - 15s 5ms/step - loss: 0.2189 - accuracy: 0.9130 - auc_pr: 0.9560 - precision: 0.8789 - recall: 0.9581 - val_loss: 1.1836 - val_accuracy: 0.7898 - val_auc_pr: 0.8148 - val_precision: 0.8754 - val_recall: 0.6759\n",
      "Epoch 2/20\n",
      "3236/3236 [==============================] - 16s 5ms/step - loss: 0.2155 - accuracy: 0.9146 - auc_pr: 0.9570 - precision: 0.8809 - recall: 0.9588 - val_loss: 1.2356 - val_accuracy: 0.7925 - val_auc_pr: 0.8257 - val_precision: 0.8846 - val_recall: 0.6728\n",
      "Epoch 3/20\n",
      "3236/3236 [==============================] - 16s 5ms/step - loss: 0.2157 - accuracy: 0.9149 - auc_pr: 0.9566 - precision: 0.8811 - recall: 0.9592 - val_loss: 1.2648 - val_accuracy: 0.7722 - val_auc_pr: 0.8151 - val_precision: 0.8713 - val_recall: 0.6387\n",
      "Epoch 4/20\n",
      "3236/3236 [==============================] - 16s 5ms/step - loss: 0.2136 - accuracy: 0.9160 - auc_pr: 0.9576 - precision: 0.8826 - recall: 0.9598 - val_loss: 1.3242 - val_accuracy: 0.7662 - val_auc_pr: 0.8174 - val_precision: 0.8713 - val_recall: 0.6247\n",
      "Epoch 5/20\n",
      "3236/3236 [==============================] - 16s 5ms/step - loss: 0.2125 - accuracy: 0.9161 - auc_pr: 0.9580 - precision: 0.8824 - recall: 0.9601 - val_loss: 1.4590 - val_accuracy: 0.7792 - val_auc_pr: 0.8114 - val_precision: 0.8777 - val_recall: 0.6487\n",
      "Epoch 6/20\n",
      "3236/3236 [==============================] - 15s 5ms/step - loss: 0.2114 - accuracy: 0.9170 - auc_pr: 0.9583 - precision: 0.8836 - recall: 0.9604 - val_loss: 1.4616 - val_accuracy: 0.7740 - val_auc_pr: 0.8048 - val_precision: 0.8743 - val_recall: 0.6400\n",
      "Epoch 7/20\n",
      "3236/3236 [==============================] - 15s 5ms/step - loss: 0.2108 - accuracy: 0.9172 - auc_pr: 0.9585 - precision: 0.8838 - recall: 0.9607 - val_loss: 1.3678 - val_accuracy: 0.7696 - val_auc_pr: 0.8016 - val_precision: 0.8713 - val_recall: 0.6327\n",
      "Epoch 8/20\n",
      "3236/3236 [==============================] - 15s 5ms/step - loss: 0.2096 - accuracy: 0.9175 - auc_pr: 0.9587 - precision: 0.8841 - recall: 0.9609 - val_loss: 1.5905 - val_accuracy: 0.7764 - val_auc_pr: 0.8096 - val_precision: 0.8815 - val_recall: 0.6386\n",
      "Epoch 9/20\n",
      "3236/3236 [==============================] - 16s 5ms/step - loss: 0.2096 - accuracy: 0.9176 - auc_pr: 0.9587 - precision: 0.8841 - recall: 0.9612 - val_loss: 1.4367 - val_accuracy: 0.7704 - val_auc_pr: 0.8152 - val_precision: 0.8796 - val_recall: 0.6265\n",
      "Epoch 10/20\n",
      "3236/3236 [==============================] - 16s 5ms/step - loss: 0.2086 - accuracy: 0.9183 - auc_pr: 0.9590 - precision: 0.8850 - recall: 0.9615 - val_loss: 1.4108 - val_accuracy: 0.7796 - val_auc_pr: 0.8246 - val_precision: 0.8849 - val_recall: 0.6427\n",
      "Epoch 11/20\n",
      "3236/3236 [==============================] - 15s 5ms/step - loss: 0.2080 - accuracy: 0.9185 - auc_pr: 0.9588 - precision: 0.8852 - recall: 0.9618 - val_loss: 1.3724 - val_accuracy: 0.7847 - val_auc_pr: 0.8213 - val_precision: 0.8806 - val_recall: 0.6587\n",
      "Epoch 12/20\n",
      "3236/3236 [==============================] - 15s 5ms/step - loss: 0.2071 - accuracy: 0.9189 - auc_pr: 0.9593 - precision: 0.8857 - recall: 0.9620 - val_loss: 1.6219 - val_accuracy: 0.7702 - val_auc_pr: 0.8067 - val_precision: 0.8845 - val_recall: 0.6215\n",
      "Epoch 13/20\n",
      "3236/3236 [==============================] - 15s 5ms/step - loss: 0.2060 - accuracy: 0.9196 - auc_pr: 0.9597 - precision: 0.8864 - recall: 0.9625 - val_loss: 1.5969 - val_accuracy: 0.7733 - val_auc_pr: 0.8100 - val_precision: 0.8769 - val_recall: 0.6359\n",
      "Epoch 14/20\n",
      "3236/3236 [==============================] - 16s 5ms/step - loss: 0.2053 - accuracy: 0.9196 - auc_pr: 0.9601 - precision: 0.8868 - recall: 0.9621 - val_loss: 1.4220 - val_accuracy: 0.8213 - val_auc_pr: 0.8440 - val_precision: 0.8961 - val_recall: 0.7270\n",
      "Epoch 15/20\n",
      "3236/3236 [==============================] - 16s 5ms/step - loss: 0.2041 - accuracy: 0.9206 - auc_pr: 0.9601 - precision: 0.8878 - recall: 0.9629 - val_loss: 1.5992 - val_accuracy: 0.7862 - val_auc_pr: 0.8228 - val_precision: 0.8874 - val_recall: 0.6557\n",
      "Epoch 16/20\n",
      "3236/3236 [==============================] - 16s 5ms/step - loss: 0.2035 - accuracy: 0.9207 - auc_pr: 0.9603 - precision: 0.8876 - recall: 0.9635 - val_loss: 1.6190 - val_accuracy: 0.7882 - val_auc_pr: 0.8222 - val_precision: 0.8922 - val_recall: 0.6558\n",
      "Epoch 17/20\n",
      "3236/3236 [==============================] - 15s 5ms/step - loss: 0.2021 - accuracy: 0.9213 - auc_pr: 0.9607 - precision: 0.8886 - recall: 0.9633 - val_loss: 1.8949 - val_accuracy: 0.7742 - val_auc_pr: 0.7902 - val_precision: 0.8881 - val_recall: 0.6274\n",
      "Epoch 18/20\n",
      "3236/3236 [==============================] - 15s 5ms/step - loss: 0.2024 - accuracy: 0.9214 - auc_pr: 0.9606 - precision: 0.8885 - recall: 0.9638 - val_loss: 1.9435 - val_accuracy: 0.7706 - val_auc_pr: 0.7871 - val_precision: 0.8777 - val_recall: 0.6288\n",
      "Epoch 19/20\n",
      "3236/3236 [==============================] - 15s 5ms/step - loss: 0.2013 - accuracy: 0.9218 - auc_pr: 0.9610 - precision: 0.8891 - recall: 0.9638 - val_loss: 1.7614 - val_accuracy: 0.7744 - val_auc_pr: 0.8230 - val_precision: 0.8881 - val_recall: 0.6279\n",
      "Epoch 20/20\n",
      "3236/3236 [==============================] - 15s 5ms/step - loss: 0.2000 - accuracy: 0.9225 - auc_pr: 0.9615 - precision: 0.8899 - recall: 0.9643 - val_loss: 1.7229 - val_accuracy: 0.7889 - val_auc_pr: 0.8150 - val_precision: 0.8898 - val_recall: 0.6594\n",
      "Epoch 1/20\n",
      "6471/6471 [==============================] - 32s 5ms/step - loss: 0.2139 - accuracy: 0.9160 - auc_pr: 0.9568 - precision: 0.8829 - recall: 0.9593 - val_loss: 1.4746 - val_accuracy: 0.8146 - val_auc_pr: 0.8197 - val_precision: 0.8898 - val_recall: 0.7181\n",
      "Epoch 2/20\n",
      "6471/6471 [==============================] - 31s 5ms/step - loss: 0.2133 - accuracy: 0.9167 - auc_pr: 0.9575 - precision: 0.8836 - recall: 0.9598 - val_loss: 1.3719 - val_accuracy: 0.7701 - val_auc_pr: 0.8215 - val_precision: 0.8767 - val_recall: 0.6286\n",
      "Epoch 3/20\n",
      "6471/6471 [==============================] - 32s 5ms/step - loss: 0.2124 - accuracy: 0.9167 - auc_pr: 0.9578 - precision: 0.8835 - recall: 0.9598 - val_loss: 1.2197 - val_accuracy: 0.8313 - val_auc_pr: 0.8342 - val_precision: 0.8970 - val_recall: 0.7486\n",
      "Epoch 4/20\n",
      "6471/6471 [==============================] - 31s 5ms/step - loss: 0.2110 - accuracy: 0.9177 - auc_pr: 0.9583 - precision: 0.8849 - recall: 0.9602 - val_loss: 1.3156 - val_accuracy: 0.8444 - val_auc_pr: 0.8407 - val_precision: 0.9001 - val_recall: 0.7747\n",
      "Epoch 5/20\n",
      "6471/6471 [==============================] - 31s 5ms/step - loss: 0.2094 - accuracy: 0.9181 - auc_pr: 0.9584 - precision: 0.8855 - recall: 0.9604 - val_loss: 1.4632 - val_accuracy: 0.8074 - val_auc_pr: 0.8193 - val_precision: 0.8922 - val_recall: 0.6993\n",
      "Epoch 6/20\n",
      "6471/6471 [==============================] - 32s 5ms/step - loss: 0.2088 - accuracy: 0.9186 - auc_pr: 0.9588 - precision: 0.8860 - recall: 0.9609 - val_loss: 1.3527 - val_accuracy: 0.7874 - val_auc_pr: 0.8152 - val_precision: 0.8847 - val_recall: 0.6609\n",
      "Epoch 7/20\n",
      "6471/6471 [==============================] - 32s 5ms/step - loss: 0.2079 - accuracy: 0.9189 - auc_pr: 0.9590 - precision: 0.8864 - recall: 0.9609 - val_loss: 1.4464 - val_accuracy: 0.7759 - val_auc_pr: 0.8134 - val_precision: 0.8828 - val_recall: 0.6364\n",
      "Epoch 8/20\n",
      "6471/6471 [==============================] - 31s 5ms/step - loss: 0.2078 - accuracy: 0.9192 - auc_pr: 0.9594 - precision: 0.8865 - recall: 0.9616 - val_loss: 1.4560 - val_accuracy: 0.7848 - val_auc_pr: 0.8337 - val_precision: 0.8857 - val_recall: 0.6539\n",
      "Epoch 9/20\n",
      "6471/6471 [==============================] - 32s 5ms/step - loss: 0.2065 - accuracy: 0.9199 - auc_pr: 0.9597 - precision: 0.8876 - recall: 0.9616 - val_loss: 1.7881 - val_accuracy: 0.7678 - val_auc_pr: 0.8096 - val_precision: 0.8859 - val_recall: 0.6148\n",
      "Epoch 10/20\n",
      "6471/6471 [==============================] - 31s 5ms/step - loss: 0.2052 - accuracy: 0.9207 - auc_pr: 0.9599 - precision: 0.8883 - recall: 0.9624 - val_loss: 1.7903 - val_accuracy: 0.7724 - val_auc_pr: 0.8213 - val_precision: 0.8887 - val_recall: 0.6229\n",
      "Epoch 11/20\n",
      "6471/6471 [==============================] - 32s 5ms/step - loss: 0.2048 - accuracy: 0.9208 - auc_pr: 0.9599 - precision: 0.8886 - recall: 0.9623 - val_loss: 1.6282 - val_accuracy: 0.7770 - val_auc_pr: 0.8129 - val_precision: 0.8810 - val_recall: 0.6405\n",
      "Epoch 12/20\n",
      "6471/6471 [==============================] - 32s 5ms/step - loss: 0.2044 - accuracy: 0.9210 - auc_pr: 0.9603 - precision: 0.8885 - recall: 0.9627 - val_loss: 1.7664 - val_accuracy: 0.7767 - val_auc_pr: 0.8135 - val_precision: 0.8868 - val_recall: 0.6343\n",
      "Epoch 13/20\n",
      "6471/6471 [==============================] - 31s 5ms/step - loss: 0.2038 - accuracy: 0.9213 - auc_pr: 0.9600 - precision: 0.8892 - recall: 0.9627 - val_loss: 1.5520 - val_accuracy: 0.7878 - val_auc_pr: 0.8201 - val_precision: 0.8870 - val_recall: 0.6596\n",
      "Epoch 14/20\n",
      "6471/6471 [==============================] - 32s 5ms/step - loss: 0.2019 - accuracy: 0.9222 - auc_pr: 0.9608 - precision: 0.8898 - recall: 0.9636 - val_loss: 1.8452 - val_accuracy: 0.7647 - val_auc_pr: 0.8220 - val_precision: 0.8871 - val_recall: 0.6065\n",
      "Epoch 15/20\n",
      "6471/6471 [==============================] - 32s 5ms/step - loss: 0.2018 - accuracy: 0.9225 - auc_pr: 0.9607 - precision: 0.8901 - recall: 0.9640 - val_loss: 1.6372 - val_accuracy: 0.7830 - val_auc_pr: 0.8247 - val_precision: 0.8922 - val_recall: 0.6437\n",
      "Epoch 16/20\n",
      "6471/6471 [==============================] - 31s 5ms/step - loss: 0.2008 - accuracy: 0.9228 - auc_pr: 0.9607 - precision: 0.8906 - recall: 0.9640 - val_loss: 1.5258 - val_accuracy: 0.8280 - val_auc_pr: 0.8352 - val_precision: 0.9036 - val_recall: 0.7343\n",
      "Epoch 17/20\n",
      "6471/6471 [==============================] - 32s 5ms/step - loss: 0.2005 - accuracy: 0.9230 - auc_pr: 0.9611 - precision: 0.8910 - recall: 0.9640 - val_loss: 1.6494 - val_accuracy: 0.8367 - val_auc_pr: 0.8395 - val_precision: 0.9051 - val_recall: 0.7522\n",
      "Epoch 18/20\n",
      "6471/6471 [==============================] - 31s 5ms/step - loss: 0.1997 - accuracy: 0.9235 - auc_pr: 0.9611 - precision: 0.8918 - recall: 0.9639 - val_loss: 1.7945 - val_accuracy: 0.7628 - val_auc_pr: 0.8211 - val_precision: 0.8890 - val_recall: 0.6005\n",
      "Epoch 19/20\n",
      "6471/6471 [==============================] - 31s 5ms/step - loss: 0.1997 - accuracy: 0.9238 - auc_pr: 0.9609 - precision: 0.8920 - recall: 0.9643 - val_loss: 1.9604 - val_accuracy: 0.7609 - val_auc_pr: 0.8116 - val_precision: 0.8812 - val_recall: 0.6031\n",
      "Epoch 20/20\n",
      "6471/6471 [==============================] - 31s 5ms/step - loss: 0.1988 - accuracy: 0.9239 - auc_pr: 0.9611 - precision: 0.8919 - recall: 0.9647 - val_loss: 1.6634 - val_accuracy: 0.8132 - val_auc_pr: 0.8462 - val_precision: 0.9017 - val_recall: 0.7031\n",
      "Epoch 1/20\n",
      "12942/12942 [==============================] - 63s 5ms/step - loss: 0.2109 - accuracy: 0.9180 - auc_pr: 0.9578 - precision: 0.8855 - recall: 0.9603 - val_loss: 1.6786 - val_accuracy: 0.7705 - val_auc_pr: 0.8207 - val_precision: 0.8819 - val_recall: 0.6247\n",
      "Epoch 2/20\n",
      "12942/12942 [==============================] - 64s 5ms/step - loss: 0.2116 - accuracy: 0.9181 - auc_pr: 0.9584 - precision: 0.8858 - recall: 0.9600 - val_loss: 1.2992 - val_accuracy: 0.8373 - val_auc_pr: 0.8302 - val_precision: 0.8906 - val_recall: 0.7691\n",
      "Epoch 3/20\n",
      "12942/12942 [==============================] - 65s 5ms/step - loss: 0.2104 - accuracy: 0.9191 - auc_pr: 0.9582 - precision: 0.8869 - recall: 0.9607 - val_loss: 1.4040 - val_accuracy: 0.7904 - val_auc_pr: 0.8197 - val_precision: 0.8840 - val_recall: 0.6685\n",
      "Epoch 4/20\n",
      "12942/12942 [==============================] - 63s 5ms/step - loss: 0.2094 - accuracy: 0.9189 - auc_pr: 0.9584 - precision: 0.8869 - recall: 0.9602 - val_loss: 1.8863 - val_accuracy: 0.7694 - val_auc_pr: 0.8118 - val_precision: 0.8814 - val_recall: 0.6225\n",
      "Epoch 5/20\n",
      "12942/12942 [==============================] - 63s 5ms/step - loss: 0.2092 - accuracy: 0.9190 - auc_pr: 0.9584 - precision: 0.8867 - recall: 0.9608 - val_loss: 2.1166 - val_accuracy: 0.7667 - val_auc_pr: 0.8010 - val_precision: 0.8805 - val_recall: 0.6172\n",
      "Epoch 6/20\n",
      "12942/12942 [==============================] - 64s 5ms/step - loss: 0.2082 - accuracy: 0.9198 - auc_pr: 0.9589 - precision: 0.8878 - recall: 0.9611 - val_loss: 2.2647 - val_accuracy: 0.7892 - val_auc_pr: 0.8443 - val_precision: 0.8905 - val_recall: 0.6595\n",
      "Epoch 7/20\n",
      "12942/12942 [==============================] - 62s 5ms/step - loss: 0.2079 - accuracy: 0.9200 - auc_pr: 0.9589 - precision: 0.8877 - recall: 0.9617 - val_loss: 1.7775 - val_accuracy: 0.8256 - val_auc_pr: 0.8434 - val_precision: 0.8981 - val_recall: 0.7346\n",
      "Epoch 8/20\n",
      "12942/12942 [==============================] - 62s 5ms/step - loss: 0.2073 - accuracy: 0.9205 - auc_pr: 0.9594 - precision: 0.8885 - recall: 0.9617 - val_loss: 1.4771 - val_accuracy: 0.8212 - val_auc_pr: 0.8278 - val_precision: 0.8912 - val_recall: 0.7317\n",
      "Epoch 9/20\n",
      "12942/12942 [==============================] - 59s 5ms/step - loss: 0.2067 - accuracy: 0.9207 - auc_pr: 0.9593 - precision: 0.8886 - recall: 0.9620 - val_loss: 1.7275 - val_accuracy: 0.8210 - val_auc_pr: 0.8540 - val_precision: 0.8991 - val_recall: 0.7233\n",
      "Epoch 10/20\n",
      "12942/12942 [==============================] - 63s 5ms/step - loss: 0.2046 - accuracy: 0.9213 - auc_pr: 0.9599 - precision: 0.8897 - recall: 0.9620 - val_loss: 2.0840 - val_accuracy: 0.8142 - val_auc_pr: 0.8336 - val_precision: 0.9027 - val_recall: 0.7043\n",
      "Epoch 11/20\n",
      "12942/12942 [==============================] - 62s 5ms/step - loss: 0.2040 - accuracy: 0.9218 - auc_pr: 0.9602 - precision: 0.8899 - recall: 0.9627 - val_loss: 1.9431 - val_accuracy: 0.7616 - val_auc_pr: 0.7969 - val_precision: 0.8779 - val_recall: 0.6077\n",
      "Epoch 12/20\n",
      "12942/12942 [==============================] - 60s 5ms/step - loss: 0.2040 - accuracy: 0.9223 - auc_pr: 0.9600 - precision: 0.8904 - recall: 0.9631 - val_loss: 2.2570 - val_accuracy: 0.7713 - val_auc_pr: 0.8271 - val_precision: 0.8893 - val_recall: 0.6198\n",
      "Epoch 13/20\n",
      "12942/12942 [==============================] - 64s 5ms/step - loss: 0.2024 - accuracy: 0.9229 - auc_pr: 0.9607 - precision: 0.8911 - recall: 0.9637 - val_loss: 2.1368 - val_accuracy: 0.7738 - val_auc_pr: 0.8253 - val_precision: 0.8896 - val_recall: 0.6252\n",
      "Epoch 14/20\n",
      "12942/12942 [==============================] - 63s 5ms/step - loss: 0.2030 - accuracy: 0.9224 - auc_pr: 0.9604 - precision: 0.8904 - recall: 0.9633 - val_loss: 2.0773 - val_accuracy: 0.7908 - val_auc_pr: 0.8309 - val_precision: 0.8966 - val_recall: 0.6574\n",
      "Epoch 15/20\n",
      "12942/12942 [==============================] - 63s 5ms/step - loss: 0.2016 - accuracy: 0.9230 - auc_pr: 0.9606 - precision: 0.8912 - recall: 0.9636 - val_loss: 1.7206 - val_accuracy: 0.8197 - val_auc_pr: 0.8359 - val_precision: 0.9006 - val_recall: 0.7186\n",
      "Epoch 16/20\n",
      "12942/12942 [==============================] - 63s 5ms/step - loss: 0.2010 - accuracy: 0.9233 - auc_pr: 0.9607 - precision: 0.8917 - recall: 0.9638 - val_loss: 2.1135 - val_accuracy: 0.7640 - val_auc_pr: 0.8130 - val_precision: 0.8859 - val_recall: 0.6060\n",
      "Epoch 17/20\n",
      "12942/12942 [==============================] - 63s 5ms/step - loss: 0.2006 - accuracy: 0.9240 - auc_pr: 0.9612 - precision: 0.8919 - recall: 0.9649 - val_loss: 2.5952 - val_accuracy: 0.7702 - val_auc_pr: 0.8079 - val_precision: 0.8850 - val_recall: 0.6211\n",
      "Epoch 18/20\n",
      "12942/12942 [==============================] - 63s 5ms/step - loss: 0.1992 - accuracy: 0.9240 - auc_pr: 0.9613 - precision: 0.8923 - recall: 0.9645 - val_loss: 2.4604 - val_accuracy: 0.7665 - val_auc_pr: 0.8005 - val_precision: 0.8902 - val_recall: 0.6080\n",
      "Epoch 19/20\n",
      "12942/12942 [==============================] - 63s 5ms/step - loss: 0.1998 - accuracy: 0.9245 - auc_pr: 0.9612 - precision: 0.8928 - recall: 0.9649 - val_loss: 3.1913 - val_accuracy: 0.7594 - val_auc_pr: 0.7995 - val_precision: 0.8906 - val_recall: 0.5913\n",
      "Epoch 20/20\n",
      "12942/12942 [==============================] - 64s 5ms/step - loss: 0.1987 - accuracy: 0.9246 - auc_pr: 0.9615 - precision: 0.8930 - recall: 0.9648 - val_loss: 2.3860 - val_accuracy: 0.7756 - val_auc_pr: 0.8102 - val_precision: 0.8955 - val_recall: 0.6239\n",
      "Epoch 1/20\n",
      "25881/25883 [============================>.] - ETA: 0s - loss: 0.2112 - accuracy: 0.9189 - auc_pr: 0.9581 - precision: 0.8864 - recall: 0.9608"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m results_df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch Size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal_Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUC_PR\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_size \u001b[38;5;129;01min\u001b[39;00m batch_sizes:\n\u001b[1;32m----> 5\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_valid_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\DevLib\\Conda\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "batch_sizes = [1024, 512, 256, 128, 64, 32]\n",
    "results_df2 = pd.DataFrame(columns=[\"Batch Size\", \"Accuracy\", \"Loss\", \"Val_Accuracy\", \"AUC_PR\", \"Precision\", \"Recall\"])\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    history = model.fit(X_train_scaled, y_train, epochs=20, batch_size=batch_size, verbose=1, validation_data=(X_valid_scaled, y_valid))\n",
    "    accuracy = history.history['accuracy'][-1]\n",
    "    loss = history.history['loss'][-1]\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    auc_pr = history.history['val_auc_pr'][-1]\n",
    "    precision = history.history['val_precision'][-1]\n",
    "    recall = history.history['val_recall'][-1]\n",
    "    new_row = pd.DataFrame({\n",
    "        \"Batch Size\": [batch_size],\n",
    "        \"Accuracy\": [accuracy],\n",
    "        \"Loss\": [loss],\n",
    "        \"Val_Accuracy\": [val_accuracy],\n",
    "        \"AUC_PR\": [auc_pr],\n",
    "        \"Precision\": [precision],\n",
    "        \"Recall\": [recall]\n",
    "    })\n",
    "    results_df2 = pd.concat([results_df2, new_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Val_Accuracy</th>\n",
       "      <th>AUC_PR</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>128</td>\n",
       "      <td>0.923915</td>\n",
       "      <td>0.198803</td>\n",
       "      <td>0.813243</td>\n",
       "      <td>0.846187</td>\n",
       "      <td>0.901731</td>\n",
       "      <td>0.703110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>0.915880</td>\n",
       "      <td>0.212051</td>\n",
       "      <td>0.817292</td>\n",
       "      <td>0.824375</td>\n",
       "      <td>0.881952</td>\n",
       "      <td>0.732648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>512</td>\n",
       "      <td>0.919186</td>\n",
       "      <td>0.205489</td>\n",
       "      <td>0.790869</td>\n",
       "      <td>0.818765</td>\n",
       "      <td>0.886254</td>\n",
       "      <td>0.667393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>256</td>\n",
       "      <td>0.922450</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.788870</td>\n",
       "      <td>0.814965</td>\n",
       "      <td>0.889807</td>\n",
       "      <td>0.659399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>0.924561</td>\n",
       "      <td>0.198726</td>\n",
       "      <td>0.775554</td>\n",
       "      <td>0.810235</td>\n",
       "      <td>0.895529</td>\n",
       "      <td>0.623890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Batch Size  Accuracy      Loss  Val_Accuracy    AUC_PR  Precision    Recall\n",
       "3        128  0.923915  0.198803      0.813243  0.846187   0.901731  0.703110\n",
       "0       1024  0.915880  0.212051      0.817292  0.824375   0.881952  0.732648\n",
       "1        512  0.919186  0.205489      0.790869  0.818765   0.886254  0.667393\n",
       "2        256  0.922450  0.200001      0.788870  0.814965   0.889807  0.659399\n",
       "4         64  0.924561  0.198726      0.775554  0.810235   0.895529  0.623890"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df2.sort_values(by='AUC_PR', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6471/6471 [==============================] - 34s 5ms/step - loss: 0.3954 - accuracy: 0.8241 - auc_pr: 0.8788 - precision_1: 0.7825 - recall_1: 0.8976 - val_loss: 0.3372 - val_accuracy: 0.8581 - val_auc_pr: 0.9348 - val_precision_1: 0.8049 - val_recall_1: 0.9453\n",
      "Epoch 2/20\n",
      "6471/6471 [==============================] - 32s 5ms/step - loss: 0.3351 - accuracy: 0.8534 - auc_pr: 0.9130 - precision_1: 0.8153 - recall_1: 0.9138 - val_loss: 0.3871 - val_accuracy: 0.8687 - val_auc_pr: 0.8882 - val_precision_1: 0.8187 - val_recall_1: 0.9470\n",
      "Epoch 3/20\n",
      "6471/6471 [==============================] - 31s 5ms/step - loss: 0.3124 - accuracy: 0.8641 - auc_pr: 0.9239 - precision_1: 0.8256 - recall_1: 0.9233 - val_loss: 0.3976 - val_accuracy: 0.8739 - val_auc_pr: 0.8904 - val_precision_1: 0.8323 - val_recall_1: 0.9365\n",
      "Epoch 4/20\n",
      "6471/6471 [==============================] - 32s 5ms/step - loss: 0.2996 - accuracy: 0.8702 - auc_pr: 0.9294 - precision_1: 0.8319 - recall_1: 0.9278 - val_loss: 0.4024 - val_accuracy: 0.8759 - val_auc_pr: 0.9090 - val_precision_1: 0.8337 - val_recall_1: 0.9390\n",
      "Epoch 5/20\n",
      "6471/6471 [==============================] - 32s 5ms/step - loss: 0.2903 - accuracy: 0.8749 - auc_pr: 0.9328 - precision_1: 0.8361 - recall_1: 0.9327 - val_loss: 0.4875 - val_accuracy: 0.8752 - val_auc_pr: 0.8801 - val_precision_1: 0.8388 - val_recall_1: 0.9289\n",
      "Epoch 6/20\n",
      "6471/6471 [==============================] - 31s 5ms/step - loss: 0.2826 - accuracy: 0.8782 - auc_pr: 0.9359 - precision_1: 0.8393 - recall_1: 0.9356 - val_loss: 0.4509 - val_accuracy: 0.8805 - val_auc_pr: 0.8940 - val_precision_1: 0.8493 - val_recall_1: 0.9251\n",
      "Epoch 7/20\n",
      "6471/6471 [==============================] - 31s 5ms/step - loss: 0.2770 - accuracy: 0.8811 - auc_pr: 0.9376 - precision_1: 0.8427 - recall_1: 0.9372 - val_loss: 0.4799 - val_accuracy: 0.8770 - val_auc_pr: 0.8937 - val_precision_1: 0.8532 - val_recall_1: 0.9109\n",
      "Epoch 8/20\n",
      "6471/6471 [==============================] - 32s 5ms/step - loss: 0.2717 - accuracy: 0.8839 - auc_pr: 0.9396 - precision_1: 0.8453 - recall_1: 0.9398 - val_loss: 0.5031 - val_accuracy: 0.8755 - val_auc_pr: 0.8974 - val_precision_1: 0.8450 - val_recall_1: 0.9196\n",
      "Epoch 9/20\n",
      "6471/6471 [==============================] - 32s 5ms/step - loss: 0.2677 - accuracy: 0.8864 - auc_pr: 0.9407 - precision_1: 0.8477 - recall_1: 0.9420 - val_loss: 0.5005 - val_accuracy: 0.8793 - val_auc_pr: 0.8955 - val_precision_1: 0.8544 - val_recall_1: 0.9144\n",
      "Epoch 10/20\n",
      "6471/6471 [==============================] - 31s 5ms/step - loss: 0.2637 - accuracy: 0.8885 - auc_pr: 0.9424 - precision_1: 0.8502 - recall_1: 0.9433 - val_loss: 0.5474 - val_accuracy: 0.8746 - val_auc_pr: 0.8869 - val_precision_1: 0.8470 - val_recall_1: 0.9144\n",
      "Epoch 11/20\n",
      "6471/6471 [==============================] - 31s 5ms/step - loss: 0.2599 - accuracy: 0.8905 - auc_pr: 0.9434 - precision_1: 0.8524 - recall_1: 0.9444 - val_loss: 0.5311 - val_accuracy: 0.8785 - val_auc_pr: 0.8765 - val_precision_1: 0.8501 - val_recall_1: 0.9190\n",
      "Epoch 12/20\n",
      "6471/6471 [==============================] - 31s 5ms/step - loss: 0.2568 - accuracy: 0.8920 - auc_pr: 0.9448 - precision_1: 0.8539 - recall_1: 0.9458 - val_loss: 0.5898 - val_accuracy: 0.8808 - val_auc_pr: 0.8628 - val_precision_1: 0.8574 - val_recall_1: 0.9136\n",
      "Epoch 13/20\n",
      "6471/6471 [==============================] - 31s 5ms/step - loss: 0.2537 - accuracy: 0.8934 - auc_pr: 0.9459 - precision_1: 0.8552 - recall_1: 0.9471 - val_loss: 0.6067 - val_accuracy: 0.8799 - val_auc_pr: 0.8686 - val_precision_1: 0.8577 - val_recall_1: 0.9109\n",
      "Epoch 14/20\n",
      "6471/6471 [==============================] - 31s 5ms/step - loss: 0.2509 - accuracy: 0.8949 - auc_pr: 0.9466 - precision_1: 0.8569 - recall_1: 0.9482 - val_loss: 0.5747 - val_accuracy: 0.8768 - val_auc_pr: 0.8881 - val_precision_1: 0.8558 - val_recall_1: 0.9063\n",
      "Epoch 15/20\n",
      "6471/6471 [==============================] - 32s 5ms/step - loss: 0.2484 - accuracy: 0.8961 - auc_pr: 0.9474 - precision_1: 0.8586 - recall_1: 0.9483 - val_loss: 0.6683 - val_accuracy: 0.8812 - val_auc_pr: 0.8800 - val_precision_1: 0.8651 - val_recall_1: 0.9031\n",
      "Epoch 16/20\n",
      "6471/6471 [==============================] - 32s 5ms/step - loss: 0.2462 - accuracy: 0.8973 - auc_pr: 0.9480 - precision_1: 0.8594 - recall_1: 0.9501 - val_loss: 0.6611 - val_accuracy: 0.8732 - val_auc_pr: 0.8922 - val_precision_1: 0.8591 - val_recall_1: 0.8930\n",
      "Epoch 17/20\n",
      "6471/6471 [==============================] - 33s 5ms/step - loss: 0.2438 - accuracy: 0.8986 - auc_pr: 0.9487 - precision_1: 0.8611 - recall_1: 0.9506 - val_loss: 0.6819 - val_accuracy: 0.8744 - val_auc_pr: 0.8559 - val_precision_1: 0.8544 - val_recall_1: 0.9026\n",
      "Epoch 18/20\n",
      "6471/6471 [==============================] - 33s 5ms/step - loss: 0.2419 - accuracy: 0.8998 - auc_pr: 0.9496 - precision_1: 0.8625 - recall_1: 0.9512 - val_loss: 0.6776 - val_accuracy: 0.8687 - val_auc_pr: 0.8748 - val_precision_1: 0.8645 - val_recall_1: 0.8744\n",
      "Epoch 19/20\n",
      "6471/6471 [==============================] - 31s 5ms/step - loss: 0.2396 - accuracy: 0.9005 - auc_pr: 0.9504 - precision_1: 0.8636 - recall_1: 0.9514 - val_loss: 0.6750 - val_accuracy: 0.8746 - val_auc_pr: 0.8557 - val_precision_1: 0.8713 - val_recall_1: 0.8790\n",
      "Epoch 20/20\n",
      "6471/6471 [==============================] - 32s 5ms/step - loss: 0.2379 - accuracy: 0.9013 - auc_pr: 0.9508 - precision_1: 0.8644 - recall_1: 0.9520 - val_loss: 0.7102 - val_accuracy: 0.8703 - val_auc_pr: 0.8731 - val_precision_1: 0.8673 - val_recall_1: 0.8743\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=20, batch_size=128, verbose=1, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6471/6471 [==============================] - 36s 5ms/step - loss: 0.4181 - accuracy: 0.8165 - auc_pr: 0.8613 - precision: 0.7774 - recall: 0.8869 - val_loss: 0.3560 - val_accuracy: 0.8535 - val_auc_pr: 0.9404 - val_precision: 0.8023 - val_recall: 0.9383\n",
      "Epoch 2/20\n",
      "6471/6471 [==============================] - 34s 5ms/step - loss: 0.3559 - accuracy: 0.8451 - auc_pr: 0.8981 - precision: 0.8109 - recall: 0.9001 - val_loss: 0.3605 - val_accuracy: 0.8674 - val_auc_pr: 0.9252 - val_precision: 0.8182 - val_recall: 0.9447\n",
      "Epoch 3/20\n",
      "6471/6471 [==============================] - 34s 5ms/step - loss: 0.3328 - accuracy: 0.8570 - auc_pr: 0.9106 - precision: 0.8202 - recall: 0.9144 - val_loss: 0.3758 - val_accuracy: 0.8726 - val_auc_pr: 0.9287 - val_precision: 0.8276 - val_recall: 0.9412\n",
      "Epoch 4/20\n",
      "6471/6471 [==============================] - 35s 5ms/step - loss: 0.3192 - accuracy: 0.8634 - auc_pr: 0.9170 - precision: 0.8269 - recall: 0.9193 - val_loss: 0.3778 - val_accuracy: 0.8723 - val_auc_pr: 0.9112 - val_precision: 0.8272 - val_recall: 0.9413\n",
      "Epoch 5/20\n",
      "6471/6471 [==============================] - 35s 5ms/step - loss: 0.3066 - accuracy: 0.8687 - auc_pr: 0.9227 - precision: 0.8312 - recall: 0.9253 - val_loss: 0.4265 - val_accuracy: 0.8717 - val_auc_pr: 0.8841 - val_precision: 0.8294 - val_recall: 0.9360\n",
      "Epoch 6/20\n",
      "6471/6471 [==============================] - 35s 5ms/step - loss: 0.2997 - accuracy: 0.8724 - auc_pr: 0.9251 - precision: 0.8339 - recall: 0.9300 - val_loss: 0.4166 - val_accuracy: 0.8755 - val_auc_pr: 0.9050 - val_precision: 0.8298 - val_recall: 0.9449\n",
      "Epoch 7/20\n",
      "6471/6471 [==============================] - 35s 5ms/step - loss: 0.2919 - accuracy: 0.8758 - auc_pr: 0.9287 - precision: 0.8365 - recall: 0.9342 - val_loss: 0.4675 - val_accuracy: 0.8787 - val_auc_pr: 0.9019 - val_precision: 0.8409 - val_recall: 0.9342\n",
      "Epoch 8/20\n",
      "6471/6471 [==============================] - 34s 5ms/step - loss: 0.2867 - accuracy: 0.8788 - auc_pr: 0.9308 - precision: 0.8404 - recall: 0.9351 - val_loss: 0.5142 - val_accuracy: 0.8809 - val_auc_pr: 0.9132 - val_precision: 0.8493 - val_recall: 0.9261\n",
      "Epoch 9/20\n",
      "6471/6471 [==============================] - 35s 5ms/step - loss: 0.2821 - accuracy: 0.8814 - auc_pr: 0.9324 - precision: 0.8434 - recall: 0.9368 - val_loss: 0.5288 - val_accuracy: 0.8776 - val_auc_pr: 0.9025 - val_precision: 0.8441 - val_recall: 0.9263\n",
      "Epoch 10/20\n",
      "6471/6471 [==============================] - 35s 5ms/step - loss: 0.2780 - accuracy: 0.8835 - auc_pr: 0.9339 - precision: 0.8453 - recall: 0.9387 - val_loss: 0.5294 - val_accuracy: 0.8732 - val_auc_pr: 0.9043 - val_precision: 0.8389 - val_recall: 0.9237\n",
      "Epoch 11/20\n",
      "6471/6471 [==============================] - 35s 5ms/step - loss: 0.2736 - accuracy: 0.8856 - auc_pr: 0.9357 - precision: 0.8477 - recall: 0.9402 - val_loss: 0.5605 - val_accuracy: 0.8782 - val_auc_pr: 0.8715 - val_precision: 0.8499 - val_recall: 0.9185\n",
      "Epoch 12/20\n",
      "6471/6471 [==============================] - 35s 5ms/step - loss: 0.2706 - accuracy: 0.8869 - auc_pr: 0.9367 - precision: 0.8492 - recall: 0.9408 - val_loss: 0.6452 - val_accuracy: 0.8743 - val_auc_pr: 0.8768 - val_precision: 0.8564 - val_recall: 0.8995\n",
      "Epoch 13/20\n",
      "6471/6471 [==============================] - 35s 5ms/step - loss: 0.2676 - accuracy: 0.8887 - auc_pr: 0.9378 - precision: 0.8516 - recall: 0.9414 - val_loss: 0.5886 - val_accuracy: 0.8814 - val_auc_pr: 0.8785 - val_precision: 0.8550 - val_recall: 0.9186\n",
      "Epoch 14/20\n",
      "6471/6471 [==============================] - 35s 5ms/step - loss: 0.2641 - accuracy: 0.8904 - auc_pr: 0.9389 - precision: 0.8525 - recall: 0.9440 - val_loss: 0.6249 - val_accuracy: 0.8792 - val_auc_pr: 0.8806 - val_precision: 0.8514 - val_recall: 0.9188\n",
      "Epoch 15/20\n",
      "6471/6471 [==============================] - 34s 5ms/step - loss: 0.2619 - accuracy: 0.8916 - auc_pr: 0.9395 - precision: 0.8537 - recall: 0.9451 - val_loss: 0.6181 - val_accuracy: 0.8787 - val_auc_pr: 0.8860 - val_precision: 0.8507 - val_recall: 0.9187\n",
      "Epoch 16/20\n",
      "6471/6471 [==============================] - 34s 5ms/step - loss: 0.2608 - accuracy: 0.8922 - auc_pr: 0.9396 - precision: 0.8548 - recall: 0.9449 - val_loss: 0.6017 - val_accuracy: 0.8863 - val_auc_pr: 0.8777 - val_precision: 0.8605 - val_recall: 0.9221\n",
      "Epoch 17/20\n",
      "6471/6471 [==============================] - 34s 5ms/step - loss: 0.2573 - accuracy: 0.8941 - auc_pr: 0.9411 - precision: 0.8573 - recall: 0.9457 - val_loss: 0.7921 - val_accuracy: 0.8809 - val_auc_pr: 0.8411 - val_precision: 0.8605 - val_recall: 0.9091\n",
      "Epoch 18/20\n",
      "6471/6471 [==============================] - 34s 5ms/step - loss: 0.2561 - accuracy: 0.8944 - auc_pr: 0.9415 - precision: 0.8573 - recall: 0.9464 - val_loss: 0.7112 - val_accuracy: 0.8713 - val_auc_pr: 0.8919 - val_precision: 0.8533 - val_recall: 0.8967\n",
      "Epoch 19/20\n",
      "6471/6471 [==============================] - 35s 5ms/step - loss: 0.2546 - accuracy: 0.8956 - auc_pr: 0.9418 - precision: 0.8582 - recall: 0.9478 - val_loss: 0.7673 - val_accuracy: 0.8819 - val_auc_pr: 0.8958 - val_precision: 0.8620 - val_recall: 0.9094\n",
      "Epoch 20/20\n",
      "6471/6471 [==============================] - 34s 5ms/step - loss: 0.2528 - accuracy: 0.8964 - auc_pr: 0.9425 - precision: 0.8596 - recall: 0.9476 - val_loss: 0.8200 - val_accuracy: 0.8750 - val_auc_pr: 0.8584 - val_precision: 0.8644 - val_recall: 0.8896\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Sequential model\n",
    "from tensorflow.keras.metrics import AUC,Precision, Recall\n",
    "\n",
    "\n",
    "model2 = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',AUC(curve='PR', name='auc_pr'),Precision(), Recall()])\n",
    "\n",
    "history = model2.fit(X_train_scaled, y_train, epochs=20, batch_size=128, verbose=1, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6471/6471 [==============================] - 37s 6ms/step - loss: 0.4008 - accuracy: 0.8250 - auc_pr: 0.8733 - precision_1: 0.7871 - recall_1: 0.8909 - val_loss: 0.3429 - val_accuracy: 0.8687 - val_auc_pr: 0.9478 - val_precision_1: 0.8229 - val_recall_1: 0.9397\n",
      "Epoch 2/20\n",
      "6471/6471 [==============================] - 35s 5ms/step - loss: 0.3338 - accuracy: 0.8567 - auc_pr: 0.9107 - precision_1: 0.8227 - recall_1: 0.9093 - val_loss: 0.3829 - val_accuracy: 0.8796 - val_auc_pr: 0.9287 - val_precision_1: 0.8418 - val_recall_1: 0.9350\n",
      "Epoch 3/20\n",
      "6471/6471 [==============================] - 34s 5ms/step - loss: 0.3084 - accuracy: 0.8685 - auc_pr: 0.9229 - precision_1: 0.8337 - recall_1: 0.9206 - val_loss: 0.4793 - val_accuracy: 0.8809 - val_auc_pr: 0.9041 - val_precision_1: 0.8522 - val_recall_1: 0.9216\n",
      "Epoch 4/20\n",
      "6471/6471 [==============================] - 35s 5ms/step - loss: 0.2921 - accuracy: 0.8762 - auc_pr: 0.9298 - precision_1: 0.8402 - recall_1: 0.9291 - val_loss: 0.5086 - val_accuracy: 0.8817 - val_auc_pr: 0.9064 - val_precision_1: 0.8561 - val_recall_1: 0.9177\n",
      "Epoch 5/20\n",
      "6471/6471 [==============================] - 34s 5ms/step - loss: 0.2793 - accuracy: 0.8823 - auc_pr: 0.9348 - precision_1: 0.8455 - recall_1: 0.9355 - val_loss: 0.5178 - val_accuracy: 0.8822 - val_auc_pr: 0.8994 - val_precision_1: 0.8485 - val_recall_1: 0.9304\n",
      "Epoch 6/20\n",
      "6471/6471 [==============================] - 34s 5ms/step - loss: 0.2697 - accuracy: 0.8866 - auc_pr: 0.9386 - precision_1: 0.8504 - recall_1: 0.9383 - val_loss: 0.5557 - val_accuracy: 0.8817 - val_auc_pr: 0.8997 - val_precision_1: 0.8592 - val_recall_1: 0.9130\n",
      "Epoch 7/20\n",
      "6471/6471 [==============================] - 34s 5ms/step - loss: 0.2618 - accuracy: 0.8912 - auc_pr: 0.9412 - precision_1: 0.8549 - recall_1: 0.9424 - val_loss: 0.5685 - val_accuracy: 0.8752 - val_auc_pr: 0.8955 - val_precision_1: 0.8626 - val_recall_1: 0.8927\n",
      "Epoch 8/20\n",
      "6471/6471 [==============================] - 34s 5ms/step - loss: 0.2556 - accuracy: 0.8943 - auc_pr: 0.9430 - precision_1: 0.8579 - recall_1: 0.9452 - val_loss: 0.6952 - val_accuracy: 0.8928 - val_auc_pr: 0.8748 - val_precision_1: 0.8758 - val_recall_1: 0.9155\n",
      "Epoch 9/20\n",
      "6471/6471 [==============================] - 36s 6ms/step - loss: 0.2494 - accuracy: 0.8977 - auc_pr: 0.9455 - precision_1: 0.8621 - recall_1: 0.9469 - val_loss: 0.6570 - val_accuracy: 0.8803 - val_auc_pr: 0.8925 - val_precision_1: 0.8742 - val_recall_1: 0.8886\n",
      "Epoch 10/20\n",
      "6471/6471 [==============================] - 36s 6ms/step - loss: 0.2434 - accuracy: 0.9007 - auc_pr: 0.9473 - precision_1: 0.8647 - recall_1: 0.9501 - val_loss: 0.7912 - val_accuracy: 0.8898 - val_auc_pr: 0.8864 - val_precision_1: 0.8804 - val_recall_1: 0.9022\n",
      "Epoch 11/20\n",
      "6471/6471 [==============================] - 36s 5ms/step - loss: 0.2396 - accuracy: 0.9032 - auc_pr: 0.9481 - precision_1: 0.8669 - recall_1: 0.9527 - val_loss: 0.6333 - val_accuracy: 0.8985 - val_auc_pr: 0.9075 - val_precision_1: 0.8757 - val_recall_1: 0.9288\n",
      "Epoch 12/20\n",
      "6471/6471 [==============================] - 35s 5ms/step - loss: 0.2361 - accuracy: 0.9052 - auc_pr: 0.9494 - precision_1: 0.8693 - recall_1: 0.9539 - val_loss: 0.7387 - val_accuracy: 0.8946 - val_auc_pr: 0.8842 - val_precision_1: 0.8821 - val_recall_1: 0.9109\n",
      "Epoch 13/20\n",
      "6471/6471 [==============================] - 36s 6ms/step - loss: 0.2319 - accuracy: 0.9071 - auc_pr: 0.9507 - precision_1: 0.8716 - recall_1: 0.9548 - val_loss: 0.8481 - val_accuracy: 0.8900 - val_auc_pr: 0.8882 - val_precision_1: 0.8793 - val_recall_1: 0.9041\n",
      "Epoch 14/20\n",
      "6471/6471 [==============================] - 36s 6ms/step - loss: 0.2285 - accuracy: 0.9092 - auc_pr: 0.9517 - precision_1: 0.8738 - recall_1: 0.9565 - val_loss: 0.8190 - val_accuracy: 0.8941 - val_auc_pr: 0.8848 - val_precision_1: 0.8927 - val_recall_1: 0.8958\n",
      "Epoch 15/20\n",
      "6471/6471 [==============================] - 36s 6ms/step - loss: 0.2249 - accuracy: 0.9106 - auc_pr: 0.9527 - precision_1: 0.8752 - recall_1: 0.9578 - val_loss: 0.8794 - val_accuracy: 0.8962 - val_auc_pr: 0.8986 - val_precision_1: 0.8874 - val_recall_1: 0.9076\n",
      "Epoch 16/20\n",
      "6471/6471 [==============================] - 36s 5ms/step - loss: 0.2217 - accuracy: 0.9122 - auc_pr: 0.9538 - precision_1: 0.8771 - recall_1: 0.9588 - val_loss: 0.8756 - val_accuracy: 0.8932 - val_auc_pr: 0.8727 - val_precision_1: 0.8873 - val_recall_1: 0.9008\n",
      "Epoch 17/20\n",
      "6471/6471 [==============================] - 35s 5ms/step - loss: 0.2193 - accuracy: 0.9137 - auc_pr: 0.9545 - precision_1: 0.8792 - recall_1: 0.9592 - val_loss: 1.0276 - val_accuracy: 0.8803 - val_auc_pr: 0.8754 - val_precision_1: 0.8927 - val_recall_1: 0.8646\n",
      "Epoch 18/20\n",
      "6471/6471 [==============================] - 36s 6ms/step - loss: 0.2170 - accuracy: 0.9149 - auc_pr: 0.9551 - precision_1: 0.8807 - recall_1: 0.9598 - val_loss: 1.0063 - val_accuracy: 0.8846 - val_auc_pr: 0.8924 - val_precision_1: 0.9000 - val_recall_1: 0.8654\n",
      "Epoch 19/20\n",
      "6471/6471 [==============================] - 36s 6ms/step - loss: 0.2145 - accuracy: 0.9160 - auc_pr: 0.9558 - precision_1: 0.8813 - recall_1: 0.9615 - val_loss: 0.9071 - val_accuracy: 0.8832 - val_auc_pr: 0.8920 - val_precision_1: 0.8904 - val_recall_1: 0.8741\n",
      "Epoch 20/20\n",
      "6471/6471 [==============================] - 35s 5ms/step - loss: 0.2115 - accuracy: 0.9178 - auc_pr: 0.9568 - precision_1: 0.8839 - recall_1: 0.9620 - val_loss: 1.1030 - val_accuracy: 0.8842 - val_auc_pr: 0.8847 - val_precision_1: 0.8966 - val_recall_1: 0.8687\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Sequential model\n",
    "from tensorflow.keras.metrics import AUC,Precision, Recall\n",
    "\n",
    "\n",
    "model2 = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(96, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',AUC(curve='PR', name='auc_pr'),Precision(), Recall()])\n",
    "\n",
    "history = model2.fit(X_train_scaled, y_train, epochs=20, batch_size=128, verbose=1, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6471/6471 [==============================] - 57s 8ms/step - loss: 0.3964 - accuracy: 0.8280 - auc_pr: 0.8746 - precision_2: 0.7954 - recall_2: 0.8832 - val_loss: 0.3246 - val_accuracy: 0.8663 - val_auc_pr: 0.9400 - val_precision_2: 0.8242 - val_recall_2: 0.9312\n",
      "Epoch 2/20\n",
      "6471/6471 [==============================] - 55s 8ms/step - loss: 0.3325 - accuracy: 0.8608 - auc_pr: 0.9109 - precision_2: 0.8306 - recall_2: 0.9064 - val_loss: 0.3517 - val_accuracy: 0.8716 - val_auc_pr: 0.9196 - val_precision_2: 0.8379 - val_recall_2: 0.9216\n",
      "Epoch 3/20\n",
      "6471/6471 [==============================] - 55s 9ms/step - loss: 0.3080 - accuracy: 0.8716 - auc_pr: 0.9221 - precision_2: 0.8403 - recall_2: 0.9178 - val_loss: 0.4065 - val_accuracy: 0.8492 - val_auc_pr: 0.9011 - val_precision_2: 0.8461 - val_recall_2: 0.8537\n",
      "Epoch 4/20\n",
      "6471/6471 [==============================] - 55s 8ms/step - loss: 0.2918 - accuracy: 0.8786 - auc_pr: 0.9293 - precision_2: 0.8470 - recall_2: 0.9241 - val_loss: 0.4219 - val_accuracy: 0.8683 - val_auc_pr: 0.8932 - val_precision_2: 0.8555 - val_recall_2: 0.8863\n",
      "Epoch 5/20\n",
      "6471/6471 [==============================] - 55s 8ms/step - loss: 0.2801 - accuracy: 0.8841 - auc_pr: 0.9342 - precision_2: 0.8526 - recall_2: 0.9287 - val_loss: 0.4272 - val_accuracy: 0.8600 - val_auc_pr: 0.9089 - val_precision_2: 0.8595 - val_recall_2: 0.8606\n",
      "Epoch 6/20\n",
      "6471/6471 [==============================] - 56s 9ms/step - loss: 0.2699 - accuracy: 0.8889 - auc_pr: 0.9379 - precision_2: 0.8574 - recall_2: 0.9329 - val_loss: 0.4898 - val_accuracy: 0.8257 - val_auc_pr: 0.8883 - val_precision_2: 0.8581 - val_recall_2: 0.7804\n",
      "Epoch 7/20\n",
      "6471/6471 [==============================] - 55s 8ms/step - loss: 0.2635 - accuracy: 0.8918 - auc_pr: 0.9402 - precision_2: 0.8607 - recall_2: 0.9348 - val_loss: 0.4788 - val_accuracy: 0.8254 - val_auc_pr: 0.8927 - val_precision_2: 0.8572 - val_recall_2: 0.7808\n",
      "Epoch 8/20\n",
      "6471/6471 [==============================] - 54s 8ms/step - loss: 0.2574 - accuracy: 0.8950 - auc_pr: 0.9430 - precision_2: 0.8637 - recall_2: 0.9379 - val_loss: 0.4583 - val_accuracy: 0.8645 - val_auc_pr: 0.8841 - val_precision_2: 0.8642 - val_recall_2: 0.8649\n",
      "Epoch 9/20\n",
      "6471/6471 [==============================] - 56s 9ms/step - loss: 0.2522 - accuracy: 0.8976 - auc_pr: 0.9446 - precision_2: 0.8661 - recall_2: 0.9406 - val_loss: 0.5017 - val_accuracy: 0.8309 - val_auc_pr: 0.8550 - val_precision_2: 0.8620 - val_recall_2: 0.7879\n",
      "Epoch 10/20\n",
      "6471/6471 [==============================] - 55s 8ms/step - loss: 0.2473 - accuracy: 0.8998 - auc_pr: 0.9461 - precision_2: 0.8690 - recall_2: 0.9416 - val_loss: 0.4806 - val_accuracy: 0.8392 - val_auc_pr: 0.8663 - val_precision_2: 0.8658 - val_recall_2: 0.8028\n",
      "Epoch 11/20\n",
      "6471/6471 [==============================] - 54s 8ms/step - loss: 0.2434 - accuracy: 0.9016 - auc_pr: 0.9474 - precision_2: 0.8707 - recall_2: 0.9433 - val_loss: 0.5302 - val_accuracy: 0.8283 - val_auc_pr: 0.8729 - val_precision_2: 0.8734 - val_recall_2: 0.7681\n",
      "Epoch 12/20\n",
      "6471/6471 [==============================] - 56s 9ms/step - loss: 0.2402 - accuracy: 0.9035 - auc_pr: 0.9485 - precision_2: 0.8723 - recall_2: 0.9454 - val_loss: 0.4641 - val_accuracy: 0.8691 - val_auc_pr: 0.8881 - val_precision_2: 0.8789 - val_recall_2: 0.8561\n",
      "Epoch 13/20\n",
      "6471/6471 [==============================] - 56s 9ms/step - loss: 0.2370 - accuracy: 0.9043 - auc_pr: 0.9499 - precision_2: 0.8736 - recall_2: 0.9455 - val_loss: 0.5015 - val_accuracy: 0.8718 - val_auc_pr: 0.8928 - val_precision_2: 0.8835 - val_recall_2: 0.8567\n",
      "Epoch 14/20\n",
      "6471/6471 [==============================] - 54s 8ms/step - loss: 0.2338 - accuracy: 0.9064 - auc_pr: 0.9505 - precision_2: 0.8755 - recall_2: 0.9475 - val_loss: 0.5542 - val_accuracy: 0.8740 - val_auc_pr: 0.8838 - val_precision_2: 0.8946 - val_recall_2: 0.8478\n",
      "Epoch 15/20\n",
      "6471/6471 [==============================] - 55s 8ms/step - loss: 0.2307 - accuracy: 0.9080 - auc_pr: 0.9516 - precision_2: 0.8773 - recall_2: 0.9486 - val_loss: 0.5352 - val_accuracy: 0.8712 - val_auc_pr: 0.8958 - val_precision_2: 0.8882 - val_recall_2: 0.8491\n",
      "Epoch 16/20\n",
      "6471/6471 [==============================] - 56s 9ms/step - loss: 0.2285 - accuracy: 0.9090 - auc_pr: 0.9524 - precision_2: 0.8784 - recall_2: 0.9495 - val_loss: 0.5994 - val_accuracy: 0.8654 - val_auc_pr: 0.8689 - val_precision_2: 0.8958 - val_recall_2: 0.8269\n",
      "Epoch 17/20\n",
      "6471/6471 [==============================] - 54s 8ms/step - loss: 0.2268 - accuracy: 0.9098 - auc_pr: 0.9525 - precision_2: 0.8790 - recall_2: 0.9503 - val_loss: 0.5955 - val_accuracy: 0.8369 - val_auc_pr: 0.8769 - val_precision_2: 0.8826 - val_recall_2: 0.7771\n",
      "Epoch 18/20\n",
      "6471/6471 [==============================] - 55s 9ms/step - loss: 0.2242 - accuracy: 0.9112 - auc_pr: 0.9539 - precision_2: 0.8813 - recall_2: 0.9504 - val_loss: 0.6364 - val_accuracy: 0.8319 - val_auc_pr: 0.8744 - val_precision_2: 0.8868 - val_recall_2: 0.7609\n",
      "Epoch 19/20\n",
      "6471/6471 [==============================] - 56s 9ms/step - loss: 0.2225 - accuracy: 0.9122 - auc_pr: 0.9541 - precision_2: 0.8819 - recall_2: 0.9519 - val_loss: 0.5656 - val_accuracy: 0.8468 - val_auc_pr: 0.8844 - val_precision_2: 0.8800 - val_recall_2: 0.8031\n",
      "Epoch 20/20\n",
      "6471/6471 [==============================] - 55s 8ms/step - loss: 0.2211 - accuracy: 0.9128 - auc_pr: 0.9544 - precision_2: 0.8825 - recall_2: 0.9525 - val_loss: 0.6703 - val_accuracy: 0.8629 - val_auc_pr: 0.8759 - val_precision_2: 0.8978 - val_recall_2: 0.8191\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization,Activation\n",
    "\n",
    "model3 = Sequential([\n",
    "    Dense(256, input_shape=(X_train_scaled.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(128),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(96),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(64),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',AUC(curve='PR', name='auc_pr'),Precision(), Recall()])\n",
    "\n",
    "history = model3.fit(X_train_scaled, y_train, epochs=20, batch_size=128, verbose=1, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f'Test accuracy: {accuracy:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict_classes(X_test_scaled)\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other machine learning methodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, auc, precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Define the models to benchmark\n",
    "models = [\n",
    "    (\"Logistic Regression\", LogisticRegression(max_iter=1000)),\n",
    "    (\"SVM\", SVC(probability=True)),\n",
    "    (\"K-Nearest Neighbors\", KNeighborsClassifier()),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
    "    (\"Random Forest\", RandomForestClassifier()),\n",
    "    (\"Gradient Boosting\", GradientBoostingClassifier()),\n",
    "    (\"MLP Classifier\", MLPClassifier(max_iter=1000))\n",
    "]\n",
    "\n",
    "# Function to calculate AUC-PR\n",
    "def calculate_auc_pr(y_true, y_scores):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    return auc(recall, precision)\n",
    "\n",
    "# Evaluate each model\n",
    "results = []\n",
    "for name, model in models:\n",
    "    # Fit the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_valid_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_valid_scaled)[:,1] if hasattr(model, \"predict_proba\") else model.decision_function(X_valid_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    precision = precision_score(y_valid, y_pred)\n",
    "    recall = recall_score(y_valid, y_pred)\n",
    "    auc_pr = calculate_auc_pr(y_valid, y_pred_proba)\n",
    "    \n",
    "    # Store the results\n",
    "    results.append((name, accuracy, precision, recall, auc_pr))\n",
    "\n",
    "# Display the results\n",
    "for name, accuracy, precision, recall, auc_pr in results:\n",
    "    print(f\"{name}: Accuracy={accuracy:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, AUC-PR={auc_pr:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
